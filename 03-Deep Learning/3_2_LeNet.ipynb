{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e4250-57c6-40f1-bd2e-1863b0479e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.test.is_gpu_available()  # should return True\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda1b9e-1d0f-43a6-ac53-69ef5576b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten, Conv2D, MaxPooling2D, AvgPool2D # new!\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a536f0-4d56-44ff-a1f4-884cf72e09ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f009edb-9203-4c9a-8c33-80fe2ce9a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text labels\n",
    "fashion_mnist_labels = [\"T-shirt/top\",  # index 0\n",
    "                        \"Trouser\",      # index 1\n",
    "                        \"Pullover\",     # index 2 \n",
    "                        \"Dress\",        # index 3 \n",
    "                        \"Coat\",         # index 4\n",
    "                        \"Sandal\",       # index 5\n",
    "                        \"Shirt\",        # index 6 \n",
    "                        \"Sneaker\",      # index 7 \n",
    "                        \"Bag\",          # index 8 \n",
    "                        \"Ankle boot\"]   # index 9\n",
    "\n",
    "# Image index, you can pick any number between 0 and 59,999\n",
    "img_index = random.randint(0,60000)\n",
    "\n",
    "# y_train contains the lables, ranging from 0 to 9\n",
    "label_index = y_train[img_index]\n",
    "\n",
    "# Print the label, for random example\n",
    "print (\"y [\" + str(img_index) + \"]= \" + str(label_index) + \" \" +(fashion_mnist_labels[label_index]))\n",
    "\n",
    "# # Show one of the images from the training dataset\n",
    "plt.imshow(x_train[img_index], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A single pixel is of type Integer\n",
    "type (x_train[0][0][0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68099a8b66b12b11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A complete image is stored as numpy Array\n",
    "type (x_train[img_index])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45655f745790a38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We received 28x28 pixel images\n",
    "x_train[img_index].shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f9fca48c5d8a13a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# In PyCharm we can display this 28x28 Image showing its Integer Numbers\n",
    "x_train[img_index]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59b82e06142a4a25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868fd2a4-a45a-4c5b-b789-b260ff9728c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to reshape all 60.000 images (and 10.000 test images) in order to give them one color-layer (not 3 color layers)\n",
    "x_train = x_train.reshape(60000, 28, 28, 1).astype('float32')\n",
    "x_test = x_test.reshape(10000, 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d1cd08-9232-4bd8-802b-15c5291e8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We normalize Gray Scale to float 0..1\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We now have an extra dimension for the gray layer.\n",
    "# The numbers stored ar floats\n",
    "type(x_train[0][0][0][0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34549ba581e612a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# An image is still an numpy array\n",
    "type(x_train[img_index])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4218bc415693d727"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Shape of a single image no longer is 28x28 but 28x28x1 (the gray layer)\n",
    "x_train[img_index].shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3206b747e0187dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we still can display this gray layer in PyCharm\n",
    "x_train[img_index].reshape(28,28)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a00218386d97696f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ef1d6a-26ee-45d0-9b26-629d870aa48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to one-hot-encode our Y values\n",
    "n_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f61e65-914b-434d-bb67-99b19ea26a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple LeNet implementation as found on d2l.ai\n",
    "model = Sequential([\n",
    "            Conv2D(filters=6, kernel_size=5, activation='sigmoid', padding='same', input_shape=(28,28,1)),\n",
    "            AvgPool2D(pool_size=2, strides=2),\n",
    "            Conv2D(filters=16, kernel_size=5, activation='sigmoid'),\n",
    "            AvgPool2D(pool_size=2, strides=2),\n",
    "            Flatten(),\n",
    "            Dense(120, activation='sigmoid'),\n",
    "            Dense(84, activation='sigmoid'),\n",
    "            Dense(n_classes, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9938189-a9dd-4497-b9cf-2bbf3e0f60ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecaac5f-d80e-45a5-8d8f-3d9aec7dba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow allows to creat a visual representation of our model.\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='../img/model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca6936-27ff-434d-9616-f78d81c7da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define two so called \"callback\" object.\n",
    "# these objects can be injected to the training process.\n",
    "# here: \n",
    "#   - the checkpointer callback collects the bestperforming model and stores the respective model parameters\n",
    "#   - the history callback collects all metrics from all epochs\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import History\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose = 1, save_best_only=True)                     \n",
    "history = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e5aed-f11c-4705-9e81-e460594faf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ecfcb5-225a-4a81-acb8-769b3686f84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.fit(x_train, y_train, batch_size=128, epochs=10, verbose=1\n",
    "                   #, validation_data=(x_valid, y_valid)\n",
    "                   , validation_split=0.2\n",
    "                   , callbacks = [history, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7710fe-fc2b-41fb-a536-45b6d8744b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edce943-b77a-4462-8ab5-10070f261c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(history.history['loss'], color='red', linestyle='--')\n",
    "ax1.plot(history.history['val_loss'], color='green', linestyle='--')\n",
    "plt.title('model performance')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'val loss', 'train acc', 'varl acc'], loc='upper left')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(history.history['accuracy'])\n",
    "ax2.plot(history.history['val_accuracy'])\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.legend(['train acc', 'val acc'], loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d32fc7-e852-46a0-8d78-4e6d1625ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights with the best validation accuracy\n",
    "model.load_weights('model.weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840929e-39c0-4126-866f-1a58e829b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test set\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e80fe-d3fb-4169-8188-e43eb87e03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2069a6e0-bf0a-44dd-a7f9-90ca122fefc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a random sample of 15 test images, their predicted labels and ground truth\n",
    "figure = plt.figure(figsize=(20, 8))\n",
    "for i, index in enumerate(np.random.choice(x_test.shape[0], size=15, replace=False)):\n",
    "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "    # Display each image\n",
    "    ax.imshow(np.squeeze(x_test[index]), cmap='gray_r')\n",
    "    predict_index = np.argmax(y_hat[index])\n",
    "    true_index = np.argmax(y_test[index])\n",
    "    # Set the title for each image\n",
    "    ax.set_title(\"{} ({})\".format(fashion_mnist_labels[predict_index], \n",
    "                                  fashion_mnist_labels[true_index]),\n",
    "                                  color=(\"green\" if predict_index == true_index else \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62b03b-52c7-4312-b3c5-aed1e84232ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0de75e-11dc-4c37-be30-57b97a9a28ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()  # should return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c46e401-44b4-4454-844b-016afc5c41e4",
   "metadata": {},
   "source": [
    "# other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba117ec5-ee65-416e-8bf6-774d1fec6679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6870553a-c682-4ccc-ac9e-24e1347c05cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47e9158-5e86-47ec-a4a9-888fc7b2cf25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "636494c8-0e91-4d4a-a9c9-bd60c80d09f7",
   "metadata": {},
   "source": [
    "# todos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955a70e-45e8-46ee-80cd-3c78e5729374",
   "metadata": {},
   "source": [
    "- zusätzliche Layer, wie dropout, normalization, regularisierung,...\n",
    "- augmentierung\n",
    "- transfer learning\n",
    "- überblick über architekturen\n",
    "- metriken\n",
    "- balancierte / unbalancierte sets\n",
    "- Auto tuner https://github.com/keras-team/keras-tuner\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa608ca-c902-47d5-bd50-2bbecd300ffb",
   "metadata": {},
   "source": [
    "# denkbare aufgaben\n",
    "- nachvollziehen: https://keras.io/guides/transfer_learning/\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de813dc-e57b-4947-97c4-823abd3f1f58",
   "metadata": {},
   "source": [
    "# NLP\n",
    "- https://www.tensorflow.org/hub\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
