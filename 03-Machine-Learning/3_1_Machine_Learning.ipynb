{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b5cea71a3770e00",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Maschinelles Lernen\n",
    "\n",
    "Nun kommen wir (endlich) zum eigentlichen Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e228721ba72c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T07:45:21.719675Z",
     "start_time": "2024-04-04T07:45:21.697984Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#import some necessary librairies\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "warnings.warn_explicit = ignore_warn\n",
    "\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae33a41f465d866f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Daten aus dem vorigen Schritt laden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe73b86950116b2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T07:45:23.370984Z",
     "start_time": "2024-04-04T07:45:23.363423Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/house-prices-advanced-regression-techniques/x_preprocessed_train.pkl', 'rb') as handle:\n",
    "    X_preprocessed_train = pickle.load(handle)\n",
    "\n",
    "with open('../data/house-prices-advanced-regression-techniques/y_train.pkl', 'rb') as handle:\n",
    "    y_preprocessed_train = pickle.load(handle)\n",
    "    \n",
    "with open('../data/house-prices-advanced-regression-techniques/x_test.pkl', 'rb') as handle:\n",
    "    X_preprocessed_test = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69512506a4704850",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Verfahren maschinellen Lernens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705aaa23960b2ddf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Supervised Learning\n",
    "\n",
    "<br>\n",
    "<img src=\"../img/ml-overview.png\" width=\"70%\" align=\"center\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1109601add608ffe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Supervised Learning\n",
    "Der Bereich des Supervised Learnings (das überwachte maschinelle Lernen) lässt sich in drei wichtige Bereiche unterteilen: die Klassifikation, die Regression und die Segmentierung. Für die Klassifikation kann die logistische Regression eingesetzt werden. Vor allem die Support Vector Machine (SVM) und verschiedene Decision-Tree-Verfahren sind hier als Alternativen hervorzuheben. Tiefe neuronale Netze sind in diesem Bereich die wichtigsten Vertreter. Beim überwachten maschinellen Lernen werden Algorithmen verwendet, um ein Modell zu trainieren, das Muster in einem Datensatz mit Bezeichnungen und Merkmalen findet. Das trainierte Modell wird sodann verwendet, um die Bezeichnungen für die Merkmale eines neuen Datensatzes vorherzusagen. Entscheidend hierbei ist, dass für das Training Annotationen – d. h. manuelle Zuordnungen zwischen Eingangsdaten und Ausgangsdaten – vorliegen. Ist das Modell trainiert, dann kann für unbekannte Daten („New Data“) durch die Benut-zung des Modells ein Label vorhergesagt werden („Use Model“).\n",
    "\n",
    "<br>\n",
    "<img src=\"../img/supervised.png\" width=\"70%\" align=\"center\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174256e0822b7362",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Prognose, Verlust und Optimierung\n",
    "Bei vielen Verfahren des maschinellen Lernens ist das Ziel, ausgehend von bekannten Eingangsdaten eine Vorhersage über eine unbekannte Zielgröße zu treffen. Beim überwachten Lernen trainiert man ein Modell mit Eingangsdaten, für die der „richtige“ Wert der Zielgröße bekannt ist, um das Modell dahingehend zu optimieren, dass es auch für Eingangsdaten die Zielgröße vorhersagen kann, zu denen diese nicht bekannt ist. Wichtige Begriffe sind Prognose, Verlust und Optimierung. Ausgegangen wird von Eingangsdaten, die in der Abbildung mit (1) bezeichnet sind.\n",
    "Das können je nach Verfahren Bilder, Töne, numerische Werte, Texte oder andere Daten (z. B. Sensordaten aus dem Betrieb von Maschinen) sein. Für diese Daten ist die Zielgröße (engl. Target) – auch als $Y$ bezeichnet – bekannt, in der Abbildung als (2) bezeichnet. Im Beispiel geht es um eine Klassifizierung, d. h. die Entscheidung, welche Zahl eine handgeschriebene Ziffer darstellt. Man spricht davon, dass die Eingangsdaten – auch als $X$ bezeichnet – einer bestimmten Klasse zugehören (hier sind dies die Klassen 0 bis 9). Man führt nun die Eingangsdaten dem Modell zu und berechnet mit Hilfe des Modells die Zielgröße. Dies wird als Prognose (3) bezeichnet und mit $\\hat{Y}$ gekennzeichnet. An anderen Stellen ist auch vom Abschätzen oder einer Vorhersage die Rede. Dies wird für alle Trainingsdaten wiederholt. In Schritt (4) wird nun für alle Ergebnisse $Y$ mit $\\hat{Y}$ verglichen und die „Abweichung“ zwischen $Y$ und $\\hat{Y}$ für diesen Durchlauf ermittelt. Diese Abweichung bezeichnet man als Verlust. Hierfür kommt die Verlustfunktion zum Einsatz, mit deren Hilfe diese Abweichung berechnet werden kann. Das Ergebnis der Verlustfunktion wird nun genutzt, um die internen Parameter – die so genannten Modellparameter – derart zu verändern, dass im nächsten Durchlauf der Verlust verringert wird. Dieses Verfahren wird Optimierung genannt und in der Abbildung mit (5) bezeichnet. Mit Hilfe eine Optimierungsverfahrens, z. B. dem so genannten Gradientenabstieg, können nun die Modellparameter optimiert werden. Ergänzend sei erwähnt, dass es neben dem Gradientenabstieg viele weitere Optimierungsverfahren gibt, die unterschiedliche Eigenschaften, Vor- und Nachteile haben.\n",
    "\n",
    "<br>\n",
    "<img src=\"../img/prozess.png\" width=\"70%\" align=\"center\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c662cd1e6552bc89",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Regression\n",
    "\n",
    "## Interpolation\n",
    "\n",
    "<br>\n",
    "<img src=\"../img/interpolation.png\" width=\"70%\" align=\"center\">\n",
    "<br>\n",
    "\n",
    "## Extrapolation\n",
    "\n",
    "<img src=\"../img/extrapolation.png\" width=\"70%\" align=\"center\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29098cc9716d61",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Decision Tree\n",
    "\n",
    "Mit Hilfe von Decision-Tree-Algorithmen werden aus den Attributen der annotierten Daten binäre Bäume erstellt, entlang derer ein Result (also die Annotation) entweder wahr oder falsch ist. Hier-durch kann ein Result über mehrere Äste des Baums als wahr oder falsch erkannt werden. In der Abbildung wird ein solcher Decision Tree vorgestellt. Im Beispiel wird die Vorhersage von Kaufpreisen von Häusern dargestellt. Die zugrunde liegenden Daten werden im gezeigten Decision Tree angeordnet und können dann zum Training und schließlich zur Vorhersage verwendet werden. Decision Trees können sowohl zur Klassifizierung als auch zur Interpolation kontinuierlicher – also reeller – Werte herangezogen werden. Das Ergebnis eines Decision Tree ist also entweder eine Ganzzahl (die „Nummer“ der gefundenen Klasse) oder eine Fließkommazahl. \n",
    "\n",
    "\n",
    "<img src=\"../img/decisiontree.png\" width=\"70%\" align=\"center\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca5fb6-2cb6-4ba1-942c-1399b0241a1f",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "Bagging ist eine Methode des so genannten Ensemble Learnings. Hierbei wird nicht nur eine Hypothese – also eine Prognose anhand eines trainierten Zustandes eines Modells – zur Vorhersage herangezogen, sondern eine Sammlung solcher Hypothesen. Beim Bagging werden zufällig $N$ Samples (Muster)  aus den Trainingsdaten entnommen, wobei bei jedem zufälligen Ziehen der Muster jeder Datensatz erneut gezogen werden darf. Es kann also Überlappungen in den gezogenen Daten geben. Dann wird das Modell mit diesen $N$ Mustern trainiert, um die erste Hypothese zu erhalten. Weitere $K$ Hypothesen entstehen durch die zufällige Auswahl von K Mustergruppen (je bestehend aus $N$ einzelnen Mustern, also Trainingsdaten). Wird nun das Modell angewendet, werden alle so trainierten $K$ Hypothesen für eine Prognose herangezogen. Bei Klassifizierungsaufgaben wird dann eine Mehrheitsentscheidung dieser Prognosen berechnet, d. h., diejenige Klasse, für die die meisten Hypothesen „stimmen“, wird als Ergebnis zurückgegeben. Bei Regressionsproblemen wird der Durchschnitt der Hypothesen gebildet:\n",
    "\n",
    "$$h(x)\\ =\\ \\frac{1}{K}\\sum_{i=1}^{K}{h_i(x)}$$\n",
    "\n",
    "Hierbei ist $h_i$ die Hypothese bezogen auf den (unbekannten) Beispieldatensatz $x$. Das Bagging kann gut angewendet werden, wenn wenig Trainingsdaten zur Verfügung stehen, und führt zu weniger Varianz in den Ergebnissen. Darüber hinaus kann das Verfahren gut parallelisiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b4b6c-792f-48ac-ab29-d964c56dce4a",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Beim Boosting wird ähnlich wie beim Bagging vorgegangen. Allerdings wird nicht für jede Hypothese eine neue zufällige Sammlung von Mustern aus den Trainingsdaten entnommen, sondern jede weitere Hypothese $h_{i+1}$ wird aus der Vorgängerhypothese $h_i$ abgeleitet. Beim Boosting wird jedem Datensatz der Trainingsdaten ein Gewicht $w_j\\geq0$ zugeordnet, das festlegt, „wie wichtig“ es für das Training sein soll. \n",
    "Zunächst erhält jedes Muster $x_j$ (jeder Datensatz der Trainingsdaten) das Gewicht $w_j=1$. Man kann sich das Gewicht derart vorstellen, dass es die Häufigkeit bezeichnet, mit der ein Trainingsdatensatz beim Training wiederholt verwendet wird. Ein großes Gewicht erhöht also die Bedeutung eines Datensatzes für das Training. Wurde nun durch das Training eine Hypothese $h_1$ ermittelt, so wird diese Hypothese falsche Ergebnisse für einige der Trainingsdaten ermitteln. Solche Trainingsdaten sollen durch die Vergrößerung des jeweiligen Gewichts stärkeren Einfluss auf das Training erhalten, die übrigen ein geringeres. Anhand dieses gewichteten Trainingssatzes wird nun erneut ein Training durchgeführt, das eine neue Hypothese $h_2$ hervorbringt. Dies wird so lange wiederholt, bis $K$ Hypothesen – mithin trainierte Modelle – vorliegen. Diese Vorgehensweise hat zur Folge, dass ein Modell stärker mit „schwierigen“ Datensätzen umgehen lernen muss. Diese Vorgehensweise ist ein Greedy-Algorithmus  – d. h. ein Algorithmus, der versucht, als nächsten Schritte immer den im Moment als besten Schritt erscheinenden auszuführen, denn es erfolgt kein Backtracking; eine einmal gefundene Hypothese wird nicht nachträglich verworfen, sollte sie sich als weniger geeignet herausstellen. Stehen nun $K$ Hypothesen fest, werden erneut Parameter vergeben – diesmal je ein $z_i$ für jede Hypothese $h_i$. Für eine Regression oder Klassifikation ergibt sich:\n",
    "$$h(x)\\ =\\ \\sum_{i=1}^{K}z_i\\bullet h_i(x)$$\n",
    "\n",
    "Ein bedeutender Boosting-Algorithmus ist der AdaBoost-Algorithmus. Dieser hat die besondere Eigenschaft, dass er – sofern nur der zugrunde liegende Modellalgorithmus mindestens bessere Ergebnisse liefert, als es das reine Raten ergeben würde – mit jeder Iteration für $h_{j+1}$ bessere Ergebnisse als sein Vorgänger liefert. D. h., der Boosting-Algorithmus verstärkt („boosted“) den ursprünglichen ungewichteten Algorithmus, verbessert mithin dessen Eigenschaften.\n",
    "\n",
    "Das Boosting ist allerdings nicht nur ein Greedy-Algorithmus, sondern auch ein lineares Verfahren – jede folgende Hypothese ergibt sich aus der vorherigen – und kann daher nicht parallelisiert werden. Daher wurde das Gradient Boosting entwickelt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df9c0ef6237253",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Gradient Boosted Decision Trees (GBDT)\n",
    "\n",
    "Bei GBDT – Gradient Boosted Decision Trees – werden mehrere solcher Entscheidungsbäume kombiniert. Die Idee dahinter ist, dass über ein Gradient-Descent-Verfahren der Verlust mehrerer für sich genommen möglicherweise schwacher Decision-Tree-Modelle insgesamt der Verlust minimiert werden kann.\n",
    "\n",
    "Beim Gradient Boosting werden nicht mehr spezielle Datensätze besonders (durch die Gewichte) berücksichtigt, sondern es wird der Gradient zwischen den korrekt vorhergesagten (klassifizierten, berechneten) Mustern und den Antworten der vorherigen Hypothese betrachtet. Für die Optimierung, dem Gradientenabstieg, wird eine differenzierbare Verlustfunktion benötigt; das kann für eine Klassifizierung der „squared error“ und für die Regression der „logarithmische Verlust“ sein. Anstatt die Parameter des Modells mit Hilfe des Gradientenabstiegs zu optimieren, werden – in Richtung der stärksten Verringerung des Gradienten – die Parameter der nächsten Hypothese, also des nächsten Modells, angepasst. \n",
    "\n",
    "# XGBoost\n",
    "\n",
    "Der XGBoost-Algorithmus ist eine Implementierung des GBDT für hochparallele Verarbeitung z. B. auf GPU-Prozessoren, die gegenüber CPU-Prozessoren sehr viel mehr Recheneinheiten zur Verfügung haben, die derartige Optimierungsprozesse – also Lernprozesse – parallelisieren und damit die Geschwindigkeit des Trainings steigern können. XGBoost ist eine Open-Source-Implementierung, die Gradient Boosting zusammen mit Pruning und Regularisierung einsetzt.\n",
    "\n",
    "<img src=\"../img/gbdt.png\" width=\"60%\" align=\"center\">\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25d24443583de9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Laden der Daten und Aufteilung in Train und Test Daten\n",
    "\n",
    "Um die Güte eines trainierten Modells zu testen, werden sogenannte Test-Daten zurückgehalen. Diese Daten hat das Modell noch nie gesehen. So kann gemessen werden, wie gut das Netz generalisiert, also mit unbekannten Daten umgehen kann.\n",
    "\n",
    "**Cross Validation**\n",
    "Mit der n-fachen Kreuzvalidierung werden innerhalb der Validierungsdaten nun zwei Subsets definiert: Das Trainings-Set und das Validieruns-Set. Dieses wird über den gesamten Traingings-Datenbestand mutiert.\n",
    "\n",
    "<img src=\"../img/test_train_split.png\" width=\"70%\" align=\"center\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa6bd94fb1b805",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T08:28:22.593361Z",
     "start_time": "2024-04-04T08:28:22.011616Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 4354 # Random_state ist ein seed, damit gegebenenfalls immer mit der selben pseudo Random Folge gearbeitet wird.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed_train, y_preprocessed_train, test_size=0.2, random_state=seed) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82cd41f1c844991",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Grid-Search\n",
    "Machine Learning Modelle benötigen in der Regel Hyperparameter, d.h. Steurungsparameter, die außerhalb der eigenentlichen internen (angelernten) Modell-Parameter liegen.\n",
    "Diese können über eine systematische Suche gefunden werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4013fa58-0b02-49ab-afcb-251423345b39",
   "metadata": {},
   "source": [
    "## Lineare Regression\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "Die Lineare Regression ist ein lineares Modell mit Koeffizienten w = (w1, ..., wp), um die Residualquadratsumme zwischen den beobachteten Zielwerten y im Datensatz und den durch die lineare Approximation vorhergesagten Zielen zu minimieren.\n",
    "\n",
    "Eine einfache Lineare Regression stellt sich dar als:\n",
    "$$y=a+b\\bullet x$$\n",
    "\n",
    "Um b zu bestimmen, berechnet man:\n",
    "$$b=\\frac{\\sum_{i=1}^{n}{(x_i-\\bar{x})\\ \\bullet}\\ (y_i-\\bar{y})}{\\sum_{i=1}^{n}{{(x}_i-\\bar{x)}}^2}$$\n",
    "\n",
    "Um a zu bestimmen, berechnet man:\n",
    "$$a=\\bar{y}\\ -\\ b\\ \\bullet\\ \\bar{x}$$\n",
    "\n",
    "$\\bar{x}$ und $\\bar{y}$ sind die jeweiligen Mittelwerte aller bekannten Datenpunkte.\n",
    "\n",
    "**Die Lineare Regression besitzt keine Hyperparameter.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b76b9c9-a76c-421c-b4d2-3edb6d89fbf3",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn-ensemble-randomforestregressor\n",
    "\n",
    "Ein Random Forest Regressor ist ein Meta-Modell, das eine Reihe von Decision-Tree-Regressoren auf verschiedene Teil-Stichproben des Datensatzes anpasst und eine Mittelwertbildung verwendet, um die Vorhersagegenauigkeit zu verbessern und das Overfitting zu kontrollieren. Die Bäume im Random Forest verwenden die \"best split\"-Strategie, d. h. sie übergeben splitter=\"best\" an den zugrunde liegenden DecisionTreeRegressor. Die Größe der Teil-Stichproben wird mit dem Parameter max_samples gesteuert, wenn bootstrap=True (Standard), andernfalls wird der gesamte Datensatz zur Erstellung jedes Baums verwendet.\n",
    "\n",
    "**In diesem Beispiel werden folgende Hyperparameter optimiert:**\n",
    "* n_estimators: Die Anzahl der Teil-Bäume im Random Forest\n",
    "* max_depth: Die maximale Tiefe, in der ein Teil-Baum durchsucht wird oder \"None\" für die Suche bis keine weiteren Splits mehr möglich sind.\n",
    "* min_samples_split: Die Mindestanzahl von Stichproben, die erforderlich ist, um einen internen Knoten zu teilen.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc4f84a-f05e-40c1-a234-eada7b79215a",
   "metadata": {},
   "source": [
    "## XGBoost Regressor\n",
    "https://xgboost.readthedocs.io/en/stable/\n",
    "\n",
    "**In diesem Beispiel werden folgende Hyperparameter optimiert:**\n",
    "* n_estimators: Die Anzahl der boosted trees.\n",
    "* max_depth: Die maximale Tiefe, in der ein Teil-Baum durchsucht wird oder \"None\" für die Suche bis keine weiteren Splits mehr möglich sind.\n",
    "* learning_rate: Boosting learning rate (xgb’s “eta”): Beim Gradient Boosting werden nacheinander Bäume erstellt und dem Modell hinzugefügt. Neue Bäume werden erstellt, um die \"residual errors\" in den Vorhersagen aus der bestehenden Serie von Bäumen zu korrigieren. Dies hat zur Folge, dass das Modell schnell konvergiert aber ebenso schnell overfitten kann. Eine Technik zur Verlangsamung des Lernprozesses im Gradient-Boosting-Modell besteht darin, einen Gewichtungsfaktor für die Korrekturen durch neue Bäume anzuwenden, wenn diese dem Modell hinzugefügt werden. Diese Gewichtung wird als **Lernrate** (oder shrinkage factor) bezeichnet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd720f3012f98e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T08:28:25.726298Z",
     "start_time": "2024-04-04T08:28:25.709164Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Wir wollen drei Modelle untersuchen\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'RandomForest': RandomForestRegressor(random_state=seed),\n",
    "    'XGBoost': XGBRegressor(random_state=seed)\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    'LinearRegression': {},\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [None, 10, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'max_depth': [3, 6, 10],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14496a0b2dbb5495",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Festlegung der Kreuzvalidierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c631ad1b90cd5358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T08:28:28.717632Z",
     "start_time": "2024-04-04T08:28:28.701647Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 3-fold cross-validation\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bfae89-b16d-4700-b884-dd3ded2943f3",
   "metadata": {},
   "source": [
    "**Beachten Sie:**\n",
    "\n",
    "Das CV Objekt wird später der GridSearchCV übergeben. Dies veranlasst die GridSearchCV Klasse, für jeden \"Durchlauf\" zusätzlich die definierten Ramdom-Splits durchzuführen. D.h. \n",
    "\n",
    "* Erster Durchlauf: Durchprobieren aller Grid-Werte für den **1sten** CV (CrossValidation) Sample Split\n",
    "* Zweiter Durchlauf: Durchprobieren aller Grid-Werte für den **2sten** CV (CrossValidation) Sample Split\n",
    "* usw.\n",
    "\n",
    "Für jeden Sample Split werden /Anzahl der Samples/ % /n_splits/ Samples aus den Trainingsdaten entnommen und als Validierungsset verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ae4bb8-534a-40c2-9c3c-9bc1756fc5c8",
   "metadata": {},
   "source": [
    "# Güte-Metrik(en)\n",
    "**WICHTIG:** Beachten Sie, dass nicht jede Güte-Metrik für jedes Modell geeignet ist. Siehe:\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "## MSE - Mean Squared Error\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error\n",
    "\n",
    "Der MSE ist der mittlere quadratische Fehler, eine Risikometrik, die dem Erwartungswert des quadratischen Fehlers oder Verlusts entspricht.\n",
    "Wenn ${\\hat{y}}_i$ der vorhergesagte Wert der $i$-ten Stichprobe und $y_i$ der entsprechende wahre Wert ist, dann ist der mittlere quadratische Fehler (MSE), der über $n_{samples}$ geschätzt wird, definiert als\n",
    "$$MSE(y,\\hat{y})\\ =\\ \\frac{1}{n_{samples}}\\ \\sum_{i=0}^{n_{samples}-1}{(y_i-{\\hat{y}}_i)}^2$$\n",
    "\n",
    "\n",
    "## RMSE Root Mean Squared Error\n",
    "Wurzel aus dem mittleren quadratischen Fehler:\n",
    "\n",
    "$$RMSE(y,\\hat{y})\\ =\\ \\sqrt{MSE(y,\\hat{y})}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989bb9d061a991cc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Training, Gridsearch und Kreuzvalidierung\n",
    "\n",
    "Training, Gridsearch und Kreuzvalidierung können dank der vorbereiteten Funktion in scikit-learn einfach ausgeführt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e01afd80f05dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T08:29:22.979893Z",
     "start_time": "2024-04-04T08:28:33.513248Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Train and tune the models\n",
    "grids = {}\n",
    "for model_name, model in models.items():\n",
    "    #print(f'Training and tuning {model_name}...')\n",
    "    grids[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "    grids[model_name].fit(X_train, y_train)\n",
    "    best_params = grids[model_name].best_params_\n",
    "    best_score = np.sqrt(-1 * grids[model_name].best_score_)\n",
    "    \n",
    "    print(f'Optimale Parameter für {model_name}: {best_params}')\n",
    "    print(f'Bester RMSE Score für {model_name}: {best_score}\\n') #Root-mean-square error\n",
    "    \n",
    "# Vgl. https://www.kaggle.com/code/kenjee/housing-prices-example-with-video-walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b752214a2854b8d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T15:52:17.906704Z",
     "start_time": "2024-04-02T15:52:17.902070Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "best_params = grids['XGBoost'].best_params_\n",
    "best_score = np.sqrt(-1 * grids['XGBoost'].best_score_)\n",
    "\n",
    "print(f'Optimale Parameter für XGBoost: {best_params}')\n",
    "print(f'Bester RMSE Score für XGBoost: {best_score}\\n') #Root-mean-square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c280ef-a1dc-410d-bd41-9469ab0958e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grids['XGBoost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98fb503-6e6f-42dd-b13e-6f96adf5f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grids['XGBoost']\n",
    "y_hat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58ec51-9138-4859-823c-0f088bc95bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "#calculate RMSE\n",
    "sqrt(mean_squared_error(y_test, y_hat)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1645ccb7-0cce-423e-8e48-f167bdb90318",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_vals=np.random.choice(X_test.shape[0], size=15, replace=False)\n",
    "rand_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af383f25-1c64-4988-aa8f-73472c9747b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(rand_vals):\n",
    "    predict_index = y_hat[index]\n",
    "    true_index = y_test.to_numpy()[index]\n",
    "    print (f'{index}: predict={predict_index} / true={true_index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c340072-dc3c-471f-8d59-afcadb382818",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "plt.title('Prediction Delta')\n",
    "plt.ylabel('Delta')\n",
    "plt.xlabel('Index')\n",
    "ax1.plot(np.subtract(y_test.to_numpy(),y_hat), color='red', linestyle='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b2e8e-f254-47ba-9c6a-774f800a0e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe666cd-44ad-4411-9c93-ce467272f6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
