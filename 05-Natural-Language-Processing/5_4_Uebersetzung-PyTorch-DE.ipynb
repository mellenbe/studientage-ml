{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76df21ac-003c-4667-a29b-7c97a9c7a42b",
   "metadata": {},
   "source": [
    "# Maschinelle √úbersetzung\n",
    "[Credits to: https://github.com/huggingface/notebooks/blob/main/examples/translation.ipynb]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c35f65",
   "metadata": {},
   "source": [
    "Vergewissern Sie sich, dass Ihre Version von Transformers mindestens 4.11.0 ist, da die Funktion erst mit dieser Version eingef√ºhrt wurde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0ee2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T06:58:00.414166Z",
     "start_time": "2024-04-13T06:57:57.183451Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886acfcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T06:58:01.575252Z",
     "start_time": "2024-04-13T06:58:01.566002Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from pathlib import PurePath\n",
    "\n",
    "WORKING_PATH=Path(Path(os.getcwd()).parent, Path(\"data\"), Path(\"translate\"))\n",
    "\n",
    "# shutil.rmtree(WORKING_PATH, ignore_errors=True)\n",
    "os.makedirs(WORKING_PATH, 0o777, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe201011",
   "metadata": {
    "id": "HFASsisvIrIb"
   },
   "source": [
    "Eine Skriptversion dieses Notizbuchs zur verteilten Feinabstimmung des Modells mit mehreren GPUs oder TPUs finden Sie [hier](https://github.com/huggingface/transformers/tree/master/examples/seq2seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e1e19e",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Feinabstimmung eines Modells f√ºr eine √úbersetzungsaufgabe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d100abc4",
   "metadata": {
    "id": "kTCFado4IrIc"
   },
   "source": [
    "In diesem Notizbuch werden wir sehen, wie man ein Modell des [ü§ó Transformers](https://github.com/huggingface/transformers) f√ºr eine √úbersetzungsaufgabe feinabstimmen kann. Wir werden den [WMT-Datensatz](http://www.statmt.org/wmt16/) verwenden, einen maschinellen √úbersetzungsdatensatz, der aus einer Sammlung verschiedener Quellen besteht, darunter Nachrichtenkommentare und Parlamentsprotokolle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8cc97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T06:58:09.507741Z",
     "start_time": "2024-04-13T06:58:09.492189Z"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-ro\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660dfa55",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "Dieses Notebook ist so aufgebaut, dass es mit jedem model checkpoint  aus dem [Model Hub](https://huggingface.co/models) l√§uft, solange das Modell eine Sequenz-zu-Sequenz-Version in der Transformers-Bibliothek hat. Hier haben wir den Kontrollpunkt [`Helsinki-NLP/opus-mt-en-ro`](https://huggingface.co/Helsinki-NLP/opus-mt-en-ro) gew√§hlt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bae5b1d",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Laden des Datensatzes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ffc7b",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "Wir werden die [ü§ó Datasets](https://github.com/huggingface/datasets)-Bibliothek verwenden, um die Daten herunterzuladen und die Metrik zu erhalten, die wir f√ºr die Auswertung verwenden m√ºssen (um unser Modell mit dem Benchmark zu vergleichen). Dies kann einfach mit den Funktionen `load_dataset` und `load_metric` durchgef√ºhrt werden. Wir verwenden hier den englischen/rum√§nischen Teil des WMT-Datensatzes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de330e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:01:43.767835Z",
     "start_time": "2024-04-13T07:01:14.815345Z"
    },
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "metric = evaluate.load(\"sacrebleu\", trust_remote_code=True)\n",
    "\n",
    "raw_datasets = load_dataset(\"wmt16\", \"ro-en\")\n",
    "metric = evaluate.load(\"sacrebleu\", trust_remote_code=True)\n",
    "\n",
    "# Was ist Sacrebleu: https://huggingface.co/spaces/evaluate-metric/sacrebleu\n",
    "# was ist ein bleu score: https://www.geeksforgeeks.org/nlp-bleu-score-for-evaluating-neural-machine-translation-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adff8689",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "Das `dataset`-Objekt selbst ist [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), das einen Schl√ºssel f√ºr den Trainings-, Validierungs- und Testsatz enth√§lt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e65f685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:02:32.667141Z",
     "start_time": "2024-04-13T07:02:32.654968Z"
    },
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
   },
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bba86f",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "Um auf ein tats√§chliches Element zuzugreifen, m√ºssen Sie zuerst einen Split ausw√§hlen und dann einen Index angeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cfbfec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:03:06.065873Z",
     "start_time": "2024-04-13T07:03:06.034622Z"
    },
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e104c",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "Um ein Gef√ºhl daf√ºr zu bekommen, wie die Daten aussehen, zeigt die folgende Funktion einige zuf√§llig ausgew√§hlte Beispiele aus dem Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8744db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:03:24.581198Z",
     "start_time": "2024-04-13T07:03:24.566692Z"
    },
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Es k√∂nnen nicht mehr Elemente ausgew√§hlt werden, als im Dataset vorhanden sind.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af4c8f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T19:08:41.192125Z",
     "start_time": "2024-04-11T19:08:41.176494Z"
    },
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [],
   "source": [
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b573cc6d",
   "metadata": {
    "id": "lnjDIuQ3IrI-"
   },
   "source": [
    "Die Metrik ist eine Instanz von [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8826aaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:03:45.051258Z",
     "start_time": "2024-04-13T07:03:45.035628Z"
    }
   },
   "outputs": [],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae03a0f6",
   "metadata": {
    "id": "jAWdqcUBIrJC"
   },
   "source": [
    "Sie k√∂nnen die Methode `compute` mit Ihren Vorhersagen und Beschriftungen aufrufen, die eine Liste von dekodierten Strings sein m√ºssen (Liste von Listen f√ºr die Beschriftungen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647e0ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:04:25.450043Z",
     "start_time": "2024-04-13T07:04:25.387541Z"
    },
    "id": "6XN1Rq0aIrJC",
    "outputId": "a4405435-a8a9-41ff-9f79-a13077b587c7"
   },
   "outputs": [],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [[\"hello there\"], [\"general kenobi\"]]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf78b5",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73e8028",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "Bevor wir diese Texte in unser Modell einspeisen k√∂nnen, m√ºssen wir sie vorverarbeiten. Dies geschieht durch einen ü§ó Transformers `Tokenizer`, der (wie der Name schon sagt) die Eingaben tokenisiert (einschlie√ülich der Konvertierung der Token in ihre entsprechenden IDs im vortrainierten Vokabular) und in ein Format bringt, das das Modell erwartet, sowie die anderen Eingaben generiert, die das Modell ben√∂tigt.\n",
    "\n",
    "Um all dies zu tun, instanziieren wir unseren Tokenizer mit der Methode `AutoTokenizer.from_pretrained`, die daf√ºr sorgt, dass:\n",
    "\n",
    "- wir einen Tokenizer erhalten, der der Modellarchitektur entspricht, die wir verwenden wollen,\n",
    "- wir das Vokabular herunterladen, das beim Vortraining dieses speziellen Pr√ºfpunkts verwendet wurde.\n",
    "\n",
    "Dieses Vokabular wird zwischengespeichert, so dass es beim n√§chsten Aufruf der Zelle nicht erneut heruntergeladen werden muss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba517ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:05:26.849455Z",
     "start_time": "2024-04-13T07:05:26.389105Z"
    },
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3cb05",
   "metadata": {},
   "source": [
    "F√ºr den mBART-Tokenizer (wie hier) m√ºssen wir die Ausgangs- und die Zielsprache festlegen (damit die Texte richtig vorverarbeitet werden). Sie k√∂nnen die Sprachcodes [hier](https://huggingface.co/facebook/mbart-large-cc25) √ºberpr√ºfen, wenn Sie dieses Notebook f√ºr ein anderes Sprachenpaar verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a48641",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:05:51.034384Z",
     "start_time": "2024-04-13T07:05:51.003135Z"
    }
   },
   "outputs": [],
   "source": [
    "if \"mbart\" in model_checkpoint:\n",
    "    tokenizer.src_lang = \"en-XX\"\n",
    "    tokenizer.tgt_lang = \"ro-RO\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b4a18",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "Standardm√§√üig wird der obige Aufruf einen der schnellen Tokenizer (unterst√ºtzt durch Rust) aus der ü§ó Tokenizers-Bibliothek verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cebaed",
   "metadata": {
    "id": "rowT4iCLIrJK"
   },
   "source": [
    "Sie k√∂nnen diesen Tokenizer direkt f√ºr einen Satz oder ein Satzpaar aufrufen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25954776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:07:35.931947Z",
     "start_time": "2024-04-13T07:07:35.900423Z"
    },
    "id": "a5hBlsrHIrJL",
    "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90"
   },
   "outputs": [],
   "source": [
    "tokenizer(\"Hello, this one sentence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf090eec",
   "metadata": {
    "id": "qo_0B1M2IrJM"
   },
   "source": [
    "Je nachdem, welches Modell Sie ausgew√§hlt haben, sehen Sie unterschiedliche Schl√ºssel im W√∂rterbuch, das von der obigen Zelle zur√ºckgegeben wird. Sie sind f√ºr das, was wir hier tun, nicht von gro√üer Bedeutung (Sie m√ºssen nur wissen, dass sie von dem Modell, das wir sp√§ter instanziieren werden, ben√∂tigt werden). Sie k√∂nnen mehr √ºber sie in [diesem Tutorial] (https://huggingface.co/transformers/preprocessing.html) erfahren, wenn Sie daran interessiert sind.\n",
    "\n",
    "Anstelle eines Satzes k√∂nnen wir auch eine Liste von S√§tzen weitergeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6849dc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:07:40.853867Z",
     "start_time": "2024-04-13T07:07:40.838235Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05606b",
   "metadata": {},
   "source": [
    "Um die Ziele f√ºr unser Modell vorzubereiten, m√ºssen wir sie innerhalb des Kontextmanagers \"as_target_tokenizer\" tokenisieren. Dadurch wird sichergestellt, dass der Tokenizer die speziellen Token verwendet, die den Zielen entsprechen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9158e19a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:07:50.889774Z",
     "start_time": "2024-04-13T07:07:50.858474Z"
    }
   },
   "outputs": [],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e719e72e",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "Wenn Sie eine der f√ºnf T5-checkpoints verwenden, die ein spezielles Pr√§fix vor den inputs erfordern, sollten Sie die folgende Zelle anpassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b1ddb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:09:55.747178Z",
     "start_time": "2024-04-13T07:09:55.715921Z"
    }
   },
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"translate English to Romanian: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1644c",
   "metadata": {},
   "source": [
    "Wir k√∂nnen dann die Funktion schreiben, die unsere Trainingsbeispiele vorverarbeitet. Wir geben sie einfach mit dem Argument \"truncation=True\" an den \"Tokenizer\" weiter. Dadurch wird sichergestellt, dass eine Eingabe, die l√§nger ist als das ausgew√§hlte Modell verarbeiten kann, auf die vom Modell akzeptierte H√∂chstl√§nge gek√ºrzt wird. Das Auff√ºllen wird sp√§ter (in einem data collator) behandelt, so dass wir die Beispiele auf die l√§ngste L√§nge im batch und nicht auf den gesamten Datensatz auff√ºllen (pad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a743e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:12:18.711970Z",
     "start_time": "2024-04-13T07:12:18.696234Z"
    },
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"ro\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcb2f3e",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "Diese Funktion funktioniert mit einem oder mehreren Beispielen. Bei mehreren Beispielen gibt der Tokenizer eine Liste von Listen f√ºr jeden Schl√ºssel zur√ºck:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feaf84d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:12:25.962928Z",
     "start_time": "2024-04-13T07:12:25.947283Z"
    },
    "id": "-b70jh26IrJS",
    "outputId": "acd3a42d-985b-44ee-9daa-af5d944ce1d9"
   },
   "outputs": [],
   "source": [
    "preprocess_function(raw_datasets['train'][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e99bf5",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "Um diese Funktion auf alle Satzpaare in unserem Datensatz anzuwenden, verwenden wir einfach die `map`-Methode unseres `dataset`-Objekts, das wir zuvor erstellt haben. Dadurch wird die Funktion auf alle Elemente aller Splits in `dataset` angewendet, so dass unsere Trainings-, Validierungs- und Testdaten mit einem einzigen Befehl vorverarbeitet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f37e16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:12:38.423787Z",
     "start_time": "2024-04-13T07:12:35.612709Z"
    },
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad60b5",
   "metadata": {
    "id": "voWiw8C7IrJV"
   },
   "source": [
    "Noch besser ist, dass die Ergebnisse automatisch von der ü§ó Datasets-Bibliothek zwischengespeichert werden, damit Sie bei der n√§chsten Ausf√ºhrung Ihres Notebooks keine Zeit f√ºr diesen Schritt aufwenden m√ºssen. Die ü§ó Datasets-Bibliothek ist normalerweise intelligent genug, um zu erkennen, wenn sich die Funktion, die Sie an map √ºbergeben, ge√§ndert hat (und daher die Cache-Daten nicht verwendet werden m√ºssen). Sie erkennt zum Beispiel, wenn Sie die Aufgabe in der ersten Zelle √§ndern und das Notizbuch neu starten. Datasets warnt Sie, wenn es zwischengespeicherte Dateien verwendet. Sie k√∂nnen `load_from_cache_file=False` im Aufruf von `map` √ºbergeben, damit die zwischengespeicherten Dateien nicht verwendet werden und die Vorverarbeitung erneut durchgef√ºhrt wird.\n",
    "\n",
    "Beachten Sie, dass wir `batched=True` √ºbergeben haben, um die Texte stapelweise zusammen zu kodieren. Dies dient dazu, die Vorteile des schnellen Tokenizers, den wir zuvor geladen haben, voll auszunutzen, der Multi-Threading verwendet, um die Texte in einem batch gleichzeitig zu behandeln."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba9a634",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning des Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86aea22",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "Nun, da unsere Daten fertig sind, k√∂nnen wir das trainierte Modell herunterladen und feinabstimmen. Da es sich bei unserer Aufgabe um eine Sequenz-zu-Sequenz-Aufgabe handelt, verwenden wir die Klasse `AutoModelForSeq2SeqLM`. Wie beim Tokenizer wird die Methode `from_pretrained` das Modell f√ºr uns herunterladen und zwischenspeichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66eef0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:14:11.395515Z",
     "start_time": "2024-04-13T07:14:09.307903Z"
    },
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4750e61d",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "Beachten Sie, dass wir keine Warnung wie in unserem Klassifizierungsbeispiel erhalten. Das bedeutet, dass wir alle Gewichte des vorher trainierten Modells verwendet haben und es in diesem Fall keinen zuf√§llig initialisierten Kopf gibt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3123a9",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "Um einen `Seq2SeqTrainer` zu instanziieren, m√ºssen wir drei weitere Dinge definieren. Das Wichtigste ist die Klasse [`Seq2SeqTrainingArguments`] (https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments), die alle Attribute enth√§lt, um das Training anzupassen. Sie ben√∂tigt einen Ordnernamen, der zum Speichern der Checkpoints des Modells verwendet wird, und alle anderen Argumente sind optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c2d27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:14:23.463229Z",
     "start_time": "2024-04-13T07:14:23.433585Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = str(WORKING_PATH).replace(\"\\\\\",\"/\") + \"/studientage_translate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c34c8-1594-4f0f-91a8-a587a0940add",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:15:21.343878Z",
     "start_time": "2024-04-13T07:15:21.281127Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "cuda_available = torch.cuda.is_available()\n",
    "cuda_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b31925",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:17:38.843155Z",
     "start_time": "2024-04-13T07:17:38.697747Z"
    },
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_name, #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=cuda_available,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49cf291",
   "metadata": {
    "id": "km3pGVdTIrJc"
   },
   "source": [
    "Hier stellen wir ein, dass die Auswertung am Ende jeder Epoche erfolgt, passen die Lernrate an, verwenden die oben in der Zelle definierte `batch_size` und passen den weight_decay an. Da der `Seq2SeqTrainer` das Modell regelm√§√üig speichert und unser Datensatz recht gro√ü ist, weisen wir ihn an, maximal drei Speicherungen vorzunehmen. Schlie√ülich verwenden wir die Option `predict_with_generate` (um Zusammenfassungen korrekt zu generieren) und aktivieren das Training mit gemischter Pr√§zision (um etwas schneller zu sein).\n",
    "\n",
    "Das letzte Argument, um alles einzurichten, damit wir das Modell w√§hrend des Trainings regelm√§√üig an den [Hub](https://huggingface.co/models) senden k√∂nnen. Entfernen Sie es, wenn Sie die Installationsschritte am Anfang des Notizbuchs nicht befolgt haben. Wenn Sie Ihr Modell lokal unter einem anderen Namen als dem des Repositories, in das es gepusht werden soll, speichern wollen, oder wenn Sie Ihr Modell unter einer Organisation und nicht unter Ihrem Namensraum pushen wollen, verwenden Sie das Argument `hub_model_id`, um den Namen des Repositories festzulegen (es muss der vollst√§ndige Name sein, einschlie√ülich Ihres Namensraums: zum Beispiel `\"sgugger/marian-finetuned-en-to-ro\"` oder `\"huggingface/marian-finetuned-en-to-ro\"`).\n",
    "\n",
    "Dann brauchen wir eine besondere Art von Datensammler, der nicht nur die Eingaben auf die maximale L√§nge im Stapel auff√ºllt, sondern auch die Bezeichnungen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256dcaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:18:12.117277Z",
     "start_time": "2024-04-13T07:18:12.101716Z"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "# Was sind Datenkollatoren: https://huggingface.co/docs/transformers/main_classes/data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f78cff5",
   "metadata": {
    "id": "7sZOdRlRIrJd"
   },
   "source": [
    "Das letzte, was wir f√ºr unseren `Seq2SeqTrainer` definieren m√ºssen, ist, wie wir die Metrik aus den Vorhersagen berechnen. Wir m√ºssen daf√ºr eine Funktion definieren, die einfach die zuvor geladene `Metrik` verwendet, und wir m√ºssen ein wenig Vorverarbeitung betreiben, um die Vorhersagen in Texte zu dekodieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11ee15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:18:45.414734Z",
     "start_time": "2024-04-13T07:18:45.399098Z"
    },
    "id": "UmvbnJ9JIrJd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27474a3b",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "Dann m√ºssen wir nur noch all dies zusammen mit unseren Datens√§tzen an den `Seq2SeqTrainer` √ºbergeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e0f16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:28:21.635835Z",
     "start_time": "2024-04-13T07:28:21.294272Z"
    },
    "id": "imY1oC3SIrJf"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102d232",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "Jetzt k√∂nnen wir unser Modell fine-tunen, indem wir einfach die Methode `train` aufrufen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "781403e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T19:13:10.964416Z",
     "start_time": "2024-04-11T19:09:52.582166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38145' max='38145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38145/38145 1:59:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>1.290532</td>\n",
       "      <td>28.039200</td>\n",
       "      <td>34.285600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]], 'forced_eos_token_id': 0}\n"
     ]
    }
   ],
   "source": [
    "## Entkommentieren Sie die folgenden 2 Zeilen, um das Modell neu zu trainieren!\n",
    "\n",
    "# trainer.train()\n",
    "# model.save_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58086e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:35:30.337268Z",
     "start_time": "2024-04-13T07:35:28.651400Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "translate = pipeline('translation',model=model_name, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090bffd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:35:30.910823Z",
     "start_time": "2024-04-13T07:35:30.337268Z"
    }
   },
   "outputs": [],
   "source": [
    "translate(\"Hello. I am a machine learning student.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1facc0-3ff4-4d10-a4e2-1277c87f5078",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T07:35:41.007583Z",
     "start_time": "2024-04-13T07:35:40.700544Z"
    }
   },
   "outputs": [],
   "source": [
    "translate(\"Machine learning is great.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
