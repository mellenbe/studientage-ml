{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Erste Schritte mit Hugging Face\n",
    "<br>\n",
    "<a href=\"https://huggingface.co\"><img src=\"../img/huggingface_small.png\" width=\"70%\">\n",
    "<br>https://huggingface.co</a>\n",
    "<br>\n",
    "\n"
   ],
   "id": "a2100fb7e8730c1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Textklassifikation (Text Classification)\n",
    "\n",
    "Sprachmodelle sind in der Lage, Texte zu **klassifizieren**.\n",
    "\n",
    "Das bedeutet, dass das Ergebnis der Verarbeitung eines Textes die Aussage wie wahrscheinlich es ist, dass ein Text _positiv_ oder _negativ_ konnotiert ist. Diese Ausgabe ist also eine prozentuale Klassenzugehörigkeit zu den Klassen \"POSITIVE\" oder \"NEGATIVE\".\n",
    "\n",
    "\n",
    "---\n",
    "Beispiele angelehnt an\n",
    "\n",
    "Tunstall, Lewis; Werra, Leandro von; Wolf, Thomas (2023): Natural Language Processing mit Transformern. Sprachanwendungen mit Hugging Face erstellen. Heidelberg: O'Reilly (Animals). Online verfügbar unter https://nbn-resolving.org/urn:nbn:de:bsz:31-epflicht-2107005.     \n"
   ],
   "id": "5b012cf997fc198d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T20:24:54.399271Z",
     "start_time": "2024-04-08T20:24:54.387119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime ac- tion figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\" \n",
    "# vgl. Tunstall et al. 2023, S. 10"
   ],
   "id": "6d73ca22d6d9247d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T20:24:58.934125Z",
     "start_time": "2024-04-08T20:24:54.768009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n"
   ],
   "id": "b3e4942d7b182daa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T09:16:09.200293Z",
     "start_time": "2024-04-08T09:16:09.028046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "outputs = classifier(text) \n",
    "pd.DataFrame(outputs)"
   ],
   "id": "14ef9d1c5ef81073",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.872429"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.872429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Erkennung von Namens-Entitäten (Named Entity Recognition)\n",
    "\n",
    "Sprachmodelle sind in der Lage, Produkte, Orte oder Eigennamen aus Texten zu extrahieren. Diese werden als \"Named Entity\" bezeichnet."
   ],
   "id": "ff4036639521e524"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T09:16:12.513708Z",
     "start_time": "2024-04-08T09:16:09.200293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "outputs = ner_tagger(text)\n",
    "pd.DataFrame(outputs)"
   ],
   "id": "9a04f1ac1ddda1b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  entity_group     score           word  start  end\n",
       "0          ORG  0.855356         Amazon      5   11\n",
       "1         MISC  0.992322  Optimus Prime     36   49\n",
       "2          LOC  0.999735        Germany     92   99\n",
       "3         MISC  0.606777           Mega    210  214\n",
       "4          PER  0.524003         ##tron    214  218\n",
       "5          ORG  0.630277         Decept    255  261\n",
       "6         MISC  0.506673        ##icons    261  266\n",
       "7         MISC  0.772442       Megatron    352  360\n",
       "8         MISC  0.988840  Optimus Prime    369  382\n",
       "9          PER  0.814836      Bumblebee    504  513"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.855356</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.992322</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>36</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999735</td>\n",
       "      <td>Germany</td>\n",
       "      <td>92</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.606777</td>\n",
       "      <td>Mega</td>\n",
       "      <td>210</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.524003</td>\n",
       "      <td>##tron</td>\n",
       "      <td>214</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.630277</td>\n",
       "      <td>Decept</td>\n",
       "      <td>255</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.506673</td>\n",
       "      <td>##icons</td>\n",
       "      <td>261</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.772442</td>\n",
       "      <td>Megatron</td>\n",
       "      <td>352</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.988840</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>369</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.814836</td>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>504</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Auch hier ist das Ergebnis eine statistische Zuordnung von Wörtern zu Kategorien mit der Angabe von Wahrscheinlichkeiten, die die so genannte \"confidence\" des Modells angeben. Dies ist die \"Sicherheit\" mit der das Modell die Zuordnung einschätzt. Kategorien sind z.B. \"Organisation (ORG)\", \"Location (LOC)\" oder \"Person (PER)\".\n",
    "\n",
    "Zusätzlich gibt das Modell einen Bereich an, in dem es das Wort (oder die Wortgruppe) gefunden hat.\n",
    "\n",
    "In der Ausgabe können sich auch noch Tokens (mit ## gekennzeichnet) befinden, wenn kein ganzes Wort oder eine Wortgruppe einer Kategorie zugeordnet werden konnte. Interessant ist hier, dass letztlich das \"Plural-Token\" separat behandelt wurde (##ns stammt aus dem Wort \"Decepticons\")."
   ],
   "id": "cf1ead8c672db5cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Beantwortung von Fragen (Question Answering)\n",
    "\n",
    "Sprachmodelle können Fragen beantworten. Dies setzt natürlich voraus, dass das Modell einen Kontext kennt zu dem Fragen beantwortet werden sollen.\n",
    "Hier wollen wir eine Frage zum obigen Text beantworten, der dem Modell bereits bekannt ist. Wir wollen folgende Frage stellen:\n",
    "\n",
    "**What does the customer want?**"
   ],
   "id": "3d0d83b301e2e101"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T09:16:13.379742Z",
     "start_time": "2024-04-08T09:16:12.513708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reader = pipeline(\"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\")\n",
    "question = \"What does the customer want?\"\n",
    "outputs = reader(question=question, context=text)\n",
    "pd.DataFrame([outputs])"
   ],
   "id": "96900bd57d9e70a8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      score  start  end                   answer\n",
       "0  0.533085    337  360  an exchange of Megatron"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.533085</td>\n",
       "      <td>337</td>\n",
       "      <td>360</td>\n",
       "      <td>an exchange of Megatron</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Auch hier wird zusätzlich zur Antwort ein Score ausgegeben, der die confidence des Modells angibt.\n",
    "<blockquote>Beachten Sie: \n",
    "\n",
    "Es handelt sich hier nicht um einen generativen Ansatz, sondern letztlich auch um eine Text-Klassifizierung. Das Ergebnis des Modells ist eine Textstelle, in der das Modell die Antwort vermutet. Es handelt sich bei diesem Ansatz um so genanntes **\"Extractive Question Answering\"**. Hierfür gibt das Modell die Spann mit \"start\" und \"end\" aus, in dem es die Antwort vermutet.</blockquote>\n"
   ],
   "id": "ee3cd6d076c3d9c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Textzusammenfassung (Summarization)\n",
    "\n",
    "Sprachmodelle sind in der Lage, Texte zusammenzufassen. Hierfür gibt es verschiedene methodische Ansätze. Zunäschst wollen wir nur ein einführendes Beispiel betrachten."
   ],
   "id": "89592d55b0578fb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T09:41:39.371282Z",
     "start_time": "2024-04-08T09:38:45.654092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "summary = summarizer(text, max_length=150, min_length=40, do_sample=False)"
   ],
   "id": "102013e9d971bfc7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "932ae244e7d9480c9718afad97494bcb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thomase\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\thomase\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76e52a5dabc24501b47f748496313fd7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f087aba891b4afc8bcb6f7cb88200c0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22e82d02e0694ee6b2c8bc92b63b0018"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbabe2348acd4507993405b9cd1565cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 113. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=56)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T09:43:23.731882Z",
     "start_time": "2024-04-08T09:43:23.715296Z"
    }
   },
   "cell_type": "code",
   "source": "summary[0]['summary_text']",
   "id": "145b5691f076efe7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bumblebee ordered an Optimus Prime figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead. As a lifelong enemy of the Decepticons, I hope you can understand my dilemma.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Auffällig sind am Ergebnis zwei Punkte:\n",
    "- Das Modell hat erkannt, dass der Autor \"Bumblebee\" ist.\n",
    "- Ansonsten sind im Ergebnis letztlich Textbestandteile zusammenkopiert.\n",
    "- Der eigentliche Wunsch auf \"Austausch\" der Figur wurde nicht in der Zusammenfassung erwähnt.\n",
    "\n",
    "Weiterführende Anleitung bei Keras: https://keras.io/examples/nlp/t5_hf_summarization/"
   ],
   "id": "26a1608c5eb0d84f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Übersetzen",
   "id": "3164a956c7a44840"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T20:53:50.902596Z",
     "start_time": "2024-04-08T20:53:49.576197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text = \"translate English to French: Hugging Face is a community-based open-source platform for machine learning.\"\n",
    "translator = pipeline(task=\"translation\", model=\"google-t5/t5-small\")\n",
    "translator(text)"
   ],
   "id": "c0320886b608c99b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Exception encountered when calling layer \"SelfAttention\" \"                 f\"(type TFT5Attention).\n\n{{function_node __wrapped__XlaDynamicSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Could not find compiler for platform Host: NOT_FOUND: could not find registered compiler for platform Host -- was support for that platform linked in? [Op:XlaDynamicSlice]\n\nCall arguments received by layer \"SelfAttention\" \"                 f\"(type TFT5Attention):\n  • hidden_states=tf.Tensor(shape=(4, 1, 512), dtype=float32)\n  • mask=tf.Tensor(shape=(4, 1, 1, 2), dtype=float32)\n  • key_value_states=None\n  • position_bias=None\n  • past_key_value=('tf.Tensor(shape=(4, 8, 1, 64), dtype=float32)', 'tf.Tensor(shape=(4, 8, 1, 64), dtype=float32)')\n  • layer_head_mask=None\n  • query_length=None\n  • use_cache=True\n  • training=False\n  • output_attentions=False",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnimplementedError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtranslate English to French: Hugging Face is a community-based open-source platform for machine learning.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      4\u001B[0m translator \u001B[38;5;241m=\u001B[39m pipeline(task\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtranslation\u001B[39m\u001B[38;5;124m\"\u001B[39m, model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgoogle-t5/t5-small\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 5\u001B[0m \u001B[43mtranslator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:371\u001B[0m, in \u001B[0;36mTranslationPipeline.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    341\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    342\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    343\u001B[0m \u001B[38;5;124;03m    Translate the text(s) given as inputs.\u001B[39;00m\n\u001B[0;32m    344\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    369\u001B[0m \u001B[38;5;124;03m          token ids of the translation.\u001B[39;00m\n\u001B[0;32m    370\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 371\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:167\u001B[0m, in \u001B[0;36mText2TextGenerationPipeline.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    139\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;124;03m    Generate the output text(s) using text(s) given as inputs.\u001B[39;00m\n\u001B[0;32m    141\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;124;03m          ids of the generated text.\u001B[39;00m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 167\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    169\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(args[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mlist\u001B[39m)\n\u001B[0;32m    170\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(el, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m el \u001B[38;5;129;01min\u001B[39;00m args[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m    171\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28mlen\u001B[39m(res) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m result)\n\u001B[0;32m    172\u001B[0m     ):\n\u001B[0;32m    173\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [res[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m result]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\transformers\\pipelines\\base.py:1206\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1198\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[0;32m   1199\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[0;32m   1200\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1203\u001B[0m         )\n\u001B[0;32m   1204\u001B[0m     )\n\u001B[0;32m   1205\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1206\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\transformers\\pipelines\\base.py:1213\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[0;32m   1211\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[0;32m   1212\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpreprocess_params)\n\u001B[1;32m-> 1213\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(model_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n\u001B[0;32m   1214\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n\u001B[0;32m   1215\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\transformers\\pipelines\\base.py:1107\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[1;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[0;32m   1105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   1106\u001B[0m     model_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m-> 1107\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward(model_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   1109\u001B[0m     inference_context \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_inference_context()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:191\u001B[0m, in \u001B[0;36mText2TextGenerationPipeline._forward\u001B[1;34m(self, model_inputs, **generate_kwargs)\u001B[0m\n\u001B[0;32m    184\u001B[0m     in_b, input_length \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mshape(model_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_inputs(\n\u001B[0;32m    187\u001B[0m     input_length,\n\u001B[0;32m    188\u001B[0m     generate_kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin_length\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmin_length),\n\u001B[0;32m    189\u001B[0m     generate_kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_length\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmax_length),\n\u001B[0;32m    190\u001B[0m )\n\u001B[1;32m--> 191\u001B[0m output_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mgenerate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mgenerate_kwargs)\n\u001B[0;32m    192\u001B[0m out_b \u001B[38;5;241m=\u001B[39m output_ids\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\transformers\\generation\\tf_utils.py:979\u001B[0m, in \u001B[0;36mTFGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, seed, **kwargs)\u001B[0m\n\u001B[0;32m    970\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[0;32m    971\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m    972\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    975\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m    976\u001B[0m     )\n\u001B[0;32m    978\u001B[0m     \u001B[38;5;66;03m# 12. run beam search\u001B[39;00m\n\u001B[1;32m--> 979\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbeam_search(\n\u001B[0;32m    980\u001B[0m         input_ids,\n\u001B[0;32m    981\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mmax_length,\n\u001B[0;32m    982\u001B[0m         pad_token_id\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mpad_token_id,\n\u001B[0;32m    983\u001B[0m         eos_token_id\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39meos_token_id,\n\u001B[0;32m    984\u001B[0m         length_penalty\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mlength_penalty,\n\u001B[0;32m    985\u001B[0m         early_stopping\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mearly_stopping,\n\u001B[0;32m    986\u001B[0m         logits_processor\u001B[38;5;241m=\u001B[39mlogits_processor,\n\u001B[0;32m    987\u001B[0m         output_scores\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39moutput_scores,\n\u001B[0;32m    988\u001B[0m         return_dict_in_generate\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mreturn_dict_in_generate,\n\u001B[0;32m    989\u001B[0m         num_return_sequences\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[0;32m    990\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m    991\u001B[0m     )\n\u001B[0;32m    993\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_beam_sample_gen_mode:\n\u001B[0;32m    994\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m generation_config\u001B[38;5;241m.\u001B[39mnum_beams \u001B[38;5;241m<\u001B[39m generation_config\u001B[38;5;241m.\u001B[39mnum_return_sequences:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\transformers\\generation\\tf_utils.py:2611\u001B[0m, in \u001B[0;36mTFGenerationMixin.beam_search\u001B[1;34m(self, input_ids, do_sample, max_length, pad_token_id, eos_token_id, length_penalty, early_stopping, logits_processor, logits_warper, num_return_sequences, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001B[0m\n\u001B[0;32m   2597\u001B[0m \u001B[38;5;66;03m# 2-to-n generation steps can then be run in autoregressive fashion (only in case 1st generation step does\u001B[39;00m\n\u001B[0;32m   2598\u001B[0m \u001B[38;5;66;03m# NOT yield EOS token though)\u001B[39;00m\n\u001B[0;32m   2599\u001B[0m maximum_iterations \u001B[38;5;241m=\u001B[39m max_length \u001B[38;5;241m-\u001B[39m cur_len\n\u001B[0;32m   2600\u001B[0m (\n\u001B[0;32m   2601\u001B[0m     cur_len,\n\u001B[0;32m   2602\u001B[0m     running_sequences,\n\u001B[0;32m   2603\u001B[0m     running_scores,\n\u001B[0;32m   2604\u001B[0m     running_beam_indices,\n\u001B[0;32m   2605\u001B[0m     sequences,\n\u001B[0;32m   2606\u001B[0m     scores,\n\u001B[0;32m   2607\u001B[0m     beam_indices,\n\u001B[0;32m   2608\u001B[0m     is_sent_finished,\n\u001B[0;32m   2609\u001B[0m     decoder_prompt_len,\n\u001B[0;32m   2610\u001B[0m     _,\n\u001B[1;32m-> 2611\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwhile_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2612\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeam_search_cond_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2613\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeam_search_body_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2614\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2615\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcur_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2616\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrunning_sequences\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2617\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrunning_scores\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2618\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrunning_beam_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2619\u001B[0m \u001B[43m        \u001B[49m\u001B[43msequences\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2620\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscores\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2621\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeam_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2622\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_sent_finished\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2623\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecoder_prompt_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2624\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2625\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2626\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaximum_iterations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximum_iterations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2627\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2629\u001B[0m \u001B[38;5;66;03m# 6. prepare outputs\u001B[39;00m\n\u001B[0;32m   2630\u001B[0m \u001B[38;5;66;03m# Account for the edge-case where there are no finished sequences for a particular batch item. If so, return\u001B[39;00m\n\u001B[0;32m   2631\u001B[0m \u001B[38;5;66;03m# running sequences for that batch item.\u001B[39;00m\n\u001B[0;32m   2632\u001B[0m none_finished \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mmath\u001B[38;5;241m.\u001B[39mreduce_any(is_sent_finished, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:629\u001B[0m, in \u001B[0;36mdeprecated_arg_values.<locals>.deprecated_wrapper.<locals>.new_func\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    622\u001B[0m           _PRINTED_WARNING[(func, arg_name)] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    623\u001B[0m         logging\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[0;32m    624\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFrom \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: calling \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m (from \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m) with \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m is deprecated and \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    625\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwill be removed \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mInstructions for updating:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    626\u001B[0m             _call_location(), decorator_utils\u001B[38;5;241m.\u001B[39mget_qualified_name(func),\n\u001B[0;32m    627\u001B[0m             func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m, arg_name, arg_value, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124min a future version\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    628\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m date \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mafter \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m date), instructions)\n\u001B[1;32m--> 629\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2513\u001B[0m, in \u001B[0;36mwhile_loop_v2\u001B[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001B[0m\n\u001B[0;32m   2337\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhile_loop\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[])\n\u001B[0;32m   2338\u001B[0m \u001B[38;5;129m@deprecation\u001B[39m\u001B[38;5;241m.\u001B[39mdeprecated_arg_values(\n\u001B[0;32m   2339\u001B[0m     \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2354\u001B[0m                   maximum_iterations\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   2355\u001B[0m                   name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   2356\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Repeat `body` while the condition `cond` is true.\u001B[39;00m\n\u001B[0;32m   2357\u001B[0m \n\u001B[0;32m   2358\u001B[0m \u001B[38;5;124;03m  `cond` is a callable returning a boolean scalar tensor. `body` is a callable\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2511\u001B[0m \n\u001B[0;32m   2512\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2513\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwhile_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2514\u001B[0m \u001B[43m      \u001B[49m\u001B[43mcond\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcond\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2515\u001B[0m \u001B[43m      \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2516\u001B[0m \u001B[43m      \u001B[49m\u001B[43mloop_vars\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloop_vars\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2517\u001B[0m \u001B[43m      \u001B[49m\u001B[43mshape_invariants\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshape_invariants\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2518\u001B[0m \u001B[43m      \u001B[49m\u001B[43mparallel_iterations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparallel_iterations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2519\u001B[0m \u001B[43m      \u001B[49m\u001B[43mback_prop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mback_prop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2520\u001B[0m \u001B[43m      \u001B[49m\u001B[43mswap_memory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mswap_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2521\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2522\u001B[0m \u001B[43m      \u001B[49m\u001B[43mmaximum_iterations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximum_iterations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2523\u001B[0m \u001B[43m      \u001B[49m\u001B[43mreturn_same_structure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2762\u001B[0m, in \u001B[0;36mwhile_loop\u001B[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001B[0m\n\u001B[0;32m   2759\u001B[0m loop_var_structure \u001B[38;5;241m=\u001B[39m nest\u001B[38;5;241m.\u001B[39mmap_structure(type_spec\u001B[38;5;241m.\u001B[39mtype_spec_from_value,\n\u001B[0;32m   2760\u001B[0m                                         \u001B[38;5;28mlist\u001B[39m(loop_vars))\n\u001B[0;32m   2761\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m cond(\u001B[38;5;241m*\u001B[39mloop_vars):\n\u001B[1;32m-> 2762\u001B[0m   loop_vars \u001B[38;5;241m=\u001B[39m \u001B[43mbody\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mloop_vars\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2763\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m try_to_pack \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(loop_vars, (\u001B[38;5;28mlist\u001B[39m, _basetuple)):\n\u001B[0;32m   2764\u001B[0m     packed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2753\u001B[0m, in \u001B[0;36mwhile_loop.<locals>.<lambda>\u001B[1;34m(i, lv)\u001B[0m\n\u001B[0;32m   2750\u001B[0m     loop_vars \u001B[38;5;241m=\u001B[39m (counter, loop_vars)\n\u001B[0;32m   2751\u001B[0m     cond \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m i, lv: (  \u001B[38;5;66;03m# pylint: disable=g-long-lambda\u001B[39;00m\n\u001B[0;32m   2752\u001B[0m         math_ops\u001B[38;5;241m.\u001B[39mlogical_and(i \u001B[38;5;241m<\u001B[39m maximum_iterations, orig_cond(\u001B[38;5;241m*\u001B[39mlv)))\n\u001B[1;32m-> 2753\u001B[0m     body \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m i, lv: (i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[43morig_body\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mlv\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   2754\u001B[0m   try_to_pack \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   2756\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m executing_eagerly:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\transformers\\generation\\tf_utils.py:2368\u001B[0m, in \u001B[0;36mTFGenerationMixin.beam_search.<locals>.beam_search_body_fn\u001B[1;34m(cur_len, running_sequences, running_scores, running_beam_indices, sequences, scores, beam_indices, is_sent_finished, decoder_prompt_len, model_kwargs)\u001B[0m\n\u001B[0;32m   2364\u001B[0m     input_ids \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mexpand_dims(running_sequences[:, :, cur_len \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m   2365\u001B[0m model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_inputs_for_generation(\n\u001B[0;32m   2366\u001B[0m     flatten_beam_dim(input_ids), use_cache\u001B[38;5;241m=\u001B[39muse_cache, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs\n\u001B[0;32m   2367\u001B[0m )\n\u001B[1;32m-> 2368\u001B[0m model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m(\n\u001B[0;32m   2369\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_inputs,\n\u001B[0;32m   2370\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m   2371\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[0;32m   2372\u001B[0m     output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[0;32m   2373\u001B[0m )\n\u001B[0;32m   2374\u001B[0m logits \u001B[38;5;241m=\u001B[39m unflatten_beam_dim(model_outputs\u001B[38;5;241m.\u001B[39mlogits[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], num_beams)\n\u001B[0;32m   2376\u001B[0m \u001B[38;5;66;03m# 2. Compute log probs\u001B[39;00m\n\u001B[0;32m   2377\u001B[0m \u001B[38;5;66;03m# get log probabilities from logits, process logits with processors (*e.g.* min_length, ...), and\u001B[39;00m\n\u001B[0;32m   2378\u001B[0m \u001B[38;5;66;03m# add new logprobs to existing running logprobs scores.\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\transformers\\modeling_tf_utils.py:437\u001B[0m, in \u001B[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    434\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\n\u001B[0;32m    436\u001B[0m unpacked_inputs \u001B[38;5;241m=\u001B[39m input_processing(func, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfn_args_and_kwargs)\n\u001B[1;32m--> 437\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39munpacked_inputs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\transformers\\models\\t5\\modeling_tf_t5.py:1464\u001B[0m, in \u001B[0;36mTFT5ForConditionalGeneration.call\u001B[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001B[0m\n\u001B[0;32m   1461\u001B[0m     decoder_input_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shift_right(labels)\n\u001B[0;32m   1463\u001B[0m \u001B[38;5;66;03m# Decode\u001B[39;00m\n\u001B[1;32m-> 1464\u001B[0m decoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1465\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1466\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1467\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1468\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1469\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1470\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1471\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1472\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1473\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1474\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1475\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1476\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1477\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1479\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m decoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1481\u001B[0m \u001B[38;5;66;03m# T5v1.1 does not tie output word embeddings and thus does not require downscaling\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\transformers\\modeling_tf_utils.py:437\u001B[0m, in \u001B[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    434\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\n\u001B[0;32m    436\u001B[0m unpacked_inputs \u001B[38;5;241m=\u001B[39m input_processing(func, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfn_args_and_kwargs)\n\u001B[1;32m--> 437\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39munpacked_inputs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\transformers\\models\\t5\\modeling_tf_t5.py:855\u001B[0m, in \u001B[0;36mTFT5MainLayer.call\u001B[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, encoder_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001B[0m\n\u001B[0;32m    853\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states:\n\u001B[0;32m    854\u001B[0m     all_hidden_states \u001B[38;5;241m=\u001B[39m all_hidden_states \u001B[38;5;241m+\u001B[39m (hidden_states,)\n\u001B[1;32m--> 855\u001B[0m layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    856\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    857\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    858\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    859\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    860\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    861\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_decoder_position_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_decoder_position_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    862\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    863\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_layer_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_head_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mencoder_head_mask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    864\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    865\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    866\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    867\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    868\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    870\u001B[0m \u001B[38;5;66;03m# layer_outputs is a tuple with:\u001B[39;00m\n\u001B[0;32m    871\u001B[0m \u001B[38;5;66;03m# hidden-states, key-value-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\u001B[39;00m\n\u001B[0;32m    872\u001B[0m hidden_states, present_key_value_state \u001B[38;5;241m=\u001B[39m layer_outputs[:\u001B[38;5;241m2\u001B[39m]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\transformers\\models\\t5\\modeling_tf_t5.py:641\u001B[0m, in \u001B[0;36mTFT5Block.call\u001B[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, encoder_layer_head_mask, past_key_value, use_cache, output_attentions, training)\u001B[0m\n\u001B[0;32m    638\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    639\u001B[0m     self_attn_past_key_value, cross_attn_past_key_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 641\u001B[0m self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    642\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    643\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    644\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    645\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    646\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    647\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    648\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    651\u001B[0m hidden_states, present_key_value_state \u001B[38;5;241m=\u001B[39m self_attention_outputs[:\u001B[38;5;241m2\u001B[39m]\n\u001B[0;32m    652\u001B[0m attention_outputs \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m2\u001B[39m:]  \u001B[38;5;66;03m# Keep self-attention outputs and relative position weights\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\transformers\\models\\t5\\modeling_tf_t5.py:509\u001B[0m, in \u001B[0;36mTFT5LayerSelfAttention.call\u001B[1;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, training)\u001B[0m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\n\u001B[0;32m    498\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    499\u001B[0m     hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    506\u001B[0m     training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    507\u001B[0m ):\n\u001B[0;32m    508\u001B[0m     normed_hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_norm(hidden_states)\n\u001B[1;32m--> 509\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSelfAttention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    510\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnormed_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    511\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    512\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    513\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    514\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    515\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    516\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    517\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    518\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    519\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m hidden_states \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(attention_output[\u001B[38;5;241m0\u001B[39m], training\u001B[38;5;241m=\u001B[39mtraining)\n\u001B[0;32m    520\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (hidden_states,) \u001B[38;5;241m+\u001B[39m attention_output[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\transformers\\models\\t5\\modeling_tf_t5.py:448\u001B[0m, in \u001B[0;36mTFT5Attention.call\u001B[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, training, output_attentions)\u001B[0m\n\u001B[0;32m    444\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    445\u001B[0m         \u001B[38;5;66;03m# we might have a padded past structure, in which case we want to fetch the position bias slice\u001B[39;00m\n\u001B[0;32m    446\u001B[0m         \u001B[38;5;66;03m# right after the most recently filled past index\u001B[39;00m\n\u001B[0;32m    447\u001B[0m         most_recently_filled_past_index \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mreduce_max(tf\u001B[38;5;241m.\u001B[39mwhere(past_key_value[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, :, \u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0.0\u001B[39m))\n\u001B[1;32m--> 448\u001B[0m         position_bias \u001B[38;5;241m=\u001B[39m \u001B[43mdynamic_slice\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    449\u001B[0m \u001B[43m            \u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    450\u001B[0m \u001B[43m            \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmost_recently_filled_past_index\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    451\u001B[0m \u001B[43m            \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_heads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreal_seq_length\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    452\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    454\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    455\u001B[0m     position_bias \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mcast(position_bias, dtype\u001B[38;5;241m=\u001B[39mmask\u001B[38;5;241m.\u001B[39mdtype)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\tensorflow\\compiler\\tf2xla\\ops\\gen_xla_ops.py:1197\u001B[0m, in \u001B[0;36mxla_dynamic_slice\u001B[1;34m(input, start_indices, size_indices, name)\u001B[0m\n\u001B[0;32m   1195\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m _result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m:\n\u001B[0;32m   1196\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[1;32m-> 1197\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m xla_dynamic_slice_eager_fallback(\n\u001B[0;32m   1198\u001B[0m       \u001B[38;5;28minput\u001B[39m, start_indices, size_indices, name\u001B[38;5;241m=\u001B[39mname, ctx\u001B[38;5;241m=\u001B[39m_ctx)\n\u001B[0;32m   1199\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_SymbolicException:\n\u001B[0;32m   1200\u001B[0m   \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# Add nodes to the TensorFlow graph.\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\studientage-ml\\lib\\site-packages\\tensorflow\\compiler\\tf2xla\\ops\\gen_xla_ops.py:1249\u001B[0m, in \u001B[0;36mxla_dynamic_slice_eager_fallback\u001B[1;34m(input, start_indices, size_indices, name, ctx)\u001B[0m\n\u001B[0;32m   1247\u001B[0m _inputs_flat \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28minput\u001B[39m, start_indices, size_indices]\n\u001B[0;32m   1248\u001B[0m _attrs \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mT\u001B[39m\u001B[38;5;124m\"\u001B[39m, _attr_T, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTindices\u001B[39m\u001B[38;5;124m\"\u001B[39m, _attr_Tindices)\n\u001B[1;32m-> 1249\u001B[0m _result \u001B[38;5;241m=\u001B[39m \u001B[43m_execute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mXlaDynamicSlice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_inputs_flat\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1250\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_attrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1251\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _execute\u001B[38;5;241m.\u001B[39mmust_record_gradient():\n\u001B[0;32m   1252\u001B[0m   _execute\u001B[38;5;241m.\u001B[39mrecord_gradient(\n\u001B[0;32m   1253\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mXlaDynamicSlice\u001B[39m\u001B[38;5;124m\"\u001B[39m, _inputs_flat, _attrs, _result)\n",
      "\u001B[1;31mUnimplementedError\u001B[0m: Exception encountered when calling layer \"SelfAttention\" \"                 f\"(type TFT5Attention).\n\n{{function_node __wrapped__XlaDynamicSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Could not find compiler for platform Host: NOT_FOUND: could not find registered compiler for platform Host -- was support for that platform linked in? [Op:XlaDynamicSlice]\n\nCall arguments received by layer \"SelfAttention\" \"                 f\"(type TFT5Attention):\n  • hidden_states=tf.Tensor(shape=(4, 1, 512), dtype=float32)\n  • mask=tf.Tensor(shape=(4, 1, 1, 2), dtype=float32)\n  • key_value_states=None\n  • position_bias=None\n  • past_key_value=('tf.Tensor(shape=(4, 8, 1, 64), dtype=float32)', 'tf.Tensor(shape=(4, 8, 1, 64), dtype=float32)')\n  • layer_head_mask=None\n  • query_length=None\n  • use_cache=True\n  • training=False\n  • output_attentions=False"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fazit\n",
    "\n",
    "Wir konnten sehen, dass bereits mit dem direkten Einsatz vortrainierter Modell größtenteils brauchbare Ergebnisse erzielt werden können.\n",
    "Um nun tatsächlich gute Ergebnisse zu erzielen werden wir tiefer eintauchen.  "
   ],
   "id": "1d83d5a978adcdb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d6375b23cd382774"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
