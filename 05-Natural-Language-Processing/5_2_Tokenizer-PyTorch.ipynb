{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d24e529a9ff43de6",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "(samples modified from:  https://huggingface.co/docs/transformers/main/en/preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b40843ed823a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T11:59:14.965464Z",
     "start_time": "2024-04-12T11:59:11.643584Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8840d71e1bb24519",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T11:59:28.288784Z",
     "start_time": "2024-04-12T11:59:28.273778Z"
    }
   },
   "outputs": [],
   "source": [
    "# see: https://huggingface.co/docs/transformers/main/en/preprocessing\n",
    "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
    "print(encoded_input['input_ids'])\n",
    "print(encoded_input['token_type_ids'])\n",
    "print(encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fc9c9-e8dc-4a42-95f7-8144cc820892",
   "metadata": {},
   "source": [
    "# Vacabulary and Raw Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11469a99-718f-4cea-80d8-52d186477153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:00:04.074014Z",
     "start_time": "2024-04-12T12:00:03.431997Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer # Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "text = \"We like Transformers in 2024!\"\n",
    "\n",
    "# Tokenize the text\n",
    "token_ids = tokenizer.encode(text, add_special_tokens=True) # Output the token IDs\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# Convert token IDs back to raw tokens and output them\n",
    "raw_tokens = [tokenizer.decode([token_id]) for token_id in token_ids]\n",
    "print(\"Raw tokens:\", raw_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51d656f-8632-4365-9f4c-5b037a9f68e7",
   "metadata": {},
   "source": [
    "# Tokens <> Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c254c-6832-4eaa-a648-ff2559e178c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:02:02.601577Z",
     "start_time": "2024-04-12T12:02:00.259118Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Load pre-trained model tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased') # Load pre-trained model\n",
    "text = \"We like Transformers in 2024!\" # Text to be tokenized\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=True) \n",
    "print(\"Token IDs:\", input_ids) # Output the token IDs\n",
    "\n",
    "# Convert token IDs back to raw tokens and output them\n",
    "raw_tokens = [tokenizer.decode([token_id]) for token_id in input_ids]\n",
    "print(\"Raw tokens:\", raw_tokens)\n",
    "\n",
    "# Convert list of IDs to a tensor\n",
    "input_ids_tensor = torch.tensor([input_ids])\n",
    "\n",
    "# Pass the input through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids_tensor)\n",
    "\n",
    "# Extract the embeddings\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Print the embeddings\n",
    "print(\"Embeddings: \", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d071c-9128-4895-a6d4-873f62d63658",
   "metadata": {},
   "source": [
    "### Dimension der Embeddings.\n",
    "F√ºr jedes der Token (9) gibt es einen Tensor (einen Vektor) mit 768 Parametern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e203c3f-ec57-4f50-a3b0-bdc4647f2fa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:02:16.210612Z",
     "start_time": "2024-04-12T12:02:16.194990Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a094b-2778-4ab8-8252-977c6d1f04ac",
   "metadata": {},
   "source": [
    "### Darstellung eines Beispiel-Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b303cf-edcc-4306-bade-0116466999b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:02:48.498633Z",
     "start_time": "2024-04-12T12:02:48.483005Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fceeb32b91736",
   "metadata": {},
   "source": [
    "# Handling multiple Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d017898cbbddc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:03:11.746801Z",
     "start_time": "2024-04-12T12:03:11.731173Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_inputs = tokenizer(batch_sentences)\n",
    "\n",
    "# Wir definieren eine kleine Hilfsfunktion, um die Parameter ausgeben zu k√∂nnen:\n",
    "def outp (inputs):\n",
    "    for name in ['input_ids', 'token_type_ids', 'attention_mask']:\n",
    "        if name in inputs:\n",
    "            print(f\"\\n{name}\")\n",
    "            for i in range (len (inputs[name])):\n",
    "                print(inputs[name][i])\n",
    "\n",
    "outp(encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe850d92ce68a6",
   "metadata": {},
   "source": [
    "# Padding & Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21cb67847215d41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:04:37.832990Z",
     "start_time": "2024-04-12T12:04:37.817363Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_inputs = tokenizer(batch_sentences, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368cc043a359bb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:04:40.140952Z",
     "start_time": "2024-04-12T12:04:40.125330Z"
    }
   },
   "outputs": [],
   "source": [
    "outp(encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a06df9694ddacd9",
   "metadata": {},
   "source": [
    "# Word based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8303a2d7b27e730",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:04:53.443583Z",
     "start_time": "2024-04-12T12:04:53.412306Z"
    }
   },
   "outputs": [],
   "source": [
    "text = 'But what about second breakfast?'\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4963e43e2391fdf",
   "metadata": {},
   "source": [
    "# Sub-Word based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a5f98d05bfbf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:04:58.518033Z",
     "start_time": "2024-04-12T12:04:57.823286Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet/xlnet-base-cased\")\n",
    "output = tokenizer.tokenize(\"Don't you love ü§ó Transformers? We sure do.\")\n",
    "for i in range (len (output)):\n",
    "    print(output[i], end=' | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca3ffc85373be9",
   "metadata": {},
   "source": [
    "- Transformers wurde in zwei Sub-Words aufgeteilt\n",
    "- Satzzeichen habe eigene Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65824e3fa124182",
   "metadata": {},
   "source": [
    "# Your own Tokenizer\n",
    "\n",
    "## Einen Textcorpus laden\n",
    "[Credits to huggingface: https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb]\n",
    "\n",
    "## Ein Dataset von huggingface laden:\n",
    "Wir laden zun√§chst das \"Wikitext\" Dataset von huggingface. Darin enthalten sind Beispieltexte von Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6deb2878c516557",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:05:12.670994Z",
     "start_time": "2024-04-12T12:05:12.066627Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47575139c782d0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:05:26.003131Z",
     "start_time": "2024-04-12T12:05:13.713740Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca268ceb7d29249",
   "metadata": {},
   "source": [
    "# Zugriffsm√∂glichkeiten auf das DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f65d7a97bfafe0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:05:34.004798Z",
     "start_time": "2024-04-12T12:05:33.989244Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb13755de4b029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:05:42.349151Z",
     "start_time": "2024-04-12T12:05:42.328962Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset[30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553f72a4db9ceca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:08.367922Z",
     "start_time": "2024-04-09T14:07:08.353418Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset[30000:30005]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf13d35baaf6fef",
   "metadata": {},
   "source": [
    "# DataSet f√ºr den Trainnigsprozess bereitstellen (Corpus)\n",
    "## A) als Liste von Listen\n",
    "Dies l√§sst sich einfach umsetzen, hat aber den Nachteil, dass die Daten im Hauptspeicher gehalten werden m√ºssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca9869e5a986ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:06:43.358852Z",
     "start_time": "2024-04-12T12:06:40.392565Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "all_texts = [dataset[i : i + batch_size][\"text\"] for i in range(0, len(dataset), batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33150b55f9cdfb1",
   "metadata": {},
   "source": [
    "## B) Als Iterator\n",
    "Ein (Python-) Iterator ist eine M√∂glichkeit, Daten sukzessive nach und nach zu laden. Da das DataSet die Daten auf der Festplatte vorh√§lt, wird f√ºr den Iterator immer nur ein Teil der Daten in den Hauptspeicher geladen.\n",
    "\n",
    "Wir definieren diesen hier als Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997ed561a96c88b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:07:49.139691Z",
     "start_time": "2024-04-12T12:07:49.124057Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_iterator():\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e5b5cdba7843d",
   "metadata": {},
   "source": [
    "# Verschiedene Tokenizer aufbauen\n",
    "# 1) Die Architektur eines vorhandenen Tokenizers clonen aber selbst trainieren\n",
    "[Credits to: https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e76835128407a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:08:04.879636Z",
     "start_time": "2024-04-12T12:08:04.614024Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Wir stellen sicher, dass wir eine \"fast\" Version der Architektur geladen haben, um das Training im Rahmen zu halten\n",
    "if not tokenizer.is_fast:\n",
    "    raise (\"Dieser Tokenizer ist nicht geeignet.\")\n",
    "else:\n",
    "    print(\"Dieser Tokenizer ist geeignet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c6c86ed70809e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:09:47.588874Z",
     "start_time": "2024-04-12T12:09:14.573272Z"
    }
   },
   "outputs": [],
   "source": [
    "gpt_clone_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034d80b1d86917b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:11:13.304935Z",
     "start_time": "2024-04-12T12:11:13.296822Z"
    }
   },
   "outputs": [],
   "source": [
    "outp(gpt_clone_tokenizer(dataset[30000:30005][\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d527b5f9df024b1",
   "metadata": {},
   "source": [
    "### Diesen (trainierten!) Tokenizer k√∂nnen wir auch zur Wiederverwertung speichern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5c11cb814806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:11:21.674725Z",
     "start_time": "2024-04-12T12:11:21.627848Z"
    }
   },
   "outputs": [],
   "source": [
    "gpt_clone_tokenizer.save_pretrained(\"../data/gpt_clone_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f976d524bd4699f5",
   "metadata": {},
   "source": [
    "## 2) Architektur eines Tokenizers selbst aufbauen\n",
    "### Beispiel: Byte Pair Encodings (wie f√ºr GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc6d7507c64152",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:12:01.471106Z",
     "start_time": "2024-04-12T12:12:01.455481Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ben√∂tigte Imports\n",
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e3b0db74be78",
   "metadata": {},
   "source": [
    "## Tokenizer-Pipeline\n",
    "\"Um zu verstehen, wie man einen Tokenizer von Grund auf erstellt, m√ºssen wir ein wenig mehr in die Tokenizer-Bibliothek und die Tokenisierungs-Pipeline eintauchen. Diese Pipeline besteht aus mehreren Schritten:\n",
    "\n",
    "* **Normalisierung**: F√ºhrt alle anf√§nglichen Transformationen √ºber die anf√§ngliche Eingabezeichenkette aus. Wenn Sie z.B. einen Text klein schreiben, ihn vielleicht entfernen oder einen der √ºblichen Unicode-Normalisierungsprozesse anwenden wollen, f√ºgen Sie einen Normalizer hinzu.\n",
    "* **Pre-Tokenizer**: Verantwortlich f√ºr die Aufteilung der urspr√ºnglichen Eingabezeichenfolge. Das ist die Komponente, die entscheidet, wo und wie die urspr√ºngliche Zeichenkette vorsegmentiert wird. Das einfachste Beispiel w√§re, einfach an Leerzeichen zu trennen.\n",
    "* **Modell**: √úbernimmt die gesamte Erkennung und Generierung von Sub-Token. Dieser Teil kann trainiert werden und ist wirklich von den Eingabedaten abh√§ngig.\n",
    "* **Post-Processing**: Bietet erweiterte Konstruktionsfunktionen, um mit einigen der Transformers-basierten SoTA-Modelle kompatibel zu sein. F√ºr BERT wird der tokenisierte Satz beispielsweise um [CLS]- und [SEP]-Token \"verpackt\".\"\n",
    "\n",
    "Vgl. https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9881174b-46eb-4373-a683-40a2c5453bcc",
   "metadata": {},
   "source": [
    "Und in die umgekehrte Richtung:\n",
    "\n",
    "**Dekodierung**: Verantwortlich f√ºr die R√ºckf√ºhrung einer tokenisierten Eingabe in die urspr√ºngliche Zeichenkette. Der Decoder wird in der Regel nach dem Pre-Tokenizer ausgew√§hlt, den wir zuvor verwendet haben.\n",
    "F√ºr das Training des Modells bietet die ü§ó Tokenizer-Bibliothek eine Trainer-Klasse, die wir verwenden werden.\n",
    "\n",
    "Alle diese Bausteine k√∂nnen kombiniert werden, um funktionierende Tokenizer-Pipelines zu erstellen. \n",
    "\n",
    "**Beispiele**:\n",
    "- GPT-2 == BPE-Tokenizer\n",
    "- BERT == WordPiece-Tokenizer\n",
    "- T5 == Unigram-Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb32439cddfcc5",
   "metadata": {},
   "source": [
    "### Ein BPE-Tokenizer ben√∂tigt einen Pre-Tokenizer (word based tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9eddb1389c7009",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:13:38.401936Z",
     "start_time": "2024-04-12T12:13:38.386406Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basismodell ist ein BPE-Tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49badaca-841a-4238-83ed-0b5dfa67aa94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:14:11.635884Z",
     "start_time": "2024-04-12T12:14:11.620230Z"
    }
   },
   "outputs": [],
   "source": [
    "print (tokenizer.pre_tokenizer)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d8cafd73ffce65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:14:15.859127Z",
     "start_time": "2024-04-12T12:14:15.844953Z"
    }
   },
   "outputs": [],
   "source": [
    "# BPE Byte Level Pre-Tokenizer werden √ºber huggingface zur Verf√ºgung gestellt.\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Tokenizer sind toll!\")\n",
    "# add_prefix_space=False verhindert, dass vor dem ersten Wort ebenfalls das Trennzeichen 'ƒ†' erscheint. Alle weiteren Token werden mit diesem Zeichen eingeleitet.\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c356680f7931dcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:14:19.376790Z",
     "start_time": "2024-04-12T12:14:19.361166Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ergebnisse des Pre-Tokenizers anziegen\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Tokenizer sind toll!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955ef5c8c7880b6",
   "metadata": {},
   "source": [
    "### Unseren Tokenizer trainieren\n",
    "Der Tokenizer muss trainiert werden, denn er muss die Byte Pairs lernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e633cafc47961af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:15:44.901760Z",
     "start_time": "2024-04-12T12:15:15.849904Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f0743ffc15f17",
   "metadata": {},
   "source": [
    "### Post-Prozessing\n",
    "Der Tokenizer erh√§lt noch ein Post-Prozessing.\n",
    "Der ByteLevel PostProcessor k√ºmmert sich um das Trimming der Offsets. Whitespaces k√∂nnen optional ebenfalls getrimmt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81bd71fc8547dcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:15:53.663734Z",
     "start_time": "2024-04-12T12:15:53.648118Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131aec6071353bf1",
   "metadata": {},
   "source": [
    "### Tokenizer f√ºr Huggingface Framework \"verpacken\"\n",
    "HuggingFace stellt Wrapper zur Verf√ºgung, die sicherstellen, dass die Tokenizer zu den APIs der Modelle passen.\n",
    "\n",
    "Hier verwenden wir einen GPT2TokenizerFast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecbef6e26ed32a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:16:28.538429Z",
     "start_time": "2024-04-12T12:16:28.475923Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "custom_gpt_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3486153449ec111c",
   "metadata": {},
   "source": [
    "### Neuen Tokenizer anwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daffd63fef1e8f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T12:16:37.275282Z",
     "start_time": "2024-04-12T12:16:37.259763Z"
    }
   },
   "outputs": [],
   "source": [
    "outp(custom_gpt_tokenizer(dataset[30000:30005][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5445ac8caa86c331",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
