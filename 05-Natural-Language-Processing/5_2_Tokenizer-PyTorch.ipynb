{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d24e529a9ff43de6",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "(samples modified from:  https://huggingface.co/docs/transformers/main/en/preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c70b40843ed823a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:06:36.840064Z",
     "start_time": "2024-04-09T14:06:34.357210Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8840d71e1bb24519",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:06:36.856074Z",
     "start_time": "2024-04-09T14:06:36.840064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2091, 1136, 1143, 13002, 1107, 1103, 5707, 1104, 16678, 1116, 117, 1111, 1152, 1132, 11515, 1105, 3613, 1106, 4470, 119, 102]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# see: https://huggingface.co/docs/transformers/main/en/preprocessing\n",
    "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
    "print(encoded_input['input_ids'])\n",
    "print(encoded_input['token_type_ids'])\n",
    "print(encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fc9c9-e8dc-4a42-95f7-8144cc820892",
   "metadata": {},
   "source": [
    "# Vacabulary and Raw Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11469a99-718f-4cea-80d8-52d186477153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [1135, 588, 39185, 287, 48609, 0]\n",
      "Raw tokens: ['We', ' like', ' Transformers', ' in', ' 2024', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer # Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "text = \"We like Transformers in 2024!\"\n",
    "\n",
    "# Tokenize the text\n",
    "token_ids = tokenizer.encode(text, add_special_tokens=True) # Output the token IDs\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# Convert token IDs back to raw tokens and output them\n",
    "raw_tokens = [tokenizer.decode([token_id]) for token_id in token_ids]\n",
    "print(\"Raw tokens:\", raw_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51d656f-8632-4365-9f4c-5b037a9f68e7",
   "metadata": {},
   "source": [
    "# Tokens <> Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e1c254c-6832-4eaa-a648-ff2559e178c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [101, 2057, 2066, 19081, 1999, 16798, 2549, 999, 102]\n",
      "Raw tokens: ['[CLS]', 'we', 'like', 'transformers', 'in', '202', '##4', '!', '[SEP]']\n",
      "Embeddings:  tensor([[[ 0.2137, -0.0775,  0.3104,  ..., -0.3053,  0.6714,  0.1316],\n",
      "         [ 0.5586, -0.0104, -0.3371,  ..., -0.2438,  1.4860, -0.1893],\n",
      "         [ 0.4292,  0.2395,  1.1432,  ..., -0.2679,  0.2587, -0.3247],\n",
      "         ...,\n",
      "         [ 0.0826, -0.0128,  0.7097,  ..., -0.5649,  0.1436,  0.4176],\n",
      "         [ 0.0555, -0.2149,  0.0038,  ...,  0.6719,  0.2073, -0.4478],\n",
      "         [ 0.7300,  0.0909,  0.2141,  ...,  0.1088, -0.4650, -0.4151]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Load pre-trained model tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased') # Load pre-trained model\n",
    "text = \"We like Transformers in 2024!\" # Text to be tokenized\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=True) \n",
    "print(\"Token IDs:\", input_ids) # Output the token IDs\n",
    "\n",
    "# Convert token IDs back to raw tokens and output them\n",
    "raw_tokens = [tokenizer.decode([token_id]) for token_id in input_ids]\n",
    "print(\"Raw tokens:\", raw_tokens)\n",
    "\n",
    "# Convert list of IDs to a tensor\n",
    "input_ids_tensor = torch.tensor([input_ids])\n",
    "\n",
    "# Pass the input through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids_tensor)\n",
    "\n",
    "# Extract the embeddings\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Print the embeddings\n",
    "print(\"Embeddings: \", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d071c-9128-4895-a6d4-873f62d63658",
   "metadata": {},
   "source": [
    "### Dimension der Embeddings.\n",
    "F√ºr jedes der Token (9) gibt es einen Tensor (einen Vektor) mit 768 Parametern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e203c3f-ec57-4f50-a3b0-bdc4647f2fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a094b-2778-4ab8-8252-977c6d1f04ac",
   "metadata": {},
   "source": [
    "### Darstellung eines Beispiel-Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52b303cf-edcc-4306-bade-0116466999b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.5860e-01, -1.0373e-02, -3.3707e-01,  2.8159e-01,  8.0756e-03,\n",
       "        -1.5892e-01,  7.8681e-02,  7.3727e-01,  6.6016e-01, -4.6414e-01,\n",
       "        -2.8798e-01, -1.8046e-01,  6.8951e-02,  3.2863e-01, -6.2951e-01,\n",
       "        -2.7793e-01,  2.0718e-01,  3.4607e-01,  3.7537e-01,  1.7058e-01,\n",
       "        -3.6978e-01, -5.1181e-02, -1.7712e-01,  3.1384e-01,  3.4984e-01,\n",
       "        -1.2083e-01, -8.5585e-02,  2.0418e-01,  4.4767e-01, -1.6739e-01,\n",
       "        -7.6793e-03,  1.3199e-01, -1.3449e-01,  2.6002e-01, -5.7250e-01,\n",
       "        -3.4680e-01, -5.1936e-01,  6.4652e-01,  4.2504e-02,  2.2167e-01,\n",
       "         3.1272e-01, -5.2754e-01,  3.5649e-01, -2.1290e-01,  1.1017e-01,\n",
       "        -2.9269e-01,  4.7714e-03, -1.7892e-01, -1.5575e-01, -8.6869e-01,\n",
       "        -5.7148e-02,  1.9105e-01, -2.1960e-01,  5.4789e-01, -2.9957e-01,\n",
       "         1.3120e-01, -2.6315e-01,  5.3071e-01,  3.3026e-01,  2.7262e-01,\n",
       "        -2.7460e-01,  2.0843e-01, -9.0856e-02, -6.0086e-01, -2.3924e-01,\n",
       "         3.0926e-01, -4.3366e-01, -4.1676e-01,  5.5854e-01,  8.1602e-02,\n",
       "         3.8977e-01, -2.5526e-01, -1.5223e-01,  5.8511e-01,  1.9917e-01,\n",
       "        -1.7483e-01,  9.4262e-03,  5.0749e-02,  3.0771e-01, -2.9736e-01,\n",
       "         3.3708e-01,  4.5482e-02, -6.7752e-01,  6.7732e-01, -2.3748e-01,\n",
       "         6.2612e-01,  1.8489e-01,  2.5392e-01,  1.9871e-02,  1.3102e-01,\n",
       "        -2.7726e-01,  4.9932e-02, -9.9078e-02,  1.3383e-01,  5.9634e-01,\n",
       "        -8.1971e-01,  1.3895e-01, -7.5624e-01,  9.6297e-01,  1.2776e-02,\n",
       "         4.3563e-02, -3.7463e-02, -1.7750e-01, -8.0615e-01, -7.7980e-02,\n",
       "        -7.5498e-02,  1.6930e-01, -5.8944e-03,  3.1399e-01, -2.0809e+00,\n",
       "         3.6142e-01, -3.8235e-01, -2.3984e-02,  1.0126e-01,  6.9458e-02,\n",
       "         6.4941e-01,  9.7138e-01, -7.1873e-01, -1.8626e-03,  2.7839e-01,\n",
       "        -6.4344e-01,  1.1886e+00,  1.9916e-01,  5.5271e-01, -4.6591e-01,\n",
       "        -1.8939e-01,  4.8810e-01,  5.5249e-01, -7.8561e-03, -1.0039e+00,\n",
       "         2.3450e-01,  5.3760e-01, -3.7975e-01, -9.6972e-01, -2.9751e-02,\n",
       "         6.8871e-02,  6.1008e-01,  5.0879e-01,  1.8337e-01, -2.8897e-01,\n",
       "         1.2240e+00, -2.3453e-01, -1.9229e-01,  1.5863e-01,  3.2145e-01,\n",
       "         2.2542e-01, -2.2163e-01, -3.8111e-01,  8.2952e-02, -5.9728e-02,\n",
       "        -1.5794e-01, -1.6055e-01, -1.1005e-01, -4.5532e-02,  2.2416e-01,\n",
       "        -9.9838e-03,  5.7899e-02,  4.4599e-01,  8.1491e-01,  8.0197e-02,\n",
       "         4.9917e-02, -6.6438e-01, -4.0455e-01,  8.3290e-01, -6.3759e-01,\n",
       "         2.6661e-01, -1.7441e-01,  7.7135e-02, -3.1975e-01,  6.0374e-01,\n",
       "         8.9795e-04, -3.8952e-01,  3.3379e-01,  2.2428e-01,  4.8355e-01,\n",
       "        -9.8711e-01,  2.7122e-02,  4.6087e-01, -5.0984e-01,  4.4157e-01,\n",
       "        -4.5572e-01,  5.7672e-01,  1.7365e-01, -1.7765e-01,  7.7454e-01,\n",
       "        -5.3153e-01,  9.4043e-02, -1.0036e+00,  7.3697e-01,  1.1311e+00,\n",
       "         8.5955e-01, -6.8052e-01,  2.7365e-01,  6.0325e-02,  5.0423e-01,\n",
       "        -3.0606e-01,  5.5105e-01, -2.8018e-01, -1.8756e-03, -1.4831e-01,\n",
       "        -5.6504e-01, -3.3163e-01,  7.8537e-01,  4.0113e-01, -5.4120e-01,\n",
       "         1.7397e+00,  1.5505e-01, -1.2410e-01, -2.8149e-01, -6.2947e-01,\n",
       "         3.8105e-01,  1.5575e-01,  1.6431e-02, -7.8882e-01,  4.8441e-01,\n",
       "        -6.6577e-02,  2.2641e-01, -7.9504e-02, -3.6388e-01, -7.6586e-02,\n",
       "         3.9807e-01, -1.6157e-01, -4.8142e-01,  3.0219e-01, -4.7341e-01,\n",
       "         9.4867e-01, -6.1707e-01,  2.3594e-01, -6.0260e-01, -5.1391e-01,\n",
       "        -7.0299e-01, -6.8730e-01,  3.5886e-01,  6.7153e-01,  1.9529e-01,\n",
       "         5.1071e-01, -5.4227e-01, -4.8812e-01, -1.8368e-01,  8.8081e-01,\n",
       "         1.5995e-02,  3.0758e-01, -2.1467e-01,  7.0539e-01, -3.3036e-01,\n",
       "        -1.1823e-01,  2.9423e-01, -2.5586e-01, -5.3759e-01,  2.8725e-01,\n",
       "        -3.4368e-01, -8.2674e-01, -5.8963e-02, -1.9328e-01,  2.5332e-01,\n",
       "         1.1783e-01, -3.0122e-01,  1.8541e-01,  1.0798e-01,  4.9935e-01,\n",
       "        -2.9082e-01, -1.2844e-01,  2.6810e-01,  2.8382e-01, -7.9876e-02,\n",
       "        -1.1148e+00, -4.6652e-01,  8.9352e-01,  4.8964e-01,  7.0292e-01,\n",
       "         9.7570e-02,  1.4647e-01,  6.8385e-02, -1.5296e+00,  4.0950e-01,\n",
       "        -3.3074e-01,  5.7496e-01,  2.2910e-01, -2.2510e-01, -2.8184e-02,\n",
       "        -1.1656e-01,  1.3347e+00,  1.8768e-01,  4.5106e-01,  4.1290e-01,\n",
       "        -3.4825e-01, -1.1299e-01, -6.9538e-01, -4.0251e-02,  6.4287e-02,\n",
       "        -5.2562e-01,  1.6387e-01, -3.6324e-01, -9.9590e-03,  3.6338e-01,\n",
       "        -5.7704e-01,  2.4387e-01, -5.1037e-01,  7.9270e-02,  2.6132e-01,\n",
       "        -6.0887e-01, -1.0175e-01, -6.2746e-01,  4.4785e-02,  6.0666e-01,\n",
       "        -5.2698e-01, -3.7482e-01,  5.7170e-02, -4.5595e+00, -9.1266e-01,\n",
       "        -2.2273e-01,  9.6268e-02, -4.1062e-01, -5.4981e-01, -8.5358e-02,\n",
       "        -3.4414e-01, -6.7072e-01, -4.1470e-03,  4.0943e-01, -5.3202e-02,\n",
       "        -6.6331e-01, -1.1256e-02,  3.9022e-01, -1.6901e-01,  2.4575e-01,\n",
       "         6.4758e-02,  1.4386e-01,  1.0093e+00,  2.4121e-01,  1.4323e-01,\n",
       "        -1.2726e-01, -4.6736e-01,  1.4991e-02,  5.8256e-02, -4.8515e-01,\n",
       "        -3.2674e-01, -2.3950e-01,  4.6037e-01,  1.0419e-01,  8.3101e-01,\n",
       "         1.4869e-01,  6.0079e-01,  4.1094e-01,  4.3819e-01, -3.8118e-01,\n",
       "        -1.0560e-02, -2.0539e-02,  6.4652e-02, -3.1644e-01,  4.5048e-01,\n",
       "        -2.1402e-01,  2.4691e-01,  8.4306e-01,  3.9412e-01,  1.7271e-01,\n",
       "        -5.4790e-01,  5.1354e-01, -2.1041e-02,  5.1947e-02, -6.8682e-01,\n",
       "         9.9357e-01,  2.2633e-01,  1.8591e-01, -1.0894e-04,  2.2209e-01,\n",
       "         5.4298e-01, -2.6544e-01, -8.0738e-01,  2.7592e-01, -3.2651e-01,\n",
       "        -2.0792e-01,  4.4201e-01,  2.9821e-01, -3.7990e-01, -3.6467e-01,\n",
       "        -1.8853e-01,  2.6989e-01,  1.7322e-02,  5.0927e-01, -5.7073e-01,\n",
       "        -2.0577e-01, -1.5954e+00, -4.4311e-01,  7.8570e-02,  1.5411e-01,\n",
       "        -4.9007e-01,  2.4021e-01, -4.2151e-01, -1.7244e-01, -4.0390e-02,\n",
       "        -3.3692e-01,  2.6332e-03,  3.2218e-01, -2.8963e-01,  1.1813e+00,\n",
       "         1.3005e-01, -2.1206e-01,  7.6665e-01, -2.6277e-01, -2.3633e-01,\n",
       "         6.5471e-01,  2.6735e-01,  5.0216e-01, -4.8865e-01,  6.6082e-02,\n",
       "        -7.1263e-01,  5.1429e-02, -2.9650e-01, -8.5583e-01,  3.2095e-01,\n",
       "         2.8217e-01, -1.1902e-01,  1.3895e-01,  2.1676e-01, -2.6982e-01,\n",
       "         3.1407e-01,  2.6177e-01, -1.2306e-01, -2.2187e-01, -4.5660e-01,\n",
       "        -6.4774e-01,  7.4580e-02,  1.2479e-01, -8.4468e-02, -4.3623e-02,\n",
       "        -2.0280e-01, -1.0295e+00, -2.8162e-01,  3.7243e-01,  3.5242e-01,\n",
       "        -4.3068e-01, -8.3547e-01, -4.6046e-01, -1.8138e-02,  4.1896e-01,\n",
       "         3.6820e-01, -1.0180e-01, -4.3834e-02,  6.2632e-02, -1.3409e-01,\n",
       "         1.3345e-01, -6.2305e-01, -2.5353e-01,  1.3170e-01, -6.2642e-01,\n",
       "         3.7717e-01,  1.7124e-01, -2.3015e-01,  8.2430e-01,  9.7897e-02,\n",
       "        -5.9394e-01,  8.4001e-01, -1.2283e-01,  5.1838e-01, -1.0990e-01,\n",
       "        -2.6145e-01, -2.6980e-01,  1.2984e-01, -2.0504e-01, -1.1807e-01,\n",
       "         5.3972e-02, -8.6855e-02,  4.1735e-01, -9.1587e-01,  3.3382e-01,\n",
       "        -7.5968e-01,  1.5654e-01, -5.7334e-01, -6.7241e-01,  6.9698e-01,\n",
       "        -4.5745e-01, -3.8455e-01,  1.1038e+00,  1.0075e-01, -2.6499e-02,\n",
       "         1.1569e-01, -5.5668e-02,  4.5523e-02,  2.3471e-01,  1.0134e+00,\n",
       "        -1.6290e-01, -2.4547e-01, -1.2258e-02, -8.6312e-01,  1.3876e-01,\n",
       "         6.9254e-01, -3.6801e-01, -8.4803e-02, -1.4022e-01, -5.1048e-01,\n",
       "        -6.8599e-01, -2.0994e-01, -3.8834e-01,  1.9399e-02, -6.4309e-01,\n",
       "         1.9799e-01, -2.8430e-01,  8.3089e-01, -1.4272e-01, -2.0827e-02,\n",
       "        -8.6791e-02, -8.7437e-01, -6.7481e-01, -3.5755e-01, -3.9665e-01,\n",
       "        -1.0535e-01,  1.1535e-04,  5.4744e-01,  7.5157e-02,  4.9432e-01,\n",
       "        -3.1381e-01, -3.2554e-01, -2.8276e-01, -2.2609e-01, -1.3030e-01,\n",
       "         3.5125e-01, -1.4056e-01,  1.4347e-01, -6.9196e-01, -2.5389e-01,\n",
       "        -6.9894e-01, -5.4025e-01, -1.4916e-01, -7.1806e-01,  2.0636e-01,\n",
       "         2.1726e-01, -3.4791e-01,  9.5837e-02, -4.3409e-01, -8.7442e-02,\n",
       "        -5.4507e-01,  2.1114e-01,  1.8577e-01, -1.2333e-01, -2.2235e-01,\n",
       "        -6.4226e-01, -4.8364e-01, -3.8061e-01, -2.2271e-01, -3.0078e-01,\n",
       "         1.1416e+00,  6.8124e-01,  1.8443e-01,  3.9616e-02, -1.0199e-01,\n",
       "         2.5422e-01,  5.8668e-01,  5.9441e-01, -4.0591e-01, -9.6828e-01,\n",
       "         2.2696e-01, -1.9464e-01,  2.0211e-01, -5.3274e-01, -1.9357e-01,\n",
       "         3.1862e-01, -6.1107e-01,  7.0581e-02, -3.8509e-01,  4.0479e-02,\n",
       "        -2.3113e-01, -4.8574e-01, -4.5922e-01,  2.7335e-01, -2.1608e-01,\n",
       "        -1.5806e-02, -2.4750e-01, -2.2872e-01, -1.6902e-01,  2.2547e-01,\n",
       "         4.6896e-01, -2.2795e-01,  1.0700e-01,  6.6240e-01, -5.8238e-02,\n",
       "        -2.2894e-03, -5.8080e-01,  9.8891e-01, -2.8961e-01, -9.8558e-02,\n",
       "        -5.7529e-02, -4.6558e-03,  7.2070e-01, -2.5843e-01,  3.4013e-01,\n",
       "        -2.4166e-01, -8.1984e-01,  2.4317e-01,  8.4196e-01,  7.7342e-01,\n",
       "         1.0601e-01, -3.8327e-01, -2.5134e-01, -3.7618e-01, -4.3321e-01,\n",
       "         3.5767e-01,  3.5309e-01,  6.9537e-01, -4.0712e-02, -3.5269e-01,\n",
       "         3.5245e-01,  2.7849e-02,  4.5299e-01, -8.6426e-02, -7.8948e-02,\n",
       "        -8.5697e-01, -2.4280e-01,  2.1291e-01, -3.2736e-01,  7.4394e-01,\n",
       "         5.4577e-01,  8.7324e-02,  7.9032e-02,  2.0546e-01,  1.7966e-01,\n",
       "         3.1465e-01,  2.6250e-01, -4.6764e-02, -6.5517e-01, -3.7904e-02,\n",
       "         1.2099e-01,  2.4812e-01,  3.0063e-01,  2.1895e-01,  1.8650e-01,\n",
       "        -9.4341e-01, -4.8855e-01,  4.6604e-02, -1.8064e-01, -5.9479e-01,\n",
       "         9.0589e-02,  2.5292e-02,  2.8229e-02, -4.8658e-01,  1.2146e-01,\n",
       "         4.9829e-01,  3.5605e-01,  9.2652e-01,  4.4352e-01,  7.3951e-01,\n",
       "         3.4255e-01,  6.5042e-01,  5.1350e-02, -7.0375e-02, -2.8172e-01,\n",
       "        -7.0012e-01, -5.4826e-01,  5.4339e-01,  1.1978e-02,  2.9418e-01,\n",
       "         9.3850e-01,  3.9666e-01, -9.1930e-02, -3.5799e-04, -1.1099e+00,\n",
       "         2.8633e-01,  8.9818e-01,  1.5169e-01,  2.3956e-01,  5.4681e-02,\n",
       "         4.1019e-02, -3.5006e-01,  2.4537e-01,  3.0713e-01,  2.0568e-01,\n",
       "         5.0113e-01,  5.3909e-01, -1.9615e+00, -2.1872e-01,  2.4190e-01,\n",
       "         6.3920e-01, -4.8349e-01,  6.4142e-01,  3.3689e-01, -3.2126e-01,\n",
       "         1.8933e-01,  5.9532e-02, -3.5209e-01,  8.4972e-04,  8.7736e-02,\n",
       "        -4.3746e-02,  7.7629e-01,  2.6972e-01,  1.4943e+00,  5.7005e-01,\n",
       "        -1.5999e-01,  4.4767e-01, -1.6754e-01,  3.1340e-01,  3.8478e-01,\n",
       "        -4.2999e-01, -6.1646e-02,  1.7813e-02, -5.5264e-01,  1.8654e-01,\n",
       "        -1.6280e-01,  7.4638e-01,  3.3080e-01,  3.7753e-01, -9.5671e-02,\n",
       "         3.7228e-01,  8.9878e-02, -1.4843e-01, -1.2373e-01, -1.3399e-01,\n",
       "         2.7955e-01, -5.7126e-01, -2.2330e-01,  9.4120e-01,  3.9736e-01,\n",
       "         1.3705e-01, -4.2874e-01,  6.7698e-01,  3.0552e-01, -8.2887e-01,\n",
       "        -3.6827e-01, -3.9308e-01, -9.7854e-01,  5.0361e-01,  4.9222e-01,\n",
       "        -6.1905e-01,  4.1020e-01,  1.0557e-01,  1.0407e+00,  2.3982e-02,\n",
       "        -4.1565e-01, -3.0648e-01, -9.1489e-01, -2.2650e-01, -9.1947e-02,\n",
       "        -9.2445e-02, -2.1591e-01,  2.3170e-01,  1.4660e-01,  2.2684e-01,\n",
       "         5.3403e-01, -3.0953e-01, -4.9599e-01, -2.1639e-01,  9.2002e-02,\n",
       "         8.2118e-01, -8.1299e-01, -6.1592e-01,  6.0114e-02,  1.2146e+00,\n",
       "         1.4170e-01,  5.8898e-01, -5.0243e-01,  8.4921e-01, -3.3243e-01,\n",
       "         4.9612e-01, -5.0034e-01, -3.2182e+00, -5.9081e-01, -3.8287e-01,\n",
       "        -2.5839e-02,  3.2023e-01, -4.6811e-01, -6.3458e-02, -7.7299e-01,\n",
       "         8.1514e-01,  2.4744e-01,  3.4915e-01, -8.9388e-02, -4.9026e-01,\n",
       "        -2.4377e-01,  1.4860e+00, -1.8929e-01])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fceeb32b91736",
   "metadata": {},
   "source": [
    "# Handling multiple Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95d017898cbbddc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:06:39.218339Z",
     "start_time": "2024-04-09T14:06:39.197202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_ids\n",
      "[101, 2021, 2054, 2055, 2117, 6350, 1029, 102]\n",
      "[101, 2123, 1005, 1056, 2228, 2002, 4282, 2055, 2117, 6350, 1010, 28315, 1012, 102]\n",
      "[101, 2054, 2055, 5408, 14625, 1029, 102]\n",
      "\n",
      "token_type_ids\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "attention_mask\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_inputs = tokenizer(batch_sentences)\n",
    "\n",
    "# Wir definieren eine kleine Hilfsfunktion, um die Parameter ausgeben zu k√∂nnen:\n",
    "def outp (inputs):\n",
    "    for name in ['input_ids', 'token_type_ids', 'attention_mask']:\n",
    "        if name in inputs:\n",
    "            print(f\"\\n{name}\")\n",
    "            for i in range (len (inputs[name])):\n",
    "                print(inputs[name][i])\n",
    "\n",
    "outp(encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe850d92ce68a6",
   "metadata": {},
   "source": [
    "# Padding & Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d21cb67847215d41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:06:41.035249Z",
     "start_time": "2024-04-09T14:06:41.019575Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_inputs = tokenizer(batch_sentences, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e368cc043a359bb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:06:42.392584Z",
     "start_time": "2024-04-09T14:06:42.376692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_ids\n",
      "[101, 2021, 2054, 2055, 2117, 6350, 1029, 102, 0, 0, 0, 0, 0, 0]\n",
      "[101, 2123, 1005, 1056, 2228, 2002, 4282, 2055, 2117, 6350, 1010, 28315, 1012, 102]\n",
      "[101, 2054, 2055, 5408, 14625, 1029, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "token_type_ids\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "attention_mask\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "outp(encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a06df9694ddacd9",
   "metadata": {},
   "source": [
    "# Word based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8303a2d7b27e730",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:06:44.159503Z",
     "start_time": "2024-04-09T14:06:44.143836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['but', 'what', 'about', 'second', 'breakfast', '?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'But what about second breakfast?'\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4963e43e2391fdf",
   "metadata": {},
   "source": [
    "# Sub-Word based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b04a5f98d05bfbf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:06:46.971312Z",
     "start_time": "2024-04-09T14:06:46.137742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñÅDon | ' | t | ‚ñÅyou | ‚ñÅlove | ‚ñÅ | ü§ó | ‚ñÅ | Transform | ers | ? | ‚ñÅWe | ‚ñÅsure | ‚ñÅdo | . | "
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet/xlnet-base-cased\")\n",
    "output = tokenizer.tokenize(\"Don't you love ü§ó Transformers? We sure do.\")\n",
    "for i in range (len (output)):\n",
    "    print(output[i], end=' | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca3ffc85373be9",
   "metadata": {},
   "source": [
    "- Transformers wurde in zwei Sub-Words aufgeteilt\n",
    "- Satzzeichen habe eigene Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65824e3fa124182",
   "metadata": {},
   "source": [
    "# Your own Tokenizer\n",
    "\n",
    "## Einen Textcorpus laden\n",
    "[Credits to huggingface: https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb]\n",
    "\n",
    "## Ein Dataset von huggingface laden:\n",
    "Wir laden zun√§chst das \"Wikitext\" Dataset von huggingface. Darin enthalten sind Beispieltexte von Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6deb2878c516557",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:06:50.471862Z",
     "start_time": "2024-04-09T14:06:49.963665Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e47575139c782d0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:02.970897Z",
     "start_time": "2024-04-09T14:06:53.178387Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca268ceb7d29249",
   "metadata": {},
   "source": [
    "# Zugriffsm√∂glichkeiten auf das DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f65d7a97bfafe0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:05.839270Z",
     "start_time": "2024-04-09T14:07:05.823632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1801350\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99bb13755de4b029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:06.934781Z",
     "start_time": "2024-04-09T14:07:06.919079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \" Le souper de Beaucaire was a political pamphlet written by Napoleon Bonaparte in 1793 . With the French Revolution into its fourth year , civil war had spread across France between various rival political factions . Napoleon was involved in military action , on the government 's side , against some rebellious cities of southern France . It was during these events , in 1793 , that he spoke with four merchants from the Midi and heard their views . As a loyal soldier of the Republic he responded in turn , set on dispelling the fears of the merchants and discouraging their beliefs . He later wrote about his conversation in the form of a pamphlet , calling for an end to the civil war . \\n\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "553f72a4db9ceca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:08.367922Z",
     "start_time": "2024-04-09T14:07:08.353418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [\" Le souper de Beaucaire was a political pamphlet written by Napoleon Bonaparte in 1793 . With the French Revolution into its fourth year , civil war had spread across France between various rival political factions . Napoleon was involved in military action , on the government 's side , against some rebellious cities of southern France . It was during these events , in 1793 , that he spoke with four merchants from the Midi and heard their views . As a loyal soldier of the Republic he responded in turn , set on dispelling the fears of the merchants and discouraging their beliefs . He later wrote about his conversation in the form of a pamphlet , calling for an end to the civil war . \\n\",\n",
       "  '',\n",
       "  ' = = Background = = \\n',\n",
       "  '',\n",
       "  ' During the French Revolution the National Convention became the executive power of France , following the execution of King Louis XVI . With powerful members , such as Maximilien Robespierre and Georges Danton , the Jacobin Club , a French political party established in 1790 , at the birth of the revolution , managed to secure control of the government and pursue the revolution to their own ends , culminating in a \" Reign of Terror \" . Its repressive policies resulted in insurrection across much of France , including the three largest cities after Paris , namely Lyon , Marseille and Toulon , in the south of France . \\n']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[30000:30005]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf13d35baaf6fef",
   "metadata": {},
   "source": [
    "# DataSet f√ºr den Trainnigsprozess bereitstellen (Corpus)\n",
    "## A) als Liste von Listen\n",
    "Dies l√§sst sich einfach umsetzen, hat aber den Nachteil, dass die Daten im Hauptspeicher gehalten werden m√ºssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43ca9869e5a986ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:10.675512Z",
     "start_time": "2024-04-09T14:07:10.628186Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "all_texts = [dataset[i : i + batch_size][\"text\"] for i in range(0, len(dataset), batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33150b55f9cdfb1",
   "metadata": {},
   "source": [
    "## B) Als Iterator\n",
    "Ein (Python-) Iterator ist eine M√∂glichkeit, Daten sukzessive nach und nach zu laden. Da das DataSet die Daten auf der Festplatte vorh√§lt, wird f√ºr den Iterator immer nur ein Teil der Daten in den Hauptspeicher geladen.\n",
    "\n",
    "Wir definieren diesen hier als Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4997ed561a96c88b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:12.402730Z",
     "start_time": "2024-04-09T14:07:12.387102Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_iterator():\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e5b5cdba7843d",
   "metadata": {},
   "source": [
    "# Verschiedene Tokenizer aufbauen\n",
    "# 1) Die Architektur eines vorhandenen Tokenizers clonen aber selbst trainieren\n",
    "[Credits to: https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6e76835128407a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:15.244135Z",
     "start_time": "2024-04-09T14:07:15.016485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dieser Tokenizer ist geeignet.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Wir stellen sicher, dass wir eine \"fast\" Version der Architektur geladen haben, um das Training im Rahmen zu halten\n",
    "if not tokenizer.is_fast:\n",
    "    raise (\"Dieser Tokenizer ist nicht geeignet.\")\n",
    "else:\n",
    "    print(\"Dieser Tokenizer ist geeignet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1c6c86ed70809e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:18.398854Z",
     "start_time": "2024-04-09T14:07:16.575746Z"
    }
   },
   "outputs": [],
   "source": [
    "gpt_clone_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6034d80b1d86917b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:23.496955Z",
     "start_time": "2024-04-09T14:07:23.481324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_ids\n",
      "[799, 14477, 559, 421, 1138, 11367, 6875, 321, 259, 2051, 22798, 1753, 369, 9402, 20243, 284, 18437, 272, 1756, 261, 1535, 5176, 649, 529, 2381, 606, 264, 2991, 1018, 455, 4617, 1999, 2328, 771, 2047, 5051, 2051, 14665, 272, 9402, 321, 2559, 284, 1902, 2275, 264, 325, 261, 1385, 329, 83, 1615, 264, 901, 840, 23028, 4536, 282, 2620, 2328, 272, 621, 321, 732, 1128, 2512, 264, 284, 18437, 264, 360, 354, 7103, 357, 833, 13714, 402, 261, 2653, 73, 289, 5001, 531, 5925, 272, 739, 259, 7259, 9361, 282, 261, 2648, 354, 5981, 284, 1324, 264, 915, 325, 2743, 2083, 261, 12922, 282, 261, 13714, 289, 1106, 11149, 286, 531, 11155, 272, 538, 772, 1388, 759, 395, 11610, 284, 261, 839, 282, 259, 22798, 264, 4409, 338, 386, 797, 290, 261, 2991, 1018, 272, 316]\n",
      "[]\n",
      "[301, 301, 4789, 301, 301, 316]\n",
      "[]\n",
      "[1346, 261, 1535, 5176, 261, 1509, 9764, 920, 261, 4626, 1421, 282, 2328, 264, 1209, 261, 9407, 282, 1279, 3127, 15015, 41, 272, 1756, 4310, 1529, 264, 841, 346, 13531, 3242, 274, 2986, 1302, 945, 265, 289, 19204, 361, 8889, 264, 261, 7077, 260, 3149, 264, 259, 1535, 2051, 2341, 2151, 284, 16688, 264, 380, 261, 3624, 282, 261, 6297, 264, 4003, 290, 6230, 1585, 282, 261, 1385, 289, 9229, 261, 6297, 290, 531, 1082, 5030, 264, 18489, 284, 259, 306, 22102, 282, 19277, 306, 272, 3501, 940, 4221, 7952, 3659, 284, 1115, 21636, 1999, 1366, 282, 2328, 264, 927, 261, 777, 2606, 4536, 614, 4284, 264, 14512, 19943, 264, 22049, 289, 19363, 263, 264, 284, 261, 1344, 282, 2328, 272, 316]\n",
      "\n",
      "attention_mask\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[]\n",
      "[1, 1, 1, 1, 1, 1]\n",
      "[]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "outp(gpt_clone_tokenizer(dataset[30000:30005][\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d527b5f9df024b1",
   "metadata": {},
   "source": [
    "### Diesen (trainierten!) Tokenizer k√∂nnen wir auch zur Wiederverwertung speichern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0e5c11cb814806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:25.594616Z",
     "start_time": "2024-04-09T14:07:25.571983Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/gpt_clone_tokenizer\\\\tokenizer_config.json',\n",
       " '../data/gpt_clone_tokenizer\\\\special_tokens_map.json',\n",
       " '../data/gpt_clone_tokenizer\\\\vocab.json',\n",
       " '../data/gpt_clone_tokenizer\\\\merges.txt',\n",
       " '../data/gpt_clone_tokenizer\\\\added_tokens.json',\n",
       " '../data/gpt_clone_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_clone_tokenizer.save_pretrained(\"../data/gpt_clone_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f976d524bd4699f5",
   "metadata": {},
   "source": [
    "## 2) Architektur eines Tokenizers selbst aufbauen\n",
    "### Beispiel: Byte Pair Encodings (wie f√ºr GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8dc6d7507c64152",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:27.877400Z",
     "start_time": "2024-04-09T14:07:27.861741Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ben√∂tigte Imports\n",
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e3b0db74be78",
   "metadata": {},
   "source": [
    "## Tokenizer-Pipeline\n",
    "\"Um zu verstehen, wie man einen Tokenizer von Grund auf erstellt, m√ºssen wir ein wenig mehr in die Tokenizer-Bibliothek und die Tokenisierungs-Pipeline eintauchen. Diese Pipeline besteht aus mehreren Schritten:\n",
    "\n",
    "* **Normalisierung**: F√ºhrt alle anf√§nglichen Transformationen √ºber die anf√§ngliche Eingabezeichenkette aus. Wenn Sie z.B. einen Text klein schreiben, ihn vielleicht entfernen oder einen der √ºblichen Unicode-Normalisierungsprozesse anwenden wollen, f√ºgen Sie einen Normalizer hinzu.\n",
    "* **Pre-Tokenizer**: Verantwortlich f√ºr die Aufteilung der urspr√ºnglichen Eingabezeichenfolge. Das ist die Komponente, die entscheidet, wo und wie die urspr√ºngliche Zeichenkette vorsegmentiert wird. Das einfachste Beispiel w√§re, einfach an Leerzeichen zu trennen.\n",
    "* **Modell**: √úbernimmt die gesamte Erkennung und Generierung von Sub-Token. Dieser Teil kann trainiert werden und ist wirklich von den Eingabedaten abh√§ngig.\n",
    "* **Post-Processing**: Bietet erweiterte Konstruktionsfunktionen, um mit einigen der Transformers-basierten SoTA-Modelle kompatibel zu sein. F√ºr BERT wird der tokenisierte Satz beispielsweise um [CLS]- und [SEP]-Token \"verpackt\".\"\n",
    "\n",
    "Vgl. https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb32439cddfcc5",
   "metadata": {},
   "source": [
    "### Ein BPE-Tokenizer ben√∂tigt einen Pre-Tokenizer (word based tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab9eddb1389c7009",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:31.437486Z",
     "start_time": "2024-04-09T14:07:31.421746Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basismodell ist ein BPE-Tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49badaca-841a-4238-83ed-0b5dfa67aa94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:32.328260Z",
     "start_time": "2024-04-09T14:07:32.314985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print (tokenizer.pre_tokenizer)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33d8cafd73ffce65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:33.529129Z",
     "start_time": "2024-04-09T14:07:33.513497Z"
    }
   },
   "outputs": [],
   "source": [
    "# BPE Byte Level Pre-Tokenizer werden √ºber huggingface zur Verf√ºgung gestellt.\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Tokenizer sind toll!\")\n",
    "# add_prefix_space=False verhindert, dass vor dem ersten Wort ebenfalls das Trennzeichen 'ƒ†' erscheint. Alle weiteren Token werden mit diesem Zeichen eingeleitet.\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c356680f7931dcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:35.137421Z",
     "start_time": "2024-04-09T14:07:35.121411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tokenizer', (0, 9)),\n",
       " ('ƒ†sind', (9, 14)),\n",
       " ('ƒ†toll', (14, 19)),\n",
       " ('!', (19, 20))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ergebnisse des Pre-Tokenizers anziegen\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Tokenizer sind toll!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955ef5c8c7880b6",
   "metadata": {},
   "source": [
    "### Unseren Tokenizer trainieren\n",
    "Der Tokenizer muss trainiert werden, denn er muss die Byte Pairs lernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e633cafc47961af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:40.743815Z",
     "start_time": "2024-04-09T14:07:39.322667Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f0743ffc15f17",
   "metadata": {},
   "source": [
    "### Post-Prozessing\n",
    "Der Tokenizer erh√§lt noch ein Post-Prozessing.\n",
    "Der ByteLevel PostProcessor k√ºmmert sich um das Trimming der Offsets. Whitespaces k√∂nnen optional ebenfalls getrimmt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b81bd71fc8547dcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:41.984439Z",
     "start_time": "2024-04-09T14:07:41.968425Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131aec6071353bf1",
   "metadata": {},
   "source": [
    "### Tokenizer f√ºr Huggingface Framework \"verpacken\"\n",
    "HuggingFace stellt Wrapper zur Verf√ºgung, die sicherstellen, dass die Tokenizer zu den APIs der Modelle passen.\n",
    "\n",
    "Hier verwenden wir einen GPT2TokenizerFast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ecbef6e26ed32a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:44.368799Z",
     "start_time": "2024-04-09T14:07:44.321991Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "custom_gpt_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3486153449ec111c",
   "metadata": {},
   "source": [
    "### Neuen Tokenizer anwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4daffd63fef1e8f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:07:47.359235Z",
     "start_time": "2024-04-09T14:07:47.353229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_ids\n",
      "[747, 14425, 507, 369, 1086, 11315, 6823, 269, 207, 1999, 22746, 1701, 317, 9350, 20191, 232, 18385, 220, 1704, 209, 1483, 5124, 597, 477, 2329, 554, 212, 2939, 966, 403, 4565, 1947, 2276, 719, 1995, 4999, 1999, 14613, 220, 9350, 269, 2507, 232, 1850, 2223, 212, 273, 209, 1333, 277, 83, 1563, 212, 849, 788, 22976, 4484, 230, 2568, 2276, 220, 569, 269, 680, 1076, 2460, 212, 232, 18385, 212, 308, 302, 7051, 305, 781, 13662, 350, 209, 2601, 73, 237, 4949, 479, 5873, 220, 687, 207, 7207, 9309, 230, 209, 2596, 302, 5929, 232, 1272, 212, 863, 273, 2691, 2031, 209, 12870, 230, 209, 13662, 237, 1054, 11097, 234, 479, 11103, 220, 486, 720, 1336, 707, 343, 11558, 232, 209, 787, 230, 207, 22746, 212, 4357, 286, 334, 745, 238, 209, 2939, 966, 220, 264]\n",
      "[]\n",
      "[249, 249, 4737, 249, 249, 264]\n",
      "[]\n",
      "[1294, 209, 1483, 5124, 209, 1457, 9712, 868, 209, 4574, 1369, 230, 2276, 212, 1157, 209, 9355, 230, 1227, 3075, 14963, 41, 220, 1704, 4258, 1477, 212, 789, 294, 13479, 3190, 222, 2934, 1250, 893, 213, 237, 19152, 309, 8837, 212, 209, 7025, 208, 3097, 212, 207, 1483, 1999, 2289, 2099, 232, 16636, 212, 328, 209, 3572, 230, 209, 6245, 212, 3951, 238, 6178, 1533, 230, 209, 1333, 237, 9177, 209, 6245, 238, 479, 1030, 4978, 212, 18437, 232, 207, 254, 22050, 230, 19225, 254, 220, 3449, 888, 4169, 7900, 3607, 232, 1063, 21584, 1947, 1314, 230, 2276, 212, 875, 209, 725, 2554, 4484, 562, 4232, 212, 14460, 19891, 212, 21997, 237, 19311, 211, 212, 232, 209, 1292, 230, 2276, 220, 264]\n",
      "\n",
      "attention_mask\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[]\n",
      "[1, 1, 1, 1, 1, 1]\n",
      "[]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "outp(custom_gpt_tokenizer(dataset[30000:30005][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5445ac8caa86c331",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
