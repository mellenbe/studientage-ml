{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a1eebf",
   "metadata": {
    "colab_type": "text",
    "id": "Vp3XPuaTu9jl"
   },
   "source": [
    "\n",
    "# Textgenerierung: Verwendung verschiedener Dekodierungsmethoden f√ºr die Spracherzeugung mit Transformatoren\n",
    "\n",
    "[Credits to: https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dea498",
   "metadata": {
    "colab_type": "text",
    "id": "KxLvv6UaPa33"
   },
   "source": [
    "### **Einf√ºhrung**\n",
    "\n",
    "In den letzten Jahren ist das Interesse an der Generierung von Sprache dank des Aufkommens gro√üer transformatorbasierter Sprachmodelle, die auf Millionen von Webseiten trainiert wurden, wie z. B. das ber√ºhmte [GPT2-Modell](https://openai.com/blog/better-language-models/) von OpenAI, gestiegen. Die Ergebnisse der konditionierten, offenen Spracherzeugung sind beeindruckend, z. B. [GPT2 on unicorns](https://openai.com/blog/better-language-models/#samples), [XLNet](https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e), [Controlled language with CTRL](https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/). Neben der verbesserten Transformator-Architektur und der gro√üen Menge an un√ºberwachten Trainingsdaten haben auch **bessere Dekodierungsmethoden** eine wichtige Rolle gespielt.\n",
    "\n",
    "Dieses Notebook gibt einen kurzen √úberblick √ºber verschiedene Dekodierungsstrategien und zeigt vor allem, wie *Sie* diese mit sehr geringem Aufwand mit der beliebten Bibliothek `transformers` umsetzen k√∂nnen!\n",
    "\n",
    "Alle der folgenden Funktionen k√∂nnen f√ºr die **autoregressive** Spracherzeugung verwendet werden ([hier](http://jalammar.github.io/illustrated-gpt2/) eine Auffrischung). Kurz gesagt, *autoregressive* Sprachgenerierung basiert auf der Annahme, dass die Wahrscheinlichkeitsverteilung einer Wortfolge in das Produkt der bedingten n√§chsten Wortverteilungen zerlegt werden kann:\n",
    "$$ P(w_{1:T} | W_0 ) = \\prod_{t=1}^T P(w_{t} | w_{1: t-1}, W_0) \\text{ ,with } w_{1: 0} = \\emptyset, $$\n",
    "\n",
    "und $W_0$ ist die anf√§ngliche *Kontext*-Wortfolge. Die L√§nge $T$ der Wortfolge wird √ºblicherweise *on-the-fly* bestimmt und entspricht dem Zeitschritt $t=T$, in dem das EOS-Token aus $P(w_{t} | w_{1: t-1}, W_{0})$ erzeugt wird.\n",
    "\n",
    "\n",
    "Auto-regressive Spracherzeugung ist nun verf√ºgbar f√ºr `GPT2`, `XLNet`, `OpenAi-GPT`, `CTRL`, `TransfoXL`, `XLM`, `Bart`, `T5` sowohl in PyTorch als auch in Tensorflow >= 2.0!\n",
    "\n",
    "Wir geben einen √úberblick √ºber die derzeit bekanntesten Dekodierungsmethoden, vor allem *Greedy search*, *Beam search*, *Top-K sampling* und *Top-p sampling*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254f830b",
   "metadata": {
    "colab_type": "text",
    "id": "Si4GyYhOQMzi"
   },
   "source": [
    "Zun√§chst m√ºssen Sie die Bibliothek \"transformers\" und das Modell laden. Wir werden GPT2 in Tensorflow 2.1 zur Demonstration verwenden, aber die API ist 1 zu 1 die gleiche f√ºr PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2f4b6c0",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ue2kOQhXTAMU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# das EOS-Token als PAD-Token hinzuf√ºgen, um Warnungen zu vermeiden\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d72c4",
   "metadata": {
    "colab_type": "text",
    "id": "a8Y7cgu9ohXP"
   },
   "source": [
    "### **Greedy Search**\n",
    "\n",
    "Die Greedy Search w√§hlt einfach das Wort mit der h√∂chsten Wahrscheinlichkeit als n√§chstes Wort: $w_t = argmax_{w}P(w | w_{1:t-1})$ in jedem Zeitschritt $t$. Die folgende Skizze zeigt die Greedy Search.\n",
    "\n",
    "![Gierige Suche](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/greedy_search.png)\n",
    "\n",
    "Ausgehend von dem Wort $\\text{\"The\"}$, w√§hlt der Greedy Algorithmus das n√§chste Wort mit der h√∂chsten Wahrscheinlichkeit $\\text{\"nice\"}$ und so weiter, so dass die endg√ºltige generierte Wortfolge $\\text{\"The\", \"nice\", \"woman\"}$ mit einer Gesamtwahrscheinlichkeit von $0,5 \\mal 0,4 = 0,2$ ist.\n",
    "\n",
    "Im Folgenden werden wir Wortfolgen mit GPT2 auf dem Kontext $(\\text{\"I\", \"enjoy\", \"walking\", \"with\", \"my\", \"cute\", \"dog\"})$ erzeugen. Schauen wir uns an, wie die gierige Suche in `Transformers` wie folgt verwendet werden kann:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b61dea6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "OWLd_J6lXz_t",
    "outputId": "3b9dfd1e-21e6-44f4-f27f-8e975010f9af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure if I'll\n"
     ]
    }
   ],
   "source": [
    "# Kodierung des Kontexts, der f√ºr die Generierung erforderlich ist\n",
    "input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='tf')\n",
    "\n",
    "# Text generieren, bis die Ausgabel√§nge (die die Kontextl√§nge einschlie√üt) 50 erreicht\n",
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "print(\"Ausgabe:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162447f6",
   "metadata": {
    "colab_type": "text",
    "id": "BBn1ePmJvhrl"
   },
   "source": [
    "Na also! Wir haben unseren ersten kurzen Text mit GPT2 generiert üòä. Die generierten W√∂rter, die dem Kontext folgen, sind vern√ºnftig, aber das Modell f√§ngt schnell an, sich zu wiederholen! Dies ist ein sehr h√§ufiges Problem bei der Spracherzeugung im Allgemeinen und scheint bei der Greedy- und Beam-Suche noch h√§ufiger aufzutreten - siehe [Vijayakumar et al., 2016](https://arxiv.org/abs/1610.02424) und [Shao et al., 2017](https://arxiv.org/abs/1701.03185).\n",
    "\n",
    "Der gr√∂√üte Nachteil der Greedy Search ist jedoch, dass sie W√∂rter mit hoher Wahrscheinlichkeit √ºbersieht, die sich hinter einem Wort mit niedriger Wahrscheinlichkeit verbergen, wie in unserer Skizze oben zu sehen ist:\n",
    "\n",
    "Das Wort $\\text{\"has\"}$ mit seiner hohen bedingten Wahrscheinlichkeit von $0,9$ ist hinter dem Wort $\\text{\"dog\"}$ versteckt, das nur die zweith√∂chste bedingte Wahrscheinlichkeit hat, so dass die gierige Suche die Wortfolge $\\text{\"The\"}, \\text{\"dog\"}, \\text{\"has\"}$ √ºbersieht.\n",
    "\n",
    "Zum Gl√ºck gibt es die Strahlensuche, um dieses Problem zu lindern!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaff72d",
   "metadata": {
    "colab_type": "text",
    "id": "g8DnXZ1WiuNd"
   },
   "source": [
    "### **Beam Search**\n",
    "\n",
    "Die Beam Search verringert das Risiko, versteckte Wortfolgen mit hoher Wahrscheinlichkeit zu √ºbersehen, indem bei jedem Zeitschritt die wahrscheinlichsten `num_beams` der Hypothesen beibehalten und schlie√ülich die Hypothese mit der insgesamt h√∂chsten Wahrscheinlichkeit ausgew√§hlt wird. Zur Veranschaulichung nehmen wir `num_beams=2`:\n",
    "\n",
    "![Beam search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n",
    "\n",
    "Im Zeitschritt $1$ wird neben der wahrscheinlichsten Hypothese $\\text{\"The\", \"nice\"}$ auch die zweitwahrscheinlichste $\\text{\"The\", \"dog\"}$ verfolgt. Im Zeitschritt $2$ stellt die Strahlensuche fest, dass die Wortfolge $\\text{\"The\", \"dog\", \"has\"}$ mit $0,36$ eine h√∂here Wahrscheinlichkeit hat als $\\text{\"The\", \"nice\", \"woman\"}$, die $0,2$ hat. Toll, es hat die wahrscheinlichste Wortfolge in unserem Spielzeugbeispiel gefunden!\n",
    "\n",
    "Die Balkensuche findet immer eine Ausgabesequenz mit h√∂herer Wahrscheinlichkeit als die gierige Suche, aber es ist nicht garantiert, dass sie die wahrscheinlichste Ausgabe findet.\n",
    "\n",
    "Schauen wir uns an, wie die Strahlensuche in `Transformers` verwendet werden kann. Wir setzen `num_beams > 1` und `early_stopping=True`, so dass die Erzeugung beendet ist, wenn alle Strahlhypothesen das EOS-Token erreicht haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c443d5d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "R1R5kx30Ynej",
    "outputId": "574f068b-f418-48b5-8334-8451d2221032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with him again. I'm not sure if I'll\n"
     ]
    }
   ],
   "source": [
    "# Beam Search und Fr√ºhstopp aktivieren\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Ausgabe:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f820c8",
   "metadata": {
    "colab_type": "text",
    "id": "AZ6xs-KLi9jT"
   },
   "source": [
    "Das Ergebnis ist zwar deutlich fl√ºssiger, aber die Ausgabe enth√§lt immer noch Wiederholungen der gleichen Wortfolgen.  \n",
    "Eine einfache Abhilfe ist die Einf√ºhrung von *n-Grammen* (*a.k.a.* Wortfolgen aus $n$ W√∂rtern), wie sie von [Paulus et al. (2017)](https://arxiv.org/abs/1705.04304) und [Klein et al. (2017)](https://arxiv.org/abs/1701.02810) eingef√ºhrt wurden. Die g√§ngigste *n-Gramme*-Penalty stellt sicher, dass kein *n-Gramm* zweimal vorkommt, indem sie die Wahrscheinlichkeit der n√§chsten W√∂rter, die ein bereits gesehenes *n-Gramm* erzeugen k√∂nnten, manuell auf $0$ setzt.\n",
    "\n",
    "Probieren wir es aus, indem wir `no_repeat_ngram_size=2` setzen, so dass kein *2-Gramm* zweimal erscheint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c1c8be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "jy3iVJgfnkMi",
    "outputId": "4d3e6511-711a-4594-a715-aaeb6e48e1a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n"
     ]
    }
   ],
   "source": [
    "# no_repeat_ngram_size auf 2 setzen\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Ausgabe:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0768aae7",
   "metadata": {
    "colab_type": "text",
    "id": "nxsksOGDpmA0"
   },
   "source": [
    "Sch√∂n, das sieht viel besser aus! Wir k√∂nnen sehen, dass die Wiederholung nicht mehr vorkommt. Dennoch m√ºssen *n-gram* penalties mit Vorsicht verwendet werden. Ein Artikel, der √ºber die Stadt *New York* erstellt wird, sollte keine *2-Gramm*-Strafe verwenden, da sonst der Name der Stadt nur einmal im gesamten Text vorkommt!\n",
    "\n",
    "Ein weiteres wichtiges Merkmal der Beam Search (Strahlensuche) ist, dass wir die Top-Strahlen nach der Generierung vergleichen und den generierten Strahl ausw√§hlen k√∂nnen, der am besten zu unserem Zweck passt.\n",
    "\n",
    "In `transformers` setzen wir einfach den Parameter `num_return_sequences` auf die Anzahl der am h√∂chsten bewerteten Balken, die zur√ºckgegeben werden sollen. Stellen Sie aber sicher, dass `num_return_sequences <= num_beams`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34624c6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "5ClO3VphqGp6",
    "outputId": "2296891c-024f-4fd2-9071-bff7c11a3e04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n",
      "1: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to get back to\n",
      "2: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n",
      "3: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to get back to\n",
      "4: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a step\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# jetzt haben wir 3 Ausgabesequenzen\n",
    "print(\"Ausgabe:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e92b89d",
   "metadata": {
    "colab_type": "text",
    "id": "HhLKyfdbsjXc"
   },
   "source": [
    "Wie man sieht, unterscheiden sich die f√ºnf Strahlenhypothesen nur geringf√ºgig voneinander - was bei nur 5 Strahlen nicht allzu √ºberraschend sein sollte.\n",
    "\n",
    "Bei der offenen Generierung wurden in letzter Zeit einige Gr√ºnde angef√ºhrt, warum die Beam Search m√∂glicherweise nicht die bestm√∂gliche Option ist:\n",
    "\n",
    "- Die Beam Search kann sehr gut bei Aufgaben funktionieren, bei denen die L√§nge der gew√ºnschten Generierung mehr oder weniger vorhersehbar ist, wie bei der maschinellen √úbersetzung oder Zusammenfassung - siehe [Murray et al. (2018)](https://arxiv.org/abs/1808.10006) und [Yang et al. (2018)](https://arxiv.org/abs/1808.09582). Dies gilt jedoch nicht f√ºr die Generierung mit offenem Ende, bei der die gew√ºnschte Ausgabel√§nge stark variieren kann, z. B. bei der Generierung von Dialogen und Geschichten.\n",
    "\n",
    "- Wir haben gesehen, dass die Beam Search stark unter der repetitiven Generierung leidet. Dies ist besonders schwer mit *n-Gramm*- oder anderen penalties bei der Generierung von Geschichten in den Griff zu bekommen, da die Suche nach einem guten Kompromiss zwischen erzwungener \"Nicht-Wiederholung\" und sich wiederholenden Zyklen identischer *n-Gramme* eine Menge Feinabstimmung erfordert.\n",
    "\n",
    "- Wie in [Ari Holtzman et al. (2019)] (https://arxiv.org/abs/1904.09751) argumentiert wird, folgt hochwertige menschliche Sprache nicht einer Verteilung von mit hoher Wahrscheinlichkeit folgenden W√∂rtern. Mit anderen Worten: Als Menschen wollen wir, dass der generierte Text uns √ºberrascht und nicht langweilig/vorhersehbar ist. Die Autoren zeigen dies sehr sch√∂n, indem sie die Wahrscheinlichkeit, die ein Modell einem menschlichen Text geben w√ºrde, im Vergleich zu dem, was die Beam Search tut, grafisch darstellen.\n",
    "\n",
    "![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)\n",
    "\n",
    "\n",
    "Also lasst uns aufh√∂ren, langweilig zu sein, und etwas Zuf√§lligkeit einf√ºhren ü§™."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f64c0c",
   "metadata": {
    "colab_type": "text",
    "id": "XbbIyK84wHq6"
   },
   "source": [
    "### **Sampling**\n",
    "\n",
    "In seiner einfachsten Form bedeutet Sampling die zuf√§llige Auswahl des n√§chsten Wortes $w_t$ gem√§√ü seiner bedingten Wahrscheinlichkeitsverteilung:\n",
    "\n",
    "$$w_t \\sim P(w|w_{1:t-1})$$\n",
    "\n",
    "Anhand des obigen Beispiels veranschaulicht die folgende Grafik die Spracherzeugung beim Sampling.\n",
    "\n",
    "![vanilla_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/sampling_search.png)\n",
    "\n",
    "Es wird deutlich, dass die Spracherzeugung mittels Sampling nicht mehr *deterministisch* ist. Das Wort\n",
    "$\\text{\"car\"}$ wird aus der bedingten Wahrscheinlichkeitsverteilung $P(w | \\text{\"The\"})$ entnommen, gefolgt von einer Stichprobe $\\text{\"drives\"}$ aus $P(w | \\text{\"The\"}, \\text{\"car\"})$.\n",
    "\n",
    "In `transformers` setzen wir `do_sample=True` und deaktivieren *Top-K* Sampling (mehr dazu sp√§ter) √ºber `top_k=0`. Im Folgenden werden wir zur Veranschaulichung `random_seed=0` festlegen. Es steht Ihnen frei, die `random_seed` zu √§ndern, um mit dem Modell herumzuspielen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968174c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "aRAz4D-Ks0_4",
    "outputId": "1b78d191-15f6-4cbe-e2b1-23c77366fc21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog and never seem to get nervous until it turns into battery unfrosted. (Laughs) Go ahead with the cover and bring it on. \"3 Light Doppelgangers\" by AFOB\n",
      "\n",
      "‚Ä¶\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seed setzen, um Ergebnisse zu reproduzieren. Sie k√∂nnen den Seed aber auch √§ndern, um andere Ergebnisse zu erhalten\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Stichproben aktivieren und top_k deaktivieren, indem top_k sampling auf 0 gesetzt wird\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Ausgabe:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6602836",
   "metadata": {
    "colab_type": "text",
    "id": "mQHuo911wfT-"
   },
   "source": [
    "Interessant! Der Text scheint in Ordnung zu sein - aber wenn man genauer hinsieht, ist er nicht sehr koh√§rent. Die *3-Gramme* *New Hand Sense* und *Local Batte Harness* sind sehr seltsam und klingen nicht so, als w√§ren sie von einem Menschen geschrieben worden. Das ist das gro√üe Problem beim Sampling von Wortfolgen: Die Modelle erzeugen oft inkoh√§rentes Kauderwelsch, *cf.* [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751).\n",
    "\n",
    "Ein Trick besteht darin, die Verteilung $P(w|w_{1:t-1})$ sch√§rfer zu machen (die Wahrscheinlichkeit von W√∂rtern mit hoher Wahrscheinlichkeit zu erh√∂hen und die Wahrscheinlichkeit von W√∂rtern mit niedriger Wahrscheinlichkeit zu verringern), indem man die so genannte \"Temperatur\" des [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max) senkt.\n",
    "\n",
    "Ein Beispiel f√ºr die Anwendung der Temperatur auf unser obiges Beispiel k√∂nnte folgenderma√üen aussehen.\n",
    "\n",
    "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/sampling_search_with_temp.png?raw=true)\n",
    "\n",
    "Die bedingte Verteilung des n√§chsten Wortes in Schritt $t=1$ wird viel sch√§rfer, so dass das Wort $\\text{\"car\"}$ fast keine Chance mehr hat, ausgew√§hlt zu werden.\n",
    "\n",
    "\n",
    "Schauen wir uns an, wie wir die Verteilung in der Bibliothek abk√ºhlen k√∂nnen, indem wir `temperature=0.7` einstellen:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab6f94",
   "metadata": {
    "colab_type": "text",
    "id": "kzGuu24hZZnq"
   },
   "source": [
    "OK. Es gibt weniger seltsame n-Gramme und die Ausgabe ist jetzt etwas koh√§renter! W√§hrend die Anwendung von Temperatur eine Verteilung weniger zuf√§llig machen kann, wird das temperaturskalierte Sampling im Grenzfall, wenn `Temperatur` auf $0$ gesetzt wird, gleichbedeutend mit gieriger Dekodierung und leidet unter den gleichen Problemen wie zuvor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb38c7d9-68a5-4788-a273-50084eabc184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog. I've always been an avid dog lover. My favorite type of dog is one that has been trained to do many things. I do not have any special preferences for particular breeds. I don't think I have\n"
     ]
    }
   ],
   "source": [
    "# Seed setzen, um Ergebnisse zu reproduzieren. Es steht Ihnen frei, den Seed zu √§ndern, um andere Ergebnisse zu erhalten\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Temperatur verwenden, um die Empfindlichkeit gegen√ºber Kandidaten mit geringer Wahrscheinlichkeit zu verringern\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0, \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bad215",
   "metadata": {
    "colab_type": "text",
    "id": "binNTroyzQBu"
   },
   "source": [
    "### **Top-K Sampling**\n",
    "\n",
    "[Fan et. al (2018)](https://arxiv.org/pdf/1805.04833.pdf) f√ºhrte ein einfaches, aber sehr leistungsf√§higes Stichprobenverfahren ein, das sogenannte ***Top-K***-Sampling. Beim *Top-K*-Sampling werden die *K* wahrscheinlichsten n√§chsten W√∂rter gefiltert und die Wahrscheinlichkeitsmasse wird nur auf diese *K* n√§chsten W√∂rter umverteilt.\n",
    "GPT2 hat dieses Stichprobenverfahren √ºbernommen, was einer der Gr√ºnde f√ºr seinen Erfolg bei der Generierung von Geschichten war.\n",
    "\n",
    "Zur besseren Veranschaulichung des *Top-K*-Samplings erweitern wir im obigen Beispiel den Bereich der f√ºr beide Sampling-Schritte verwendeten W√∂rter von 3 auf 10 W√∂rter.\n",
    "\n",
    "![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n",
    "\n",
    "Nachdem wir $K = 6$ gesetzt haben, beschr√§nken wir unseren Stichprobenpool in beiden Schritten auf 6 W√∂rter. W√§hrend die 6 wahrscheinlichsten W√∂rter, definiert als $V_{\\text{top-K}}$, im ersten Schritt nur *ca.* zwei Drittel der gesamten Wahrscheinlichkeitsmasse umfassen, schlie√üen sie im zweiten Schritt fast die gesamte Wahrscheinlichkeitsmasse ein. Nichtsdestotrotz sehen wir, dass sie erfolgreich die eher seltsamen Kandidaten $\\text{\"nicht\", \"die\", \"klein\", \"gesagt\"}$ eliminiert\n",
    "im zweiten Schritt der Stichprobenziehung.\n",
    "\n",
    "\n",
    "Schauen wir uns an, wie *Top-K* in der Bibliothek verwendet werden kann, indem wir `top_k=50` einstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2412799",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "HBtDOdD0wx3l",
    "outputId": "cfc97fac-0956-42ee-a6e5-cad14fc942d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog in the street from 2 1/2 miles (just before sunset) to 3 miles. But there are a few things that you need to know! It's NOT A REPUBLICANS EXCELLENT!\n"
     ]
    }
   ],
   "source": [
    "# Seed setzen, um Ergebnisse zu reproduzieren. Es steht Ihnen frei, den Seed zu √§ndern, um andere Ergebnisse zu erhalten\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# top_k auf 50 setzen\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Ausgabe:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42fa30",
   "metadata": {
    "colab_type": "text",
    "id": "Y77H5m4ZmhEX"
   },
   "source": [
    "Gar nicht so schlecht! Der Text ist wohl der bisher am *menschlichsten* klingende Text.\n",
    "Ein Problem beim *Top-K*-Sampling ist jedoch, dass es die Anzahl der W√∂rter, die aus der n√§chsten Wortwahrscheinlichkeitsverteilung $P(w|w_{1:t-1})$ gefiltert werden, nicht dynamisch anpasst.\n",
    "Dies kann problematisch sein, da einige W√∂rter aus einer sehr scharfen Verteilung (Verteilung rechts im obigen Diagramm), andere hingegen aus einer viel flacheren Verteilung (Verteilung links im obigen Diagramm) gezogen werden k√∂nnten.\n",
    "\n",
    "In Schritt $t=1$ eliminiert *Top-K* die M√∂glichkeit\n",
    "$\\text{\"people\", \"big\", \"house\", \"cat\"}$, die als vern√ºnftige Kandidaten erscheinen. Andererseits nimmt das Verfahren in Schritt $t=2$ die wohl schlecht passenden W√∂rter $\\text{\"down\", \"a\"}$ in den Stichprobenpool auf. Eine Begrenzung des Stichprobenpools auf eine feste Gr√∂√üe *K* k√∂nnte also dazu f√ºhren, dass das Modell bei scharfen Verteilungen Kauderwelsch produziert und bei flachen Verteilungen die Kreativit√§t des Modells einschr√§nkt.\n",
    "Diese Intuition hat [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751) dazu veranlasst, das ***Top-p***- oder ***nucleus***-Sampling zu entwickeln.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a82e5a4",
   "metadata": {
    "colab_type": "text",
    "id": "ki9LAaexzV3H"
   },
   "source": [
    "### **Top-p (nucleus) sampling**\n",
    "\n",
    "Anstatt nur aus den wahrscheinlichsten *K* W√∂rtern eine Stichprobe zu ziehen, wird bei *Top-p* aus der kleinstm√∂glichen Menge von W√∂rtern ausgew√§hlt, deren kumulative Wahrscheinlichkeit die Wahrscheinlichkeit *p* √ºbersteigt. Die Wahrscheinlichkeitsmasse wird dann auf diese Menge von W√∂rtern umverteilt. Auf diese Weise kann die Gr√∂√üe der Wortmenge (*a.k.a.* die Anzahl der W√∂rter in der Menge) entsprechend der Wahrscheinlichkeitsverteilung des n√§chsten Wortes dynamisch zu- und abnehmen. Ok, das war sehr wortreich, lassen Sie uns das veranschaulichen.\n",
    "\n",
    "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/top_p_sampling.png?raw=true)\n",
    "\n",
    "Nachdem $p=0,92$ festgelegt wurde, w√§hlt *Top-p* die *minimale* Anzahl von W√∂rtern aus, die zusammen $p=92\\%$ der Wahrscheinlichkeitsmasse √ºberschreiten, definiert als $V_{\\text{top-p}}$. Im ersten Beispiel umfasste dies die 9 wahrscheinlichsten W√∂rter, w√§hrend im zweiten Beispiel nur die obersten 3 W√∂rter ausgew√§hlt werden m√ºssen, um 92 % zu √ºberschreiten. Eigentlich ganz einfach! Es ist zu erkennen, dass eine gro√üe Anzahl von W√∂rtern erhalten bleibt, bei denen das n√§chste Wort wohl weniger vorhersehbar ist, *z.B.* $P(w | \\text{\"The\"})$, und nur wenige W√∂rter, bei denen das n√§chste Wort vorhersehbarer erscheint, *z.B.* $P(w | \\text{\"The\", \"car\"})$.\n",
    "\n",
    "Also gut, dann probieren wir es mal mit `transformers` aus!\n",
    "Wir aktivieren *Top-p* Sampling, indem wir `0 < top_p < 1` setzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73a4f207",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "EvwIc7YAx77F",
    "outputId": "57e2b785-5dcb-4e06-9869-078b758b6a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog and very often see her immediately excited at moving on.\" - Alicia Davis\n",
      "\n",
      "Recently, running around Los Angeles on an expeditions had me thinking, \"What'd I get for free here?\" I worked as a\n"
     ]
    }
   ],
   "source": [
    "# Seed setzen, um Ergebnisse zu reproduzieren. Es steht Ihnen frei, den Seed zu √§ndern, um andere Ergebnisse zu erhalten\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Deaktivieren Sie das top_k-Sampling und w√§hlen Sie nur die 92% wahrscheinlichsten W√∂rter aus.\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_p=0.92,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Ausgabe:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c899c47",
   "metadata": {
    "colab_type": "text",
    "id": "tn-8gLaR4lat"
   },
   "source": [
    "Toll, das klingt, als h√§tte es ein Mensch geschrieben. Nun, vielleicht noch nicht ganz.\n",
    "\n",
    "Obwohl *Top-p* in der Theorie eleganter erscheint als *Top-K*, funktionieren beide Methoden in der Praxis gut. *Top-p* kann auch in Kombination mit *Top-K* verwendet werden, wodurch sehr niedrig eingestufte W√∂rter vermieden werden k√∂nnen und gleichzeitig eine dynamische Auswahl m√∂glich ist.\n",
    "\n",
    "Um schlie√ülich mehrere unabh√§ngig voneinander abgetastete Ausgaben zu erhalten, k√∂nnen wir *wieder* den Parameter `num_return_sequences > 1` setzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f8f8763",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "3kY8P9VG8Gi9",
    "outputId": "6103051e-1681-4ab9-a9c1-1fad437c299d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog\n",
      "\n",
      "In my life I love having fun\n",
      "\n",
      "What does it cost to feed a cat to get a heart attack?\n",
      "\n",
      "I think, this is actually a really good question: what are some of the\n",
      "1: I enjoy walking with my cute dog.\"\n",
      "\n",
      "We didn't realize it until the next day that her husband had become very busy, so we were not surprised when the dog went to visit him outside his apartment and suddenly she had the courage to ask\n",
      "2: I enjoy walking with my cute dog, I like to get a dog from the grocery store and it's been a pleasure to sit and talk to her (when she is sleeping), and she's been so happy with our visit to the supermarket (we\n"
     ]
    }
   ],
   "source": [
    "# Seed setzen, um Ergebnisse zu reproduzieren. Es steht Ihnen frei, den Seed zu √§ndern, um andere Ergebnisse zu erhalten\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# top_k = 50 und top_p = 0,95 und num_return_sequences = 3 setzen\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Ausgabe:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7227b70",
   "metadata": {
    "colab_type": "text",
    "id": "-vRPfMl88rk0"
   },
   "source": [
    "Cool, jetzt solltest du alle Werkzeuge haben, um dein Modell deine Geschichten mit `Transformers` schreiben zu lassen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2063588",
   "metadata": {
    "colab_type": "text",
    "id": "NsWd7e98Vcs3"
   },
   "source": [
    "### **Fazit**\n",
    "\n",
    "Als *ad-hoc* Dekodierungsmethoden scheinen *top-p* und *top-K* Sampling einen fl√ºssigeren Text zu produzieren als die traditionelle *Greedy*- und *Beam*-Suche bei der Erzeugung von Sprache mit offenem Ende.\n",
    "In letzter Zeit mehren sich jedoch die Hinweise darauf, dass die offensichtlichen Schw√§chen von *Greedy* und *Beam*-Suche - vor allem die Erzeugung sich wiederholender Wortfolgen - durch das Modell (insbesondere die Art und Weise, wie das Modell trainiert wird) und nicht durch die Dekodierungsmethode verursacht werden, *vgl.* [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf). Wie in [Welleck et al. (2020)](https://arxiv.org/abs/2002.02492) gezeigt wird, scheinen auch *top-K* und *top-p* Sampling unter der Erzeugung sich wiederholender Wortfolgen zu leiden.\n",
    "\n",
    "In [Welleck et al. (2019)] (https://arxiv.org/pdf/1908.04319.pdf) zeigen die Autoren, dass die *Beam*-Suche nach menschlicher Einsch√§tzung fl√ºssigere Texte erzeugen kann als das *Top-p*-Sampling, wenn man das Trainingsziel des Modells anpasst.\n",
    "\n",
    "Die Generierung offener Sprache ist ein sich rasch entwickelndes Forschungsgebiet, und wie so oft gibt es auch hier keine Einheitsmethode, so dass man sehen muss, was im eigenen Anwendungsfall am besten funktioniert.\n",
    "\n",
    "Gut, dass *du* in `transfomers` alle verschiedenen Dekodierungsmethoden ausprobieren kannst ü§ó.\n",
    "\n",
    "Dies war eine kurze Einf√ºhrung in die Verwendung verschiedener Dekodierungsmethoden in \"Transformatoren\" und in die j√ºngsten Trends bei der Erzeugung von Sprachen mit offenem Ende.\n",
    "\n",
    "Feedback und Fragen sind auf dem [Github-Repository] (https://github.com/huggingface/transformers) sehr willkommen.\n",
    "\n",
    "Weitere lustige Geschichten finden Sie unter [Schreiben mit Transformers] (https://transformer.huggingface.co).\n",
    "\n",
    "Vielen Dank an alle, die zu diesem Blogbeitrag beigetragen haben: Alexander Rush, Julien Chaumand, Thomas Wolf, Victor Sanh, Sam Shleifer, Cl√©ment Delangue, Yacine Jernite, Oliver √Östrand und John de Wasseige.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1442fb97",
   "metadata": {
    "colab_type": "text",
    "id": "w4CYi91h11yd"
   },
   "source": [
    "### **Anhang**\n",
    "\n",
    "Es gibt eine Reihe zus√§tzlicher Parameter f√ºr die Methode `generate`, die oben nicht erw√§hnt wurden. Wir werden sie hier kurz erkl√§ren!\n",
    "\n",
    "- min_length\" kann verwendet werden, um das Modell zu zwingen, kein EOS-Token zu produzieren (= den Satz nicht zu beenden), bevor \"min_length\" erreicht ist. Dies wird h√§ufig bei Zusammenfassungen verwendet, kann aber auch allgemein n√ºtzlich sein, wenn der Benutzer l√§ngere Ausgaben haben m√∂chte.\n",
    "- repetition_penalty\" kann verwendet werden, um W√∂rter zu bestrafen, die bereits generiert wurden oder zum Kontext geh√∂ren. Sie wurde erstmals von [Kesker et al. (2019)](https://arxiv.org/abs/1909.05858) eingef√ºhrt und wird auch im Trainingsziel in [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf) verwendet. Es kann recht effektiv sein, um Wiederholungen zu verhindern, scheint aber sehr empfindlich auf verschiedene Modelle und Anwendungsf√§lle zu reagieren, *siehe z.B.* diese [Diskussion](https://github.com/huggingface/transformers/pull/2303) auf Github.\n",
    "\n",
    "- `attention_mask` kann verwendet werden, um aufgef√ºllte Token zu maskieren\n",
    "- `pad_token_id`, `bos_token_id`, `eos_token_id`: Wenn das Modell diese Token nicht standardm√§√üig enth√§lt, kann der Benutzer manuell andere Token-IDs ausw√§hlen, um sie zu repr√§sentieren.\n",
    "\n",
    "F√ºr weitere Informationen schauen Sie bitte auch in die Funktion `generate` [docstring](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.TFPreTrainedModel.generate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c71316",
   "metadata": {},
   "source": [
    "# Weitere Links (mit Colab Notebooks)\n",
    "\n",
    "https://keras.io/examples/generative/gpt2_text_generation_with_kerasnlp/\n",
    "\n",
    "https://keras.io/examples/generative/gpt2_text_generation_with_kerasnlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe48194-e130-444d-ba0d-67b473131f27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "Formate": "md",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
