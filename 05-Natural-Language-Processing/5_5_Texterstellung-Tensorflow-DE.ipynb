{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a1eebf",
   "metadata": {
    "colab_type": "text",
    "id": "Vp3XPuaTu9jl"
   },
   "source": [
    "\n",
    "# Textgenerierung: Verwendung verschiedener Dekodierungsmethoden fÃ¼r die Spracherzeugung mit Transformatoren\n",
    "\n",
    "[Credits to: https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dea498",
   "metadata": {
    "colab_type": "text",
    "id": "KxLvv6UaPa33"
   },
   "source": [
    "### **EinfÃ¼hrung**\n",
    "\n",
    "In den letzten Jahren ist das Interesse an der Generierung von Sprache dank des Aufkommens groÃŸer transformatorbasierter Sprachmodelle, die auf Millionen von Webseiten trainiert wurden, wie z. B. das berÃ¼hmte [GPT2-Modell](https://openai.com/blog/better-language-models/) von OpenAI, gestiegen. Die Ergebnisse der konditionierten, offenen Spracherzeugung sind beeindruckend, z. B. [GPT2 on unicorns](https://openai.com/blog/better-language-models/#samples), [XLNet](https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e), [Controlled language with CTRL](https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/). Neben der verbesserten Transformator-Architektur und der groÃŸen Menge an unÃ¼berwachten Trainingsdaten haben auch **bessere Dekodierungsmethoden** eine wichtige Rolle gespielt.\n",
    "\n",
    "Dieses Notebook gibt einen kurzen Ãœberblick Ã¼ber verschiedene Dekodierungsstrategien und zeigt vor allem, wie *Sie* diese mit sehr geringem Aufwand mit der beliebten Bibliothek `transformers` umsetzen kÃ¶nnen!\n",
    "\n",
    "Alle der folgenden Funktionen kÃ¶nnen fÃ¼r die **autoregressive** Spracherzeugung verwendet werden ([hier](http://jalammar.github.io/illustrated-gpt2/) eine Auffrischung). Kurz gesagt, *autoregressive* Sprachgenerierung basiert auf der Annahme, dass die Wahrscheinlichkeitsverteilung einer Wortfolge in das Produkt der bedingten nÃ¤chsten Wortverteilungen zerlegt werden kann:\n",
    "$$ P(w_{1:T} | W_0 ) = \\prod_{t=1}^T P(w_{t} | w_{1: t-1}, W_0) \\text{ ,with } w_{1: 0} = \\emptyset, $$\n",
    "\n",
    "und $W_0$ ist die anfÃ¤ngliche *Kontext*-Wortfolge. Die LÃ¤nge $T$ der Wortfolge wird Ã¼blicherweise *on-the-fly* bestimmt und entspricht dem Zeitschritt $t=T$, in dem das EOS-Token aus $P(w_{t} | w_{1: t-1}, W_{0})$ erzeugt wird.\n",
    "\n",
    "\n",
    "Auto-regressive Spracherzeugung ist nun verfÃ¼gbar fÃ¼r `GPT2`, `XLNet`, `OpenAi-GPT`, `CTRL`, `TransfoXL`, `XLM`, `Bart`, `T5` sowohl in PyTorch als auch in Tensorflow >= 2.0!\n",
    "\n",
    "Wir geben einen Ãœberblick Ã¼ber die derzeit bekanntesten Dekodierungsmethoden, vor allem *Greedy search*, *Beam search*, *Top-K sampling* und *Top-p sampling*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254f830b",
   "metadata": {
    "colab_type": "text",
    "id": "Si4GyYhOQMzi"
   },
   "source": [
    "ZunÃ¤chst mÃ¼ssen Sie die Bibliothek \"transformers\" und das Modell laden. Wir werden GPT2 in Tensorflow 2.1 zur Demonstration verwenden, aber die API ist 1 zu 1 die gleiche fÃ¼r PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2f4b6c0",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ue2kOQhXTAMU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# das EOS-Token als PAD-Token hinzufÃ¼gen, um Warnungen zu vermeiden\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d72c4",
   "metadata": {
    "colab_type": "text",
    "id": "a8Y7cgu9ohXP"
   },
   "source": [
    "### **Greedy Search**\n",
    "\n",
    "Die Greedy Search wÃ¤hlt einfach das Wort mit der hÃ¶chsten Wahrscheinlichkeit als nÃ¤chstes Wort: $w_t = argmax_{w}P(w | w_{1:t-1})$ in jedem Zeitschritt $t$. Die folgende Skizze zeigt die Greedy Search.\n",
    "\n",
    "![Gierige Suche](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/greedy_search.png)\n",
    "\n",
    "Ausgehend von dem Wort $\\text{\"The\"}$, wÃ¤hlt der Greedy Algorithmus das nÃ¤chste Wort mit der hÃ¶chsten Wahrscheinlichkeit $\\text{\"nice\"}$ und so weiter, so dass die endgÃ¼ltige generierte Wortfolge $\\text{\"The\", \"nice\", \"woman\"}$ mit einer Gesamtwahrscheinlichkeit von $0,5 \\mal 0,4 = 0,2$ ist.\n",
    "\n",
    "Im Folgenden werden wir Wortfolgen mit GPT2 auf dem Kontext $(\\text{\"I\", \"enjoy\", \"walking\", \"with\", \"my\", \"cute\", \"dog\"})$ erzeugen. Schauen wir uns an, wie die gierige Suche in `Transformers` wie folgt verwendet werden kann:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b61dea6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "OWLd_J6lXz_t",
    "outputId": "3b9dfd1e-21e6-44f4-f27f-8e975010f9af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure if I'll\n"
     ]
    }
   ],
   "source": [
    "# Kodierung des Kontexts, der fÃ¼r die Generierung erforderlich ist\n",
    "input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='tf')\n",
    "\n",
    "# Text generieren, bis die AusgabelÃ¤nge (die die KontextlÃ¤nge einschlieÃŸt) 50 erreicht\n",
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "print(\"Ausgabe:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162447f6",
   "metadata": {
    "colab_type": "text",
    "id": "BBn1ePmJvhrl"
   },
   "source": [
    "Na also! Wir haben unseren ersten kurzen Text mit GPT2 generiert ðŸ˜Š. Die generierten WÃ¶rter, die dem Kontext folgen, sind vernÃ¼nftig, aber das Modell fÃ¤ngt schnell an, sich zu wiederholen! Dies ist ein sehr hÃ¤ufiges Problem bei der Spracherzeugung im Allgemeinen und scheint bei der Greedy- und Beam-Suche noch hÃ¤ufiger aufzutreten - siehe [Vijayakumar et al., 2016](https://arxiv.org/abs/1610.02424) und [Shao et al., 2017](https://arxiv.org/abs/1701.03185).\n",
    "\n",
    "Der grÃ¶ÃŸte Nachteil der Greedy Search ist jedoch, dass sie WÃ¶rter mit hoher Wahrscheinlichkeit Ã¼bersieht, die sich hinter einem Wort mit niedriger Wahrscheinlichkeit verbergen, wie in unserer Skizze oben zu sehen ist:\n",
    "\n",
    "Das Wort $\\text{\"has\"}$ mit seiner hohen bedingten Wahrscheinlichkeit von $0,9$ ist hinter dem Wort $\\text{\"dog\"}$ versteckt, das nur die zweithÃ¶chste bedingte Wahrscheinlichkeit hat, so dass die gierige Suche die Wortfolge $\\text{\"The\"}, \\text{\"dog\"}, \\text{\"has\"}$ Ã¼bersieht.\n",
    "\n",
    "Zum GlÃ¼ck gibt es die Strahlensuche, um dieses Problem zu lindern!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaff72d",
   "metadata": {
    "colab_type": "text",
    "id": "g8DnXZ1WiuNd"
   },
   "source": [
    "### **Beam Search**\n",
    "\n",
    "Die Beam Search verringert das Risiko, versteckte Wortfolgen mit hoher Wahrscheinlichkeit zu Ã¼bersehen, indem bei jedem Zeitschritt die wahrscheinlichsten `num_beams` der Hypothesen beibehalten und schlieÃŸlich die Hypothese mit der insgesamt hÃ¶chsten Wahrscheinlichkeit ausgewÃ¤hlt wird. Zur Veranschaulichung nehmen wir `num_beams=2`:\n",
    "\n",
    "![Beam search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n",
    "\n",
    "Im Zeitschritt $1$ wird neben der wahrscheinlichsten Hypothese $\\text{\"The\", \"nice\"}$ auch die zweitwahrscheinlichste $\\text{\"The\", \"dog\"}$ verfolgt. Im Zeitschritt $2$ stellt die Strahlensuche fest, dass die Wortfolge $\\text{\"The\", \"dog\", \"has\"}$ mit $0,36$ eine hÃ¶here Wahrscheinlichkeit hat als $\\text{\"The\", \"nice\", \"woman\"}$, die $0,2$ hat. Toll, es hat die wahrscheinlichste Wortfolge in unserem Spielzeugbeispiel gefunden!\n",
    "\n",
    "Die Balkensuche findet immer eine Ausgabesequenz mit hÃ¶herer Wahrscheinlichkeit als die gierige Suche, aber es ist nicht garantiert, dass sie die wahrscheinlichste Ausgabe findet.\n",
    "\n",
    "Schauen wir uns an, wie die Strahlensuche in `Transformers` verwendet werden kann. Wir setzen `num_beams > 1` und `early_stopping=True`, so dass die Erzeugung beendet ist, wenn alle Strahlhypothesen das EOS-Token erreicht haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c443d5d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "R1R5kx30Ynej",
    "outputId": "574f068b-f418-48b5-8334-8451d2221032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with him again. I'm not sure if I'll\n"
     ]
    }
   ],
   "source": [
    "# Beam Search und FrÃ¼hstopp aktivieren\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Ausgabe:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f820c8",
   "metadata": {
    "colab_type": "text",
    "id": "AZ6xs-KLi9jT"
   },
   "source": [
    "Das Ergebnis ist zwar deutlich flÃ¼ssiger, aber die Ausgabe enthÃ¤lt immer noch Wiederholungen der gleichen Wortfolgen.  \n",
    "Eine einfache Abhilfe ist die EinfÃ¼hrung von *n-Grammen* (*a.k.a.* Wortfolgen aus $n$ WÃ¶rtern), wie sie von [Paulus et al. (2017)](https://arxiv.org/abs/1705.04304) und [Klein et al. (2017)](https://arxiv.org/abs/1701.02810) eingefÃ¼hrt wurden. Die gÃ¤ngigste *n-Gramme*-Penalty stellt sicher, dass kein *n-Gramm* zweimal vorkommt, indem sie die Wahrscheinlichkeit der nÃ¤chsten WÃ¶rter, die ein bereits gesehenes *n-Gramm* erzeugen kÃ¶nnten, manuell auf $0$ setzt.\n",
    "\n",
    "Probieren wir es aus, indem wir `no_repeat_ngram_size=2` setzen, so dass kein *2-Gramm* zweimal erscheint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c1c8be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "jy3iVJgfnkMi",
    "outputId": "4d3e6511-711a-4594-a715-aaeb6e48e1a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n"
     ]
    }
   ],
   "source": [
    "# no_repeat_ngram_size auf 2 setzen\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Ausgabe:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0768aae7",
   "metadata": {
    "colab_type": "text",
    "id": "nxsksOGDpmA0"
   },
   "source": [
    "SchÃ¶n, das sieht viel besser aus! Wir kÃ¶nnen sehen, dass die Wiederholung nicht mehr vorkommt. Dennoch mÃ¼ssen *n-gram* penalties mit Vorsicht verwendet werden. Ein Artikel, der Ã¼ber die Stadt *New York* erstellt wird, sollte keine *2-Gramm*-Strafe verwenden, da sonst der Name der Stadt nur einmal im gesamten Text vorkommt!\n",
    "\n",
    "Ein weiteres wichtiges Merkmal der Beam Search (Strahlensuche) ist, dass wir die Top-Strahlen nach der Generierung vergleichen und den generierten Strahl auswÃ¤hlen kÃ¶nnen, der am besten zu unserem Zweck passt.\n",
    "\n",
    "In `transformers` setzen wir einfach den Parameter `num_return_sequences` auf die Anzahl der am hÃ¶chsten bewerteten Balken, die zurÃ¼ckgegeben werden sollen. Stellen Sie aber sicher, dass `num_return_sequences <= num_beams`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34624c6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "5ClO3VphqGp6",
    "outputId": "2296891c-024f-4fd2-9071-bff7c11a3e04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n",
      "1: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to get back to\n",
      "2: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n",
      "3: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to get back to\n",
      "4: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a step\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# jetzt haben wir 3 Ausgabesequenzen\n",
    "print(\"Ausgabe:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e92b89d",
   "metadata": {
    "colab_type": "text",
    "id": "HhLKyfdbsjXc"
   },
   "source": [
    "Wie man sieht, unterscheiden sich die fÃ¼nf Strahlenhypothesen nur geringfÃ¼gig voneinander - was bei nur 5 Strahlen nicht allzu Ã¼berraschend sein sollte.\n",
    "\n",
    "Bei der offenen Generierung wurden in letzter Zeit einige GrÃ¼nde angefÃ¼hrt, warum die Beam Search mÃ¶glicherweise nicht die bestmÃ¶gliche Option ist:\n",
    "\n",
    "- Die Beam Search kann sehr gut bei Aufgaben funktionieren, bei denen die LÃ¤nge der gewÃ¼nschten Generierung mehr oder weniger vorhersehbar ist, wie bei der maschinellen Ãœbersetzung oder Zusammenfassung - siehe [Murray et al. (2018)](https://arxiv.org/abs/1808.10006) und [Yang et al. (2018)](https://arxiv.org/abs/1808.09582). Dies gilt jedoch nicht fÃ¼r die Generierung mit offenem Ende, bei der die gewÃ¼nschte AusgabelÃ¤nge stark variieren kann, z. B. bei der Generierung von Dialogen und Geschichten.\n",
    "\n",
    "- Wir haben gesehen, dass die Beam Search stark unter der repetitiven Generierung leidet. Dies ist besonders schwer mit *n-Gramm*- oder anderen penalties bei der Generierung von Geschichten in den Griff zu bekommen, da die Suche nach einem guten Kompromiss zwischen erzwungener \"Nicht-Wiederholung\" und sich wiederholenden Zyklen identischer *n-Gramme* eine Menge Feinabstimmung erfordert.\n",
    "\n",
    "- Wie in [Ari Holtzman et al. (2019)] (https://arxiv.org/abs/1904.09751) argumentiert wird, folgt hochwertige menschliche Sprache nicht einer Verteilung von mit hoher Wahrscheinlichkeit folgenden WÃ¶rtern. Mit anderen Worten: Als Menschen wollen wir, dass der generierte Text uns Ã¼berrascht und nicht langweilig/vorhersehbar ist. Die Autoren zeigen dies sehr schÃ¶n, indem sie die Wahrscheinlichkeit, die ein Modell einem menschlichen Text geben wÃ¼rde, im Vergleich zu dem, was die Beam Search tut, grafisch darstellen.\n",
    "\n",
    "![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)\n",
    "\n",
    "\n",
    "Also lasst uns aufhÃ¶ren, langweilig zu sein, und etwas ZufÃ¤lligkeit einfÃ¼hren ðŸ¤ª."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f64c0c",
   "metadata": {
    "colab_type": "text",
    "id": "XbbIyK84wHq6"
   },
   "source": [
    "### **Sampling**\n",
    "\n",
    "In seiner einfachsten Form bedeutet Sampling die zufÃ¤llige Auswahl des nÃ¤chsten Wortes $w_t$ gemÃ¤ÃŸ seiner bedingten Wahrscheinlichkeitsverteilung:\n",
    "\n",
    "$$w_t \\sim P(w|w_{1:t-1})$$\n",
    "\n",
    "Anhand des obigen Beispiels veranschaulicht die folgende Grafik die Spracherzeugung beim Sampling.\n",
    "\n",
    "![vanilla_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/sampling_search.png)\n",
    "\n",
    "Es wird deutlich, dass die Spracherzeugung mittels Sampling nicht mehr *deterministisch* ist. Das Wort\n",
    "$\\text{\"car\"}$ wird aus der bedingten Wahrscheinlichkeitsverteilung $P(w | \\text{\"The\"})$ entnommen, gefolgt von einer Stichprobe $\\text{\"drives\"}$ aus $P(w | \\text{\"The\"}, \\text{\"car\"})$.\n",
    "\n",
    "In `transformers` setzen wir `do_sample=True` und deaktivieren *Top-K* Sampling (mehr dazu spÃ¤ter) Ã¼ber `top_k=0`. Im Folgenden werden wir zur Veranschaulichung `random_seed=0` festlegen. Es steht Ihnen frei, die `random_seed` zu Ã¤ndern, um mit dem Modell herumzuspielen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968174c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "aRAz4D-Ks0_4",
    "outputId": "1b78d191-15f6-4cbe-e2b1-23c77366fc21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog and never seem to get nervous until it turns into battery unfrosted. (Laughs) Go ahead with the cover and bring it on. \"3 Light Doppelgangers\" by AFOB\n",
      "\n",
      "â€¦\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seed setzen, um Ergebnisse zu reproduzieren. Sie kÃ¶nnen den Seed aber auch Ã¤ndern, um andere Ergebnisse zu erhalten\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Stichproben aktivieren und top_k deaktivieren, indem top_k sampling auf 0 gesetzt wird\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Ausgabe:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6602836",
   "metadata": {
    "colab_type": "text",
    "id": "mQHuo911wfT-"
   },
   "source": [
    "Interessant! Der Text scheint in Ordnung zu sein - aber wenn man genauer hinsieht, ist er nicht sehr kohÃ¤rent. Die *3-Gramme* *New Hand Sense* und *Local Batte Harness* sind sehr seltsam und klingen nicht so, als wÃ¤ren sie von einem Menschen geschrieben worden. Das ist das groÃŸe Problem beim Sampling von Wortfolgen: Die Modelle erzeugen oft inkohÃ¤rentes Kauderwelsch, *cf.* [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751).\n",
    "\n",
    "Ein Trick besteht darin, die Verteilung $P(w|w_{1:t-1})$ schÃ¤rfer zu machen (die Wahrscheinlichkeit von WÃ¶rtern mit hoher Wahrscheinlichkeit zu erhÃ¶hen und die Wahrscheinlichkeit von WÃ¶rtern mit niedriger Wahrscheinlichkeit zu verringern), indem man die so genannte \"Temperatur\" des [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max) senkt.\n",
    "\n",
    "Ein Beispiel fÃ¼r die Anwendung der Temperatur auf unser obiges Beispiel kÃ¶nnte folgendermaÃŸen aussehen.\n",
    "\n",
    "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/sampling_search_with_temp.png?raw=true)\n",
    "\n",
    "Die bedingte Verteilung des nÃ¤chsten Wortes in Schritt $t=1$ wird viel schÃ¤rfer, so dass das Wort $\\text{\"car\"}$ fast keine Chance mehr hat, ausgewÃ¤hlt zu werden.\n",
    "\n",
    "\n",
    "Schauen wir uns an, wie wir die Verteilung in der Bibliothek abkÃ¼hlen kÃ¶nnen, indem wir `temperature=0.7` einstellen:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab6f94",
   "metadata": {
    "colab_type": "text",
    "id": "kzGuu24hZZnq"
   },
   "source": [
    "OK. Es gibt weniger seltsame n-Gramme und die Ausgabe ist jetzt etwas kohÃ¤renter! WÃ¤hrend die Anwendung von Temperatur eine Verteilung weniger zufÃ¤llig machen kann, wird das temperaturskalierte Sampling im Grenzfall, wenn `Temperatur` auf $0$ gesetzt wird, gleichbedeutend mit gieriger Dekodierung und leidet unter den gleichen Problemen wie zuvor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb38c7d9-68a5-4788-a273-50084eabc184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog. I've always been an avid dog lover. My favorite type of dog is one that has been trained to do many things. I do not have any special preferences for particular breeds. I don't think I have\n"
     ]
    }
   ],
   "source": [
    "# Seed setzen, um Ergebnisse zu reproduzieren. Es steht Ihnen frei, den Seed zu Ã¤ndern, um andere Ergebnisse zu erhalten\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Temperatur verwenden, um die Empfindlichkeit gegenÃ¼ber Kandidaten mit geringer Wahrscheinlichkeit zu verringern\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0, \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bad215",
   "metadata": {
    "colab_type": "text",
    "id": "binNTroyzQBu"
   },
   "source": [
    "### **Top-K Sampling**\n",
    "\n",
    "[Fan et. al (2018)](https://arxiv.org/pdf/1805.04833.pdf) fÃ¼hrte ein einfaches, aber sehr leistungsfÃ¤higes Stichprobenverfahren ein, das sogenannte ***Top-K***-Sampling. Beim *Top-K*-Sampling werden die *K* wahrscheinlichsten nÃ¤chsten WÃ¶rter gefiltert und die Wahrscheinlichkeitsmasse wird nur auf diese *K* nÃ¤chsten WÃ¶rter umverteilt.\n",
    "GPT2 hat dieses Stichprobenverfahren Ã¼bernommen, was einer der GrÃ¼nde fÃ¼r seinen Erfolg bei der Generierung von Geschichten war.\n",
    "\n",
    "Zur besseren Veranschaulichung des *Top-K*-Samplings erweitern wir im obigen Beispiel den Bereich der fÃ¼r beide Sampling-Schritte verwendeten WÃ¶rter von 3 auf 10 WÃ¶rter.\n",
    "\n",
    "![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n",
    "\n",
    "Nachdem wir $K = 6$ gesetzt haben, beschrÃ¤nken wir unseren Stichprobenpool in beiden Schritten auf 6 WÃ¶rter. WÃ¤hrend die 6 wahrscheinlichsten WÃ¶rter, definiert als $V_{\\text{top-K}}$, im ersten Schritt nur *ca.* zwei Drittel der gesamten Wahrscheinlichkeitsmasse umfassen, schlieÃŸen sie im zweiten Schritt fast die gesamte Wahrscheinlichkeitsmasse ein. Nichtsdestotrotz sehen wir, dass sie erfolgreich die eher seltsamen Kandidaten $\\text{\"nicht\", \"die\", \"klein\", \"gesagt\"}$ eliminiert\n",
    "im zweiten Schritt der Stichprobenziehung.\n",
    "\n",
    "\n",
    "Schauen wir uns an, wie *Top-K* in der Bibliothek verwendet werden kann, indem wir `top_k=50` einstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2412799",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "HBtDOdD0wx3l",
    "outputId": "cfc97fac-0956-42ee-a6e5-cad14fc942d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog in the street from 2 1/2 miles (just before sunset) to 3 miles. But there are a few things that you need to know! It's NOT A REPUBLICANS EXCELLENT!\n"
     ]
    }
   ],
   "source": [
    "# Seed setzen, um Ergebnisse zu reproduzieren. Es steht Ihnen frei, den Seed zu Ã¤ndern, um andere Ergebnisse zu erhalten\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# top_k auf 50 setzen\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Ausgabe:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42fa30",
   "metadata": {
    "colab_type": "text",
    "id": "Y77H5m4ZmhEX"
   },
   "source": [
    "Gar nicht so schlecht! Der Text ist wohl der bisher am *menschlichsten* klingende Text.\n",
    "Ein Problem beim *Top-K*-Sampling ist jedoch, dass es die Anzahl der WÃ¶rter, die aus der nÃ¤chsten Wortwahrscheinlichkeitsverteilung $P(w|w_{1:t-1})$ gefiltert werden, nicht dynamisch anpasst.\n",
    "Dies kann problematisch sein, da einige WÃ¶rter aus einer sehr scharfen Verteilung (Verteilung rechts im obigen Diagramm), andere hingegen aus einer viel flacheren Verteilung (Verteilung links im obigen Diagramm) gezogen werden kÃ¶nnten.\n",
    "\n",
    "In Schritt $t=1$ eliminiert *Top-K* die MÃ¶glichkeit\n",
    "$\\text{\"people\", \"big\", \"house\", \"cat\"}$, die als vernÃ¼nftige Kandidaten erscheinen. Andererseits nimmt das Verfahren in Schritt $t=2$ die wohl schlecht passenden WÃ¶rter $\\text{\"down\", \"a\"}$ in den Stichprobenpool auf. Eine Begrenzung des Stichprobenpools auf eine feste GrÃ¶ÃŸe *K* kÃ¶nnte also dazu fÃ¼hren, dass das Modell bei scharfen Verteilungen Kauderwelsch produziert und bei flachen Verteilungen die KreativitÃ¤t des Modells einschrÃ¤nkt.\n",
    "Diese Intuition hat [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751) dazu veranlasst, das ***Top-p***- oder ***nucleus***-Sampling zu entwickeln.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a82e5a4",
   "metadata": {
    "colab_type": "text",
    "id": "ki9LAaexzV3H"
   },
   "source": [
    "### **Top-p (nucleus) sampling**\n",
    "\n",
    "Anstatt nur aus den wahrscheinlichsten *K* WÃ¶rtern eine Stichprobe zu ziehen, wird bei *Top-p* aus der kleinstmÃ¶glichen Menge von WÃ¶rtern ausgewÃ¤hlt, deren kumulative Wahrscheinlichkeit die Wahrscheinlichkeit *p* Ã¼bersteigt. Die Wahrscheinlichkeitsmasse wird dann auf diese Menge von WÃ¶rtern umverteilt. Auf diese Weise kann die GrÃ¶ÃŸe der Wortmenge (*a.k.a.* die Anzahl der WÃ¶rter in der Menge) entsprechend der Wahrscheinlichkeitsverteilung des nÃ¤chsten Wortes dynamisch zu- und abnehmen. Ok, das war sehr wortreich, lassen Sie uns das veranschaulichen.\n",
    "\n",
    "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/top_p_sampling.png?raw=true)\n",
    "\n",
    "Nachdem $p=0,92$ festgelegt wurde, wÃ¤hlt *Top-p* die *minimale* Anzahl von WÃ¶rtern aus, die zusammen $p=92\\%$ der Wahrscheinlichkeitsmasse Ã¼berschreiten, definiert als $V_{\\text{top-p}}$. Im ersten Beispiel umfasste dies die 9 wahrscheinlichsten WÃ¶rter, wÃ¤hrend im zweiten Beispiel nur die obersten 3 WÃ¶rter ausgewÃ¤hlt werden mÃ¼ssen, um 92 % zu Ã¼berschreiten. Eigentlich ganz einfach! Es ist zu erkennen, dass eine groÃŸe Anzahl von WÃ¶rtern erhalten bleibt, bei denen das nÃ¤chste Wort wohl weniger vorhersehbar ist, *z.B.* $P(w | \\text{\"The\"})$, und nur wenige WÃ¶rter, bei denen das nÃ¤chste Wort vorhersehbarer erscheint, *z.B.* $P(w | \\text{\"The\", \"car\"})$.\n",
    "\n",
    "Also gut, dann probieren wir es mal mit `transformers` aus!\n",
    "Wir aktivieren *Top-p* Sampling, indem wir `0 < top_p < 1` setzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73a4f207",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "EvwIc7YAx77F",
    "outputId": "57e2b785-5dcb-4e06-9869-078b758b6a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog and very often see her immediately excited at moving on.\" - Alicia Davis\n",
      "\n",
      "Recently, running around Los Angeles on an expeditions had me thinking, \"What'd I get for free here?\" I worked as a\n"
     ]
    }
   ],
   "source": [
    "# Seed setzen, um Ergebnisse zu reproduzieren. Es steht Ihnen frei, den Seed zu Ã¤ndern, um andere Ergebnisse zu erhalten\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Deaktivieren Sie das top_k-Sampling und wÃ¤hlen Sie nur die 92% wahrscheinlichsten WÃ¶rter aus.\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_p=0.92,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Ausgabe:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c899c47",
   "metadata": {
    "colab_type": "text",
    "id": "tn-8gLaR4lat"
   },
   "source": [
    "Toll, das klingt, als hÃ¤tte es ein Mensch geschrieben. Nun, vielleicht noch nicht ganz.\n",
    "\n",
    "Obwohl *Top-p* in der Theorie eleganter erscheint als *Top-K*, funktionieren beide Methoden in der Praxis gut. *Top-p* kann auch in Kombination mit *Top-K* verwendet werden, wodurch sehr niedrig eingestufte WÃ¶rter vermieden werden kÃ¶nnen und gleichzeitig eine dynamische Auswahl mÃ¶glich ist.\n",
    "\n",
    "Um schlieÃŸlich mehrere unabhÃ¤ngig voneinander abgetastete Ausgaben zu erhalten, kÃ¶nnen wir *wieder* den Parameter `num_return_sequences > 1` setzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f8f8763",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "3kY8P9VG8Gi9",
    "outputId": "6103051e-1681-4ab9-a9c1-1fad437c299d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog\n",
      "\n",
      "In my life I love having fun\n",
      "\n",
      "What does it cost to feed a cat to get a heart attack?\n",
      "\n",
      "I think, this is actually a really good question: what are some of the\n",
      "1: I enjoy walking with my cute dog.\"\n",
      "\n",
      "We didn't realize it until the next day that her husband had become very busy, so we were not surprised when the dog went to visit him outside his apartment and suddenly she had the courage to ask\n",
      "2: I enjoy walking with my cute dog, I like to get a dog from the grocery store and it's been a pleasure to sit and talk to her (when she is sleeping), and she's been so happy with our visit to the supermarket (we\n"
     ]
    }
   ],
   "source": [
    "# Seed setzen, um Ergebnisse zu reproduzieren. Es steht Ihnen frei, den Seed zu Ã¤ndern, um andere Ergebnisse zu erhalten\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# top_k = 50 und top_p = 0,95 und num_return_sequences = 3 setzen\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Ausgabe:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7227b70",
   "metadata": {
    "colab_type": "text",
    "id": "-vRPfMl88rk0"
   },
   "source": [
    "Cool, jetzt solltest du alle Werkzeuge haben, um dein Modell deine Geschichten mit `Transformers` schreiben zu lassen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2063588",
   "metadata": {
    "colab_type": "text",
    "id": "NsWd7e98Vcs3"
   },
   "source": [
    "### **Fazit**\n",
    "\n",
    "Als *ad-hoc* Dekodierungsmethoden scheinen *top-p* und *top-K* Sampling einen flÃ¼ssigeren Text zu produzieren als die traditionelle *Greedy*- und *Beam*-Suche bei der Erzeugung von Sprache mit offenem Ende.\n",
    "In letzter Zeit mehren sich jedoch die Hinweise darauf, dass die offensichtlichen SchwÃ¤chen von *Greedy* und *Beam*-Suche - vor allem die Erzeugung sich wiederholender Wortfolgen - durch das Modell (insbesondere die Art und Weise, wie das Modell trainiert wird) und nicht durch die Dekodierungsmethode verursacht werden, *vgl.* [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf). Wie in [Welleck et al. (2020)](https://arxiv.org/abs/2002.02492) gezeigt wird, scheinen auch *top-K* und *top-p* Sampling unter der Erzeugung sich wiederholender Wortfolgen zu leiden.\n",
    "\n",
    "In [Welleck et al. (2019)] (https://arxiv.org/pdf/1908.04319.pdf) zeigen die Autoren, dass die *Beam*-Suche nach menschlicher EinschÃ¤tzung flÃ¼ssigere Texte erzeugen kann als das *Top-p*-Sampling, wenn man das Trainingsziel des Modells anpasst.\n",
    "\n",
    "Die Generierung offener Sprache ist ein sich rasch entwickelndes Forschungsgebiet, und wie so oft gibt es auch hier keine Einheitsmethode, so dass man sehen muss, was im eigenen Anwendungsfall am besten funktioniert.\n",
    "\n",
    "Gut, dass *du* in `transfomers` alle verschiedenen Dekodierungsmethoden ausprobieren kannst ðŸ¤—.\n",
    "\n",
    "Dies war eine kurze EinfÃ¼hrung in die Verwendung verschiedener Dekodierungsmethoden in \"Transformatoren\" und in die jÃ¼ngsten Trends bei der Erzeugung von Sprachen mit offenem Ende.\n",
    "\n",
    "Feedback und Fragen sind auf dem [Github-Repository] (https://github.com/huggingface/transformers) sehr willkommen.\n",
    "\n",
    "Weitere lustige Geschichten finden Sie unter [Schreiben mit Transformers] (https://transformer.huggingface.co).\n",
    "\n",
    "Vielen Dank an alle, die zu diesem Blogbeitrag beigetragen haben: Alexander Rush, Julien Chaumand, Thomas Wolf, Victor Sanh, Sam Shleifer, ClÃ©ment Delangue, Yacine Jernite, Oliver Ã…strand und John de Wasseige.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1442fb97",
   "metadata": {
    "colab_type": "text",
    "id": "w4CYi91h11yd"
   },
   "source": [
    "### **Anhang**\n",
    "\n",
    "Es gibt eine Reihe zusÃ¤tzlicher Parameter fÃ¼r die Methode `generate`, die oben nicht erwÃ¤hnt wurden. Wir werden sie hier kurz erklÃ¤ren!\n",
    "\n",
    "- min_length\" kann verwendet werden, um das Modell zu zwingen, kein EOS-Token zu produzieren (= den Satz nicht zu beenden), bevor \"min_length\" erreicht ist. Dies wird hÃ¤ufig bei Zusammenfassungen verwendet, kann aber auch allgemein nÃ¼tzlich sein, wenn der Benutzer lÃ¤ngere Ausgaben haben mÃ¶chte.\n",
    "- repetition_penalty\" kann verwendet werden, um WÃ¶rter zu bestrafen, die bereits generiert wurden oder zum Kontext gehÃ¶ren. Sie wurde erstmals von [Kesker et al. (2019)](https://arxiv.org/abs/1909.05858) eingefÃ¼hrt und wird auch im Trainingsziel in [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf) verwendet. Es kann recht effektiv sein, um Wiederholungen zu verhindern, scheint aber sehr empfindlich auf verschiedene Modelle und AnwendungsfÃ¤lle zu reagieren, *siehe z.B.* diese [Diskussion](https://github.com/huggingface/transformers/pull/2303) auf Github.\n",
    "\n",
    "- `attention_mask` kann verwendet werden, um aufgefÃ¼llte Token zu maskieren\n",
    "- `pad_token_id`, `bos_token_id`, `eos_token_id`: Wenn das Modell diese Token nicht standardmÃ¤ÃŸig enthÃ¤lt, kann der Benutzer manuell andere Token-IDs auswÃ¤hlen, um sie zu reprÃ¤sentieren.\n",
    "\n",
    "FÃ¼r weitere Informationen schauen Sie bitte auch in die Funktion `generate` [docstring](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.TFPreTrainedModel.generate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c71316",
   "metadata": {},
   "source": [
    "# Weitere Links (mit Colab Notebooks)\n",
    "\n",
    "https://keras.io/examples/generative/gpt2_text_generation_with_kerasnlp/\n",
    "\n",
    "https://keras.io/examples/generative/gpt2_text_generation_with_kerasnlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe48194-e130-444d-ba0d-67b473131f27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "Formate": "md",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
