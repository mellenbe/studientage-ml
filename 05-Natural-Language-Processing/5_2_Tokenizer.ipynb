{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d24e529a9ff43de6",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "(samples modified from:  https://huggingface.co/docs/transformers/main/en/preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b40843ed823a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8840d71e1bb24519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see: https://huggingface.co/docs/transformers/main/en/preprocessing\n",
    "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
    "print(encoded_input['input_ids'])\n",
    "print(encoded_input['token_type_ids'])\n",
    "print(encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fceeb32b91736",
   "metadata": {},
   "source": [
    "# Handling multiple Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d017898cbbddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_inputs = tokenizer(batch_sentences)\n",
    "\n",
    "# Wir definieren eine kleine Hilfsfunktion, um die Parameter ausgeben zu k√∂nnen:\n",
    "def outp (inputs):\n",
    "    for name in ['input_ids', 'token_type_ids', 'attention_mask']:\n",
    "        if name in inputs:\n",
    "            print(f\"\\n{name}\")\n",
    "            for i in range (len (inputs[name])):\n",
    "                print(inputs[name][i])\n",
    "\n",
    "outp(encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe850d92ce68a6",
   "metadata": {},
   "source": [
    "# Padding & Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21cb67847215d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = tokenizer(batch_sentences, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368cc043a359bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outp(encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a06df9694ddacd9",
   "metadata": {},
   "source": [
    "# Word based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8303a2d7b27e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'But what about second breakfast?'\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4963e43e2391fdf",
   "metadata": {},
   "source": [
    "# Sub-Word based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a5f98d05bfbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet/xlnet-base-cased\")\n",
    "output = tokenizer.tokenize(\"Don't you love ü§ó Transformers? We sure do.\")\n",
    "for i in range (len (output)):\n",
    "    print(output[i], end=' | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca3ffc85373be9",
   "metadata": {},
   "source": [
    "- Transformers wurde in zwei Sub-Words aufgeteilt\n",
    "- Satzzeichen habe eigene Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65824e3fa124182",
   "metadata": {},
   "source": [
    "# Your own Tokenizer\n",
    "\n",
    "## Einen Textcorpus laden\n",
    "[Credits to huggingface: https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb]\n",
    "\n",
    "## Ein Dataset von huggingface laden:\n",
    "Wir laden zun√§chst das \"Wikitext\" Dataset von huggingface. Darin enthalten sind Beispieltexte von Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6deb2878c516557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47575139c782d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca268ceb7d29249",
   "metadata": {},
   "source": [
    "# Zugriffsm√∂glichkeiten auf das DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f65d7a97bfafe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb13755de4b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553f72a4db9ceca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[30000:30005]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf13d35baaf6fef",
   "metadata": {},
   "source": [
    "# DataSet f√ºr den Trainnigsprozess bereitstellen (Corpus)\n",
    "## A) als Liste von Listen\n",
    "Dies l√§sst sich einfach umsetzen, hat aber den Nachteil, dass die Daten im Hauptspeicher gehalten werden m√ºssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca9869e5a986ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "all_texts = [dataset[i : i + batch_size][\"text\"] for i in range(0, len(dataset), batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33150b55f9cdfb1",
   "metadata": {},
   "source": [
    "## B) Als Iterator\n",
    "Ein (Python-) Iterator ist eine M√∂glichkeit, Daten sukzessive nach und nach zu laden. Da das DataSet die Daten auf der Festplatte vorh√§lt, wird f√ºr den Iterator immer nur ein Teil der Daten in den Hauptspeicher geladen.\n",
    "\n",
    "Wir definieren diesen hier als Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997ed561a96c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator():\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e5b5cdba7843d",
   "metadata": {},
   "source": [
    "# Verschiedene Tokenizer aufbauen\n",
    "# 1) Die Architektur eines vorhandenen Tokenizers clonen aber selbst trainieren\n",
    "[Credits to: https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e76835128407a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Wir stellen sicher, dass wir eine \"fast\" Version der Architektur geladen haben, um das Training im Rahmen zu halten\n",
    "if not tokenizer.is_fast:\n",
    "    raise (\"Dieser Tokenizer ist nicht geeignet.\")\n",
    "else:\n",
    "    print(\"Dieser Tokenizer ist geeignet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c6c86ed70809e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_clone_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034d80b1d86917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outp(gpt_clone_tokenizer(dataset[30000:30005][\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d527b5f9df024b1",
   "metadata": {},
   "source": [
    "### Diesen (trainierten!) Tokenizer k√∂nnen wir auch zur Wiederverwertung speichern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5c11cb814806",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_clone_tokenizer.save_pretrained(\"../data/gpt_clone_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f976d524bd4699f5",
   "metadata": {},
   "source": [
    "## 2) Architektur eines Tokenizers selbst aufbauen\n",
    "### Beispiel: Byte Pair Encodings (wie f√ºr GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dc6d7507c64152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ben√∂tigte Imports\n",
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e3b0db74be78",
   "metadata": {},
   "source": [
    "## Tokenizer-Pipeline\n",
    "\"Um zu verstehen, wie man einen Tokenizer von Grund auf erstellt, m√ºssen wir ein wenig mehr in die Tokenizer-Bibliothek und die Tokenisierungs-Pipeline eintauchen. Diese Pipeline besteht aus mehreren Schritten:\n",
    "\n",
    "* **Normalisierung**: F√ºhrt alle anf√§nglichen Transformationen √ºber die anf√§ngliche Eingabezeichenkette aus. Wenn Sie z.B. einen Text klein schreiben, ihn vielleicht entfernen oder einen der √ºblichen Unicode-Normalisierungsprozesse anwenden wollen, f√ºgen Sie einen Normalizer hinzu.\n",
    "* **Pre-Tokenizer**: Verantwortlich f√ºr die Aufteilung der urspr√ºnglichen Eingabezeichenfolge. Das ist die Komponente, die entscheidet, wo und wie die urspr√ºngliche Zeichenkette vorsegmentiert wird. Das einfachste Beispiel w√§re, einfach an Leerzeichen zu trennen.\n",
    "* **Modell**: √úbernimmt die gesamte Erkennung und Generierung von Sub-Token. Dieser Teil kann trainiert werden und ist wirklich von den Eingabedaten abh√§ngig.\n",
    "* **Post-Processing**: Bietet erweiterte Konstruktionsfunktionen, um mit einigen der Transformers-basierten SoTA-Modelle kompatibel zu sein. F√ºr BERT wird der tokenisierte Satz beispielsweise um [CLS]- und [SEP]-Token \"verpackt\".\"\n",
    "\n",
    "Vgl. https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb32439cddfcc5",
   "metadata": {},
   "source": [
    "### Ein BPE-Tokenizer ben√∂tigt einen Pre-Tokenizer (word based tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab9eddb1389c7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basismodell ist ein BPE-Tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33d8cafd73ffce65",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'pre_tokenize_str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# BPE Byte Level Pre-Tokenizer werden √ºber huggingface zur Verf√ºgung gestellt.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_tokenize_str\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer sind toll!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# add_prefix_space=False verhindert, dass vor dem ersten Wort ebenfalls das Trennzeichen 'ƒ†' erscheint. Alle weiteren Token werden mit diesem Zeichen eingeleitet.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m pre_tokenizers\u001b[38;5;241m.\u001b[39mByteLevel(add_prefix_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'pre_tokenize_str'"
     ]
    }
   ],
   "source": [
    "# BPE Byte Level Pre-Tokenizer werden √ºber huggingface zur Verf√ºgung gestellt.\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Tokenizer sind toll!\")\n",
    "# add_prefix_space=False verhindert, dass vor dem ersten Wort ebenfalls das Trennzeichen 'ƒ†' erscheint. Alle weiteren Token werden mit diesem Zeichen eingeleitet.\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c356680f7931dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnisse des Pre-Tokenizers anziegen\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Tokenizer sind toll!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955ef5c8c7880b6",
   "metadata": {},
   "source": [
    "### Unseren Tokenizer trainieren\n",
    "Der Tokenizer muss trainiert werden, denn er muss die Byte Pairs lernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e633cafc47961af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f0743ffc15f17",
   "metadata": {},
   "source": [
    "### Post-Prozessing\n",
    "Der Tokenizer erh√§lt noch ein Post-Prozessing.\n",
    "Der ByteLevel PostProcessor k√ºmmert sich um das Trimming der Offsets. Whitespaces k√∂nnen optional ebenfalls getrimmt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81bd71fc8547dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131aec6071353bf1",
   "metadata": {},
   "source": [
    "### Tokenizer f√ºr Huggingface Framework \"verpacken\"\n",
    "HuggingFace stellt Wrapper zur Verf√ºgung, die sicherstellen, dass die Tokenizer zu den APIs der Modelle passen.\n",
    "\n",
    "Hier verwenden wir einen GPT2TokenizerFast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecbef6e26ed32a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "custom_gpt_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3486153449ec111c",
   "metadata": {},
   "source": [
    "### Neuen Tokenizer anwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daffd63fef1e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outp(custom_gpt_tokenizer(dataset[30000:30005][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5445ac8caa86c331",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
