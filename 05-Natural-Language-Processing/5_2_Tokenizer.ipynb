{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tokenizer\n",
    "\n",
    "(samples modified from:  https://huggingface.co/docs/transformers/main/en/preprocessing)"
   ],
   "id": "d24e529a9ff43de6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:52:37.107905Z",
     "start_time": "2024-04-08T18:52:36.668003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")"
   ],
   "id": "c70b40843ed823a5",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:52:37.766857Z",
     "start_time": "2024-04-08T18:52:37.742692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# see: https://huggingface.co/docs/transformers/main/en/preprocessing\n",
    "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
    "print(encoded_input['input_ids'])\n",
    "print(encoded_input['token_type_ids'])\n",
    "print(encoded_input['attention_mask'])"
   ],
   "id": "8840d71e1bb24519",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2091, 1136, 1143, 13002, 1107, 1103, 5707, 1104, 16678, 1116, 117, 1111, 1152, 1132, 11515, 1105, 3613, 1106, 4470, 119, 102]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Handling multiple Sentences",
   "id": "f8fceeb32b91736"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:52:39.365310Z",
     "start_time": "2024-04-08T18:52:39.349212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_inputs = tokenizer(batch_sentences)\n",
    "\n",
    "# Wir definieren eine kleine Hilfsfunktion, um die Parameter ausgeben zu k√∂nnen:\n",
    "def outp (inputs):\n",
    "    for name in ['input_ids', 'token_type_ids', 'attention_mask']:\n",
    "        if name in inputs:\n",
    "            print(f\"\\n{name}\")\n",
    "            for i in range (len (inputs[name])):\n",
    "                print(inputs[name][i])\n",
    "\n",
    "outp(encoded_inputs)"
   ],
   "id": "95d017898cbbddc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_ids\n",
      "[101, 1252, 1184, 1164, 1248, 6462, 136, 102]\n",
      "[101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102]\n",
      "[101, 1327, 1164, 5450, 23434, 136, 102]\n",
      "\n",
      "token_type_ids\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "attention_mask\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Padding & Truncation",
   "id": "bbbe850d92ce68a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:52:41.246560Z",
     "start_time": "2024-04-08T18:52:41.230920Z"
    }
   },
   "cell_type": "code",
   "source": "encoded_inputs = tokenizer(batch_sentences, padding=True, truncation=True)",
   "id": "d21cb67847215d41",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:52:42.299142Z",
     "start_time": "2024-04-08T18:52:42.283116Z"
    }
   },
   "cell_type": "code",
   "source": "outp(encoded_inputs)",
   "id": "e368cc043a359bb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_ids\n",
      "[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102]\n",
      "[101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "token_type_ids\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "attention_mask\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Word based tokenizer",
   "id": "5a06df9694ddacd9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:52:44.796309Z",
     "start_time": "2024-04-08T18:52:44.764588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = 'But what about second breakfast?'\n",
    "tokenizer.tokenize(text)"
   ],
   "id": "a8303a2d7b27e730",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['But', 'what', 'about', 'second', 'breakfast', '?']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sub-Word based tokenizer",
   "id": "e4963e43e2391fdf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:52:47.834358Z",
     "start_time": "2024-04-08T18:52:47.474384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import XLNetTokenizer\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet/xlnet-base-cased\")\n",
    "output = tokenizer.tokenize(\"Don't you love ü§ó Transformers? We sure do.\")\n",
    "for i in range (len (output)):\n",
    "    print(output[i], end=' | ')"
   ],
   "id": "b04a5f98d05bfbf2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñÅDon | ' | t | ‚ñÅyou | ‚ñÅlove | ‚ñÅ | ü§ó | ‚ñÅ | Transform | ers | ? | ‚ñÅWe | ‚ñÅsure | ‚ñÅdo | . | "
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Transformers wurde in zwei Sub-Words aufgeteilt\n",
    "- Satzzeichen habe eigene Tokens"
   ],
   "id": "a6ca3ffc85373be9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Your own Tokenizer\n",
    "\n",
    "## Einen Textcorpus laden\n",
    "[Credits to huggingface: https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb]\n",
    "\n",
    "## Ein Dataset von huggingface laden:\n",
    "Wir laden zun√§chst das \"Wikitext\" Dataset von huggingface. Darin enthalten sind Beispieltexte von Wikipedia."
   ],
   "id": "a65824e3fa124182"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:52:53.457323Z",
     "start_time": "2024-04-08T18:52:53.441304Z"
    }
   },
   "cell_type": "code",
   "source": "from datasets import load_dataset",
   "id": "d6deb2878c516557",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:53:02.031921Z",
     "start_time": "2024-04-08T18:52:54.548038Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")",
   "id": "e47575139c782d0f",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Zugriffsm√∂glichkeiten auf das DataSet",
   "id": "1ca268ceb7d29249"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:53:04.885749Z",
     "start_time": "2024-04-08T18:53:04.870066Z"
    }
   },
   "cell_type": "code",
   "source": "dataset",
   "id": "2f65d7a97bfafe0c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:53:05.499554Z",
     "start_time": "2024-04-08T18:53:05.483912Z"
    }
   },
   "cell_type": "code",
   "source": "dataset[30000]",
   "id": "99bb13755de4b029",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \" Le souper de Beaucaire was a political pamphlet written by Napoleon Bonaparte in 1793 . With the French Revolution into its fourth year , civil war had spread across France between various rival political factions . Napoleon was involved in military action , on the government 's side , against some rebellious cities of southern France . It was during these events , in 1793 , that he spoke with four merchants from the Midi and heard their views . As a loyal soldier of the Republic he responded in turn , set on dispelling the fears of the merchants and discouraging their beliefs . He later wrote about his conversation in the form of a pamphlet , calling for an end to the civil war . \\n\"}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:53:06.205236Z",
     "start_time": "2024-04-08T18:53:06.189195Z"
    }
   },
   "cell_type": "code",
   "source": "dataset[30000:30005]",
   "id": "553f72a4db9ceca0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [\" Le souper de Beaucaire was a political pamphlet written by Napoleon Bonaparte in 1793 . With the French Revolution into its fourth year , civil war had spread across France between various rival political factions . Napoleon was involved in military action , on the government 's side , against some rebellious cities of southern France . It was during these events , in 1793 , that he spoke with four merchants from the Midi and heard their views . As a loyal soldier of the Republic he responded in turn , set on dispelling the fears of the merchants and discouraging their beliefs . He later wrote about his conversation in the form of a pamphlet , calling for an end to the civil war . \\n\",\n",
       "  '',\n",
       "  ' = = Background = = \\n',\n",
       "  '',\n",
       "  ' During the French Revolution the National Convention became the executive power of France , following the execution of King Louis XVI . With powerful members , such as Maximilien Robespierre and Georges Danton , the Jacobin Club , a French political party established in 1790 , at the birth of the revolution , managed to secure control of the government and pursue the revolution to their own ends , culminating in a \" Reign of Terror \" . Its repressive policies resulted in insurrection across much of France , including the three largest cities after Paris , namely Lyon , Marseille and Toulon , in the south of France . \\n']}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# DataSet f√ºr den Trainnigsprozess bereitstellen (Corpus)\n",
    "## A) als Liste von Listen\n",
    "Dies l√§sst sich einfach umsetzen, hat aber den Nachteil, dass die Daten im Hauptspeicher gehalten werden m√ºssen."
   ],
   "id": "dbf13d35baaf6fef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:53:08.591790Z",
     "start_time": "2024-04-08T18:53:08.543274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 1000\n",
    "all_texts = [dataset[i : i + batch_size][\"text\"] for i in range(0, len(dataset), batch_size)]"
   ],
   "id": "43ca9869e5a986ba",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## B) Als Iterator\n",
    "Ein (Python-) Iterator ist eine M√∂glichkeit, Daten sukzessive nach und nach zu laden. Da das DataSet die Daten auf der Festplatte vorh√§lt, wird f√ºr den Iterator immer nur ein Teil der Daten in den Hauptspeicher geladen.\n",
    "\n",
    "Wir definieren diesen hier als Funktion:"
   ],
   "id": "d33150b55f9cdfb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:53:10.355492Z",
     "start_time": "2024-04-08T18:53:10.339981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def batch_iterator():\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"text\"]"
   ],
   "id": "4997ed561a96c88b",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Verschiedene Tokenizer aufbauen\n",
    "# 1) Die Architektur eines vorhandenen Tokenizers clonen aber selbst trainieren\n",
    "[Credits to: https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb]"
   ],
   "id": "192e5b5cdba7843d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:53:12.309082Z",
     "start_time": "2024-04-08T18:53:12.086951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Wir stellen sicher, dass wir eine \"fast\" Version der Architektur geladen haben, um das Training im Rahmen zu halten\n",
    "if not tokenizer.is_fast:\n",
    "    raise (\"Dieser Tokenizer ist nicht geeignet.\")\n",
    "else:\n",
    "    print(\"Dieser Tokenizer ist geeignet.\")"
   ],
   "id": "d6e76835128407a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dieser Tokenizer ist geeignet.\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:53:14.912219Z",
     "start_time": "2024-04-08T18:53:13.498612Z"
    }
   },
   "cell_type": "code",
   "source": "gpt_clone_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=25000)",
   "id": "a1c6c86ed70809e8",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:53:16.075706Z",
     "start_time": "2024-04-08T18:53:16.059697Z"
    }
   },
   "cell_type": "code",
   "source": "outp(gpt_clone_tokenizer(dataset[30000:30005][\"text\"]))",
   "id": "6034d80b1d86917b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_ids\n",
      "[826, 17937, 536, 416, 1298, 13209, 5762, 321, 259, 2140, 23294, 1680, 370, 12929, 19894, 284, 22994, 272, 1648, 261, 1613, 5537, 650, 520, 2321, 607, 264, 2720, 1123, 461, 4425, 1959, 2319, 771, 2026, 5417, 2140, 17597, 272, 12929, 321, 2820, 284, 1743, 2492, 264, 326, 261, 1465, 331, 83, 1615, 264, 935, 829, 9740, 695, 4828, 281, 2260, 2319, 272, 593, 321, 724, 1084, 2051, 264, 284, 22994, 264, 358, 394, 7996, 359, 847, 18439, 403, 261, 6486, 73, 288, 5902, 507, 6462, 272, 725, 259, 12325, 10998, 281, 261, 2932, 394, 5882, 284, 1470, 264, 890, 326, 3004, 2122, 261, 12300, 281, 261, 18439, 288, 1070, 11706, 286, 507, 9621, 272, 555, 787, 1346, 756, 405, 9631, 284, 261, 803, 281, 259, 23294, 264, 4336, 337, 389, 789, 290, 261, 2720, 1123, 272, 315]\n",
      "[]\n",
      "[301, 301, 5074, 301, 301, 315]\n",
      "[]\n",
      "[1366, 261, 1613, 5537, 261, 1518, 14686, 937, 261, 5518, 1443, 281, 2319, 264, 1212, 261, 10053, 281, 1300, 4460, 17075, 41, 272, 1648, 3990, 1781, 264, 844, 344, 19096, 306, 3788, 3691, 1350, 916, 265, 288, 24000, 363, 21386, 264, 261, 8054, 260, 3842, 264, 259, 1613, 2140, 3695, 1923, 284, 17734, 264, 382, 261, 3186, 281, 261, 6319, 264, 4177, 290, 6394, 1757, 281, 261, 1465, 288, 8842, 261, 6319, 290, 507, 1122, 5485, 264, 17867, 284, 259, 302, 15994, 281, 17660, 302, 272, 3643, 1002, 5995, 10036, 4137, 284, 1082, 24956, 1959, 1364, 281, 2319, 264, 930, 261, 760, 2556, 4828, 613, 3843, 264, 16367, 7838, 263, 264, 21021, 288, 299, 2979, 263, 264, 284, 261, 1279, 281, 2319, 272, 315]\n",
      "\n",
      "attention_mask\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[]\n",
      "[1, 1, 1, 1, 1, 1]\n",
      "[]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Diesen (trainierten!) Tokenizer k√∂nnen wir auch zur Wiederverwertung speichern:",
   "id": "9d527b5f9df024b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:55:18.713302Z",
     "start_time": "2024-04-08T18:55:18.676671Z"
    }
   },
   "cell_type": "code",
   "source": "gpt_clone_tokenizer.save_pretrained(\"../data/gpt_clone_tokenizer\")",
   "id": "d0e5c11cb814806",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/gpt_clone_tokenizer\\\\tokenizer_config.json',\n",
       " '../data/gpt_clone_tokenizer\\\\special_tokens_map.json',\n",
       " '../data/gpt_clone_tokenizer\\\\vocab.json',\n",
       " '../data/gpt_clone_tokenizer\\\\merges.txt',\n",
       " '../data/gpt_clone_tokenizer\\\\added_tokens.json',\n",
       " '../data/gpt_clone_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2) Architektur eines Tokenizers selbst aufbauen\n",
    "### Beispiel: Byte Pair Encodings (wie f√ºr GPT-2)"
   ],
   "id": "f976d524bd4699f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 58,
   "source": [
    "# Ben√∂tigte Imports\n",
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer"
   ],
   "id": "b8dc6d7507c64152"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tokenizer-Pipeline\n",
    "\"Um zu verstehen, wie man einen Tokenizer von Grund auf erstellt, m√ºssen wir ein wenig mehr in die Tokenizer-Bibliothek und die Tokenisierungs-Pipeline eintauchen. Diese Pipeline besteht aus mehreren Schritten:\n",
    "\n",
    "* **Normalisierung**: F√ºhrt alle anf√§nglichen Transformationen √ºber die anf√§ngliche Eingabezeichenkette aus. Wenn Sie z.B. einen Text klein schreiben, ihn vielleicht entfernen oder einen der √ºblichen Unicode-Normalisierungsprozesse anwenden wollen, f√ºgen Sie einen Normalizer hinzu.\n",
    "* **Pre-Tokenizer**: Verantwortlich f√ºr die Aufteilung der urspr√ºnglichen Eingabezeichenfolge. Das ist die Komponente, die entscheidet, wo und wie die urspr√ºngliche Zeichenkette vorsegmentiert wird. Das einfachste Beispiel w√§re, einfach an Leerzeichen zu trennen.\n",
    "* **Modell**: √úbernimmt die gesamte Erkennung und Generierung von Sub-Token. Dieser Teil kann trainiert werden und ist wirklich von den Eingabedaten abh√§ngig.\n",
    "* **Post-Processing**: Bietet erweiterte Konstruktionsfunktionen, um mit einigen der Transformers-basierten SoTA-Modelle kompatibel zu sein. F√ºr BERT wird der tokenisierte Satz beispielsweise um [CLS]- und [SEP]-Token \"verpackt\".\"\n",
    "\n",
    "Vgl. https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb"
   ],
   "id": "a4e3b0db74be78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ein BPE-Tokenizer ben√∂tigt einen Pre-Tokenizer (word based tokens)",
   "id": "9ecb32439cddfcc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 59,
   "source": [
    "# Basismodell ist ein BPE-Tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())"
   ],
   "id": "ab9eddb1389c7009"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T19:04:48.413364Z",
     "start_time": "2024-04-08T19:04:48.397696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# BPE Byte Level Pre-Tokenizer werden √ºber huggingface zur Verf√ºgung gestellt.\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Tokenizer sind toll!\")\n",
    "# add_prefix_space=False verhindert, dass vor dem ersten Wort ebenfalls das Trennzeichen 'ƒ†' erscheint. Alle weiteren Token werden mit diesem Zeichen eingeleitet.\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n"
   ],
   "id": "33d8cafd73ffce65",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T19:04:48.941648Z",
     "start_time": "2024-04-08T19:04:48.925984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ergebnisse des Pre-Tokenizers anziegen\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Tokenizer sind toll!\")"
   ],
   "id": "4c356680f7931dcb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tokenizer', (0, 9)),\n",
       " ('ƒ†sind', (9, 14)),\n",
       " ('ƒ†toll', (14, 19)),\n",
       " ('!', (19, 20))]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Unseren Tokenizer trainieren\n",
    "Der Tokenizer muss trainiert werden, denn er muss die Byte Pairs lernen."
   ],
   "id": "1955ef5c8c7880b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T19:06:28.730976Z",
     "start_time": "2024-04-08T19:06:27.435555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
   ],
   "id": "3e633cafc47961af",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Post-Prozessing\n",
    "Der Tokenizer erh√§lt noch ein Post-Prozessing.\n",
    "Der ByteLevel PostProcessor k√ºmmert sich um das Trimming der Offsets. Whitespaces k√∂nnen optional ebenfalls getrimmt werden.\n"
   ],
   "id": "7d0f0743ffc15f17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T19:08:12.636802Z",
     "start_time": "2024-04-08T19:08:12.621297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()"
   ],
   "id": "b81bd71fc8547dcf",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenizer f√ºr Huggingface Framework \"verpacken\"\n",
    "HuggingFace stellt Wrapper zur Verf√ºgung, die sicherstellen, dass die Tokenizer zu den APIs der Modelle passen.\n",
    "\n",
    "Hier verwenden wir einen GPT2TokenizerFast.\n"
   ],
   "id": "131aec6071353bf1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T19:19:48.955427Z",
     "start_time": "2024-04-08T19:19:48.918329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "custom_gpt_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
   ],
   "id": "7ecbef6e26ed32a7",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Neuen Tokenizer anwenden",
   "id": "3486153449ec111c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T19:20:54.691570Z",
     "start_time": "2024-04-08T19:20:54.668574Z"
    }
   },
   "cell_type": "code",
   "source": "outp(custom_gpt_tokenizer(dataset[30000:30005][\"text\"]))",
   "id": "4daffd63fef1e8f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_ids\n",
      "[763, 17874, 473, 353, 1235, 13146, 5699, 258, 196, 2077, 23231, 1617, 307, 12866, 19831, 221, 22931, 209, 1585, 198, 1550, 5474, 587, 457, 2258, 544, 201, 2657, 1060, 398, 4362, 1896, 2256, 708, 1963, 5354, 2077, 17534, 209, 12866, 258, 2757, 221, 1680, 2429, 201, 263, 198, 1402, 268, 83, 1552, 201, 872, 766, 9677, 632, 4765, 218, 2197, 2256, 209, 530, 258, 661, 1021, 1988, 201, 221, 22931, 201, 295, 331, 7933, 296, 784, 18376, 340, 198, 6423, 73, 225, 5839, 444, 6399, 209, 662, 196, 12262, 10935, 218, 198, 2869, 331, 5819, 221, 1407, 201, 827, 263, 2941, 2059, 198, 12237, 218, 198, 18376, 225, 1007, 11643, 223, 444, 9558, 209, 492, 724, 1283, 693, 342, 9568, 221, 198, 740, 218, 196, 23231, 201, 4273, 274, 326, 726, 227, 198, 2657, 1060, 209, 252]\n",
      "[]\n",
      "[238, 238, 5011, 238, 238, 252]\n",
      "[]\n",
      "[1303, 198, 1550, 5474, 198, 1455, 14623, 874, 198, 5455, 1380, 218, 2256, 201, 1149, 198, 9990, 218, 1237, 4397, 17012, 41, 209, 1585, 3927, 1718, 201, 781, 281, 19033, 243, 3725, 3628, 1287, 853, 202, 225, 23937, 300, 21323, 201, 198, 7991, 197, 3779, 201, 196, 1550, 2077, 3632, 1860, 221, 17671, 201, 319, 198, 3123, 218, 198, 6256, 201, 4114, 227, 6331, 1694, 218, 198, 1402, 225, 8779, 198, 6256, 227, 444, 1059, 5422, 201, 17804, 221, 196, 239, 15931, 218, 17597, 239, 209, 3580, 939, 5932, 9973, 4074, 221, 1019, 24893, 1896, 1301, 218, 2256, 201, 867, 198, 697, 2493, 4765, 550, 3780, 201, 16304, 7775, 200, 201, 20958, 225, 236, 2916, 200, 201, 221, 198, 1216, 218, 2256, 209, 252]\n",
      "\n",
      "attention_mask\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[]\n",
      "[1, 1, 1, 1, 1, 1]\n",
      "[]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1681c1b6b9b5d387"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
