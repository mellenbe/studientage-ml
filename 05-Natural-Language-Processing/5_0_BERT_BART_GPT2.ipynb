{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22de419-5866-4771-8400-5e65523a122e",
   "metadata": {},
   "source": [
    "# BERT, BART, GPT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392693b2-52cc-45f1-b515-86db34214a7e",
   "metadata": {},
   "source": [
    "**BERT (Pre-training of Deep Bidirectional Transformers for Language Understanding)** (by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova) ist ein Encoder-only **transformerbasiertes Modell**, das **2018 von Google** eingeführt wurde. . BERT wurde anhand einer großen Menge von Textdaten mit dem so genannten unsupervised Pretraining (vor-)trainiert. Dadurch kann BERT universelle Sprachrepräsentationen erlernen, die für spezifische Aufgaben wie die Beantwortung von Fragen und die Stimmungsanalyse fein abgestimmt ('finetunig') werden können. BERT hat bei einer Vielzahl von NLP-Aufgaben sehr gute Ergebnisse erzielt und wird in der Industrie und im akademischen Bereich häufig eingesetzt. BERT wurde für **NLU (Natural Language Understanding) Aufgaben** trainiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7425ca62-df96-456c-b147-fa7d31fdc197",
   "metadata": {},
   "source": [
    "**BART (Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension)** (by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer) ist ein **transformerbasiertes Modell**, das von **'Facebook AI' im Jahr 2020** eingeführt wurde. Wie BERT wird auch BART auf einer großen Menge von Textdaten vortrainiert. Im Gegensatz zu BERT wird BART jedoch darauf trainiert, den ursprünglichen Satz aus einer 'beschädigten' Version zu rekonstruieren, was als **denoising autoencoding** bezeichnet wird. Dadurch kann BART robustere Darstellungen des Textes erlernen und komplexere Sprachaufgaben bewältigen. BART hat bei einer Vielzahl von NLP-Aufgaben sehr gute Ergebnisse erzielt und wird auch in der Industrie und im akademischen Bereich häufig eingesetzt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5356ef2-df82-434b-9f76-43558c7cda66",
   "metadata": {},
   "source": [
    "Das **GPT-2-Modell von OpenAI** ist ein Encoder-Decoder **transformerbasiertes Modell**, das für NLG (Natural Language Generation) Aufgaben trainiert wurde (**Vorhersage des nächsten Worts**). GPT-2 war das erste Modell, das in der Lage war, in ausgewählten Fällen menschenähnlichen Text zu produzieren. Das Modell wurde auf einem riesigen ungefilterten Korpus aus dem Internet trainiert. Es enthält daher verschiedene Arten von Text, wie Nachrichten, Wikipedia-Artikel oder sogar Codeschnipsel. Die Architektur des Modells besteht aus mehreren Transformator-Decoder-Blöcken (d. h. Blöcken, die aus einer vollständig verknüpften Schicht über der maskierten Selbstbeobachtung bestehen), die übereinander angeordnet sind. **GPT-2 verwendet 'masked Self-Attention'** nur für den Kontext davor, so dass Token auf der rechten Seite die Einbettung von Token auf der linken Seite nicht beeinflussen. GPT-2 ist das zweite Modell der GPT-Reihe (GPT, GPT-2, GPT-3), die von OpenAI veröffentlicht wurde."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377f3c2-3d5f-479b-bef1-3a9dd6497ef1",
   "metadata": {},
   "source": [
    "### Quellen:\n",
    "- https://huggingface.co/docs/transformers/model_doc/bert\n",
    "- https://huggingface.co/docs/transformers/model_doc/bart\n",
    "- https://bert-vs-gpt2.dbvis.de\n",
    "- https://iq.opengenus.org/bart-vs-bert/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
